CUDA_VISIBLE_DEVICES: 0
Activating TensorFlow-2.6.2 environment
Running clusterTrain.py
pybullet build time: Nov 28 2023 23:48:36
/home/tmorgan01/anaconda3/envs/dan/lib/python3.11/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 802: system not yet initialized (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)
  return torch._C._cuda_getDeviceCount() > 0
Using cpu device
Logging to Training/clusterResults/clusterLogs/PPO_3
/home/tmorgan01/anaconda3/envs/dan/lib/python3.11/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
Eval num_timesteps=1000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
New best mean reward!
Eval num_timesteps=2000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
-----------------------------
| time/              |      |
|    fps             | 810  |
|    iterations      | 1    |
|    time_elapsed    | 2    |
|    total_timesteps | 2048 |
-----------------------------
Eval num_timesteps=3000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -1           |
| time/                   |              |
|    total_timesteps      | 3000         |
| train/                  |              |
|    approx_kl            | 0.0037307371 |
|    clip_fraction        | 0.00708      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.84        |
|    explained_variance   | 0.896        |
|    learning_rate        | 3e-05        |
|    loss                 | -0.0126      |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.00323     |
|    std                  | 0.998        |
|    value_loss           | 0.0154       |
------------------------------------------
Eval num_timesteps=4000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
-----------------------------
| time/              |      |
|    fps             | 649  |
|    iterations      | 2    |
|    time_elapsed    | 6    |
|    total_timesteps | 4096 |
-----------------------------
Eval num_timesteps=5000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 300        |
|    mean_reward          | -1         |
| time/                   |            |
|    total_timesteps      | 5000       |
| train/                  |            |
|    approx_kl            | 0.00145429 |
|    clip_fraction        | 0.000195   |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.83      |
|    explained_variance   | 0.856      |
|    learning_rate        | 3e-05      |
|    loss                 | -0.0093    |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.00109   |
|    std                  | 0.998      |
|    value_loss           | 0.0125     |
----------------------------------------
Eval num_timesteps=6000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
-----------------------------
| time/              |      |
|    fps             | 634  |
|    iterations      | 3    |
|    time_elapsed    | 9    |
|    total_timesteps | 6144 |
-----------------------------
Eval num_timesteps=7000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -1           |
| time/                   |              |
|    total_timesteps      | 7000         |
| train/                  |              |
|    approx_kl            | 0.0010337826 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.83        |
|    explained_variance   | 0.821        |
|    learning_rate        | 3.01e-05     |
|    loss                 | -0.00589     |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.00111     |
|    std                  | 0.998        |
|    value_loss           | 0.00814      |
------------------------------------------
Eval num_timesteps=8000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
-----------------------------
| time/              |      |
|    fps             | 626  |
|    iterations      | 4    |
|    time_elapsed    | 13   |
|    total_timesteps | 8192 |
-----------------------------
Eval num_timesteps=9000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 300           |
|    mean_reward          | -1            |
| time/                   |               |
|    total_timesteps      | 9000          |
| train/                  |               |
|    approx_kl            | 0.00044183285 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.83         |
|    explained_variance   | 0.86          |
|    learning_rate        | 3.01e-05      |
|    loss                 | -0.00375      |
|    n_updates            | 40            |
|    policy_gradient_loss | -0.00059      |
|    std                  | 0.997         |
|    value_loss           | 0.00383       |
-------------------------------------------
Eval num_timesteps=10000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 620   |
|    iterations      | 5     |
|    time_elapsed    | 16    |
|    total_timesteps | 10240 |
------------------------------
Eval num_timesteps=11000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -1           |
| time/                   |              |
|    total_timesteps      | 11000        |
| train/                  |              |
|    approx_kl            | 0.0027425457 |
|    clip_fraction        | 0.00396      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.83        |
|    explained_variance   | 0.899        |
|    learning_rate        | 3.01e-05     |
|    loss                 | 0.00472      |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.00219     |
|    std                  | 0.998        |
|    value_loss           | 0.00108      |
------------------------------------------
Eval num_timesteps=12000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 617   |
|    iterations      | 6     |
|    time_elapsed    | 19    |
|    total_timesteps | 12288 |
------------------------------
Eval num_timesteps=13000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 300           |
|    mean_reward          | -1            |
| time/                   |               |
|    total_timesteps      | 13000         |
| train/                  |               |
|    approx_kl            | 0.00066797587 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.83         |
|    explained_variance   | 0.852         |
|    learning_rate        | 3.01e-05      |
|    loss                 | 0.00164       |
|    n_updates            | 60            |
|    policy_gradient_loss | -0.000732     |
|    std                  | 0.997         |
|    value_loss           | 0.00134       |
-------------------------------------------
Eval num_timesteps=14000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 615   |
|    iterations      | 7     |
|    time_elapsed    | 23    |
|    total_timesteps | 14336 |
------------------------------
Eval num_timesteps=15000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 300           |
|    mean_reward          | -1            |
| time/                   |               |
|    total_timesteps      | 15000         |
| train/                  |               |
|    approx_kl            | 0.00064219005 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.83         |
|    explained_variance   | 0.847         |
|    learning_rate        | 3.02e-05      |
|    loss                 | -0.00586      |
|    n_updates            | 70            |
|    policy_gradient_loss | -0.00107      |
|    std                  | 0.996         |
|    value_loss           | 0.00136       |
-------------------------------------------
Eval num_timesteps=16000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 613   |
|    iterations      | 8     |
|    time_elapsed    | 26    |
|    total_timesteps | 16384 |
------------------------------
Eval num_timesteps=17000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -1           |
| time/                   |              |
|    total_timesteps      | 17000        |
| train/                  |              |
|    approx_kl            | 0.0008475623 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.83        |
|    explained_variance   | 0.759        |
|    learning_rate        | 3.02e-05     |
|    loss                 | -0.00764     |
|    n_updates            | 80           |
|    policy_gradient_loss | -0.000709    |
|    std                  | 0.996        |
|    value_loss           | 0.00183      |
------------------------------------------
Eval num_timesteps=18000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 612   |
|    iterations      | 9     |
|    time_elapsed    | 30    |
|    total_timesteps | 18432 |
------------------------------
Eval num_timesteps=19000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -1          |
| time/                   |             |
|    total_timesteps      | 19000       |
| train/                  |             |
|    approx_kl            | 0.002019053 |
|    clip_fraction        | 0.00117     |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.83       |
|    explained_variance   | 0.752       |
|    learning_rate        | 3.02e-05    |
|    loss                 | -0.00428    |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.00153    |
|    std                  | 0.996       |
|    value_loss           | 0.00228     |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 611   |
|    iterations      | 10    |
|    time_elapsed    | 33    |
|    total_timesteps | 20480 |
------------------------------
Eval num_timesteps=21000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -1           |
| time/                   |              |
|    total_timesteps      | 21000        |
| train/                  |              |
|    approx_kl            | 0.0007822579 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.83        |
|    explained_variance   | 0.774        |
|    learning_rate        | 3.02e-05     |
|    loss                 | -0.0112      |
|    n_updates            | 100          |
|    policy_gradient_loss | -0.00106     |
|    std                  | 0.997        |
|    value_loss           | 0.0015       |
------------------------------------------
Eval num_timesteps=22000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 610   |
|    iterations      | 11    |
|    time_elapsed    | 36    |
|    total_timesteps | 22528 |
------------------------------
Eval num_timesteps=23000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -1           |
| time/                   |              |
|    total_timesteps      | 23000        |
| train/                  |              |
|    approx_kl            | 0.0015959393 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.83        |
|    explained_variance   | 0.754        |
|    learning_rate        | 3.03e-05     |
|    loss                 | -0.0269      |
|    n_updates            | 110          |
|    policy_gradient_loss | -0.00144     |
|    std                  | 0.996        |
|    value_loss           | 0.0012       |
------------------------------------------
Eval num_timesteps=24000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 609   |
|    iterations      | 12    |
|    time_elapsed    | 40    |
|    total_timesteps | 24576 |
------------------------------
Eval num_timesteps=25000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -1           |
| time/                   |              |
|    total_timesteps      | 25000        |
| train/                  |              |
|    approx_kl            | 0.0024308967 |
|    clip_fraction        | 0.00298      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.83        |
|    explained_variance   | 0.812        |
|    learning_rate        | 3.03e-05     |
|    loss                 | -0.00503     |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.00155     |
|    std                  | 0.995        |
|    value_loss           | 0.000785     |
------------------------------------------
Eval num_timesteps=26000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 608   |
|    iterations      | 13    |
|    time_elapsed    | 43    |
|    total_timesteps | 26624 |
------------------------------
Eval num_timesteps=27000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -1           |
| time/                   |              |
|    total_timesteps      | 27000        |
| train/                  |              |
|    approx_kl            | 0.0026238956 |
|    clip_fraction        | 0.0019       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.83        |
|    explained_variance   | 0.65         |
|    learning_rate        | 3.03e-05     |
|    loss                 | -0.00895     |
|    n_updates            | 130          |
|    policy_gradient_loss | -0.00157     |
|    std                  | 0.993        |
|    value_loss           | 0.000843     |
------------------------------------------
Eval num_timesteps=28000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 608   |
|    iterations      | 14    |
|    time_elapsed    | 47    |
|    total_timesteps | 28672 |
------------------------------
Eval num_timesteps=29000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -1           |
| time/                   |              |
|    total_timesteps      | 29000        |
| train/                  |              |
|    approx_kl            | 0.0016733723 |
|    clip_fraction        | 0.00107      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.82        |
|    explained_variance   | 0.679        |
|    learning_rate        | 3.03e-05     |
|    loss                 | 0.00515      |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.00208     |
|    std                  | 0.993        |
|    value_loss           | 0.00088      |
------------------------------------------
Eval num_timesteps=30000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 607   |
|    iterations      | 15    |
|    time_elapsed    | 50    |
|    total_timesteps | 30720 |
------------------------------
Eval num_timesteps=31000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -1           |
| time/                   |              |
|    total_timesteps      | 31000        |
| train/                  |              |
|    approx_kl            | 0.0011583896 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.82        |
|    explained_variance   | 0.324        |
|    learning_rate        | 3.04e-05     |
|    loss                 | 0.00612      |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.000744    |
|    std                  | 0.991        |
|    value_loss           | 0.00108      |
------------------------------------------
Eval num_timesteps=32000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 607   |
|    iterations      | 16    |
|    time_elapsed    | 53    |
|    total_timesteps | 32768 |
------------------------------
Eval num_timesteps=33000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -1          |
| time/                   |             |
|    total_timesteps      | 33000       |
| train/                  |             |
|    approx_kl            | 0.002195221 |
|    clip_fraction        | 0.00103     |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.82       |
|    explained_variance   | 0.515       |
|    learning_rate        | 3.04e-05    |
|    loss                 | 0.00113     |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.00152    |
|    std                  | 0.99        |
|    value_loss           | 0.00129     |
-----------------------------------------
Eval num_timesteps=34000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 607   |
|    iterations      | 17    |
|    time_elapsed    | 57    |
|    total_timesteps | 34816 |
------------------------------
Eval num_timesteps=35000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -1           |
| time/                   |              |
|    total_timesteps      | 35000        |
| train/                  |              |
|    approx_kl            | 0.0009905079 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.82        |
|    explained_variance   | 0.594        |
|    learning_rate        | 3.04e-05     |
|    loss                 | 0.00016      |
|    n_updates            | 170          |
|    policy_gradient_loss | -0.000762    |
|    std                  | 0.989        |
|    value_loss           | 0.00154      |
------------------------------------------
Eval num_timesteps=36000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 604   |
|    iterations      | 18    |
|    time_elapsed    | 60    |
|    total_timesteps | 36864 |
------------------------------
Eval num_timesteps=37000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -1          |
| time/                   |             |
|    total_timesteps      | 37000       |
| train/                  |             |
|    approx_kl            | 0.002838791 |
|    clip_fraction        | 0.00181     |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.81       |
|    explained_variance   | 0.241       |
|    learning_rate        | 3.04e-05    |
|    loss                 | -0.0191     |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.00153    |
|    std                  | 0.987       |
|    value_loss           | 0.000909    |
-----------------------------------------
Eval num_timesteps=38000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 604   |
|    iterations      | 19    |
|    time_elapsed    | 64    |
|    total_timesteps | 38912 |
------------------------------
Eval num_timesteps=39000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 300           |
|    mean_reward          | -1            |
| time/                   |               |
|    total_timesteps      | 39000         |
| train/                  |               |
|    approx_kl            | 0.00021037098 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.81         |
|    explained_variance   | 0.35          |
|    learning_rate        | 3.05e-05      |
|    loss                 | 0.00261       |
|    n_updates            | 190           |
|    policy_gradient_loss | -0.000263     |
|    std                  | 0.985         |
|    value_loss           | 0.00185       |
-------------------------------------------
Eval num_timesteps=40000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 604   |
|    iterations      | 20    |
|    time_elapsed    | 67    |
|    total_timesteps | 40960 |
------------------------------
Eval num_timesteps=41000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -1           |
| time/                   |              |
|    total_timesteps      | 41000        |
| train/                  |              |
|    approx_kl            | 0.0025533014 |
|    clip_fraction        | 0.00195      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.81        |
|    explained_variance   | 0.518        |
|    learning_rate        | 3.05e-05     |
|    loss                 | -0.00462     |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.00206     |
|    std                  | 0.983        |
|    value_loss           | 0.00145      |
------------------------------------------
Eval num_timesteps=42000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
Eval num_timesteps=43000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 598   |
|    iterations      | 21    |
|    time_elapsed    | 71    |
|    total_timesteps | 43008 |
------------------------------
Eval num_timesteps=44000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -1           |
| time/                   |              |
|    total_timesteps      | 44000        |
| train/                  |              |
|    approx_kl            | 0.0011158232 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.8         |
|    explained_variance   | 0.467        |
|    learning_rate        | 3.05e-05     |
|    loss                 | -0.00211     |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.000963    |
|    std                  | 0.982        |
|    value_loss           | 0.00164      |
------------------------------------------
Eval num_timesteps=45000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 598   |
|    iterations      | 22    |
|    time_elapsed    | 75    |
|    total_timesteps | 45056 |
------------------------------
Eval num_timesteps=46000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -1           |
| time/                   |              |
|    total_timesteps      | 46000        |
| train/                  |              |
|    approx_kl            | 0.0010689476 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.8         |
|    explained_variance   | 0.335        |
|    learning_rate        | 3.05e-05     |
|    loss                 | -0.00708     |
|    n_updates            | 220          |
|    policy_gradient_loss | -0.000851    |
|    std                  | 0.981        |
|    value_loss           | 0.0017       |
------------------------------------------
Eval num_timesteps=47000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 598   |
|    iterations      | 23    |
|    time_elapsed    | 78    |
|    total_timesteps | 47104 |
------------------------------
Eval num_timesteps=48000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -1          |
| time/                   |             |
|    total_timesteps      | 48000       |
| train/                  |             |
|    approx_kl            | 0.001773104 |
|    clip_fraction        | 0.000439    |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.8        |
|    explained_variance   | 0.546       |
|    learning_rate        | 3.06e-05    |
|    loss                 | 0.00262     |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.00164    |
|    std                  | 0.98        |
|    value_loss           | 0.00162     |
-----------------------------------------
Eval num_timesteps=49000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 598   |
|    iterations      | 24    |
|    time_elapsed    | 82    |
|    total_timesteps | 49152 |
------------------------------
Eval num_timesteps=50000, episode_reward=-0.98 +/- 0.03
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.984      |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.003143128 |
|    clip_fraction        | 0.00493     |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.8        |
|    explained_variance   | 0.105       |
|    learning_rate        | 3.06e-05    |
|    loss                 | -0.0246     |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.00286    |
|    std                  | 0.977       |
|    value_loss           | 0.00185     |
-----------------------------------------
New best mean reward!
Eval num_timesteps=51000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 598   |
|    iterations      | 25    |
|    time_elapsed    | 85    |
|    total_timesteps | 51200 |
------------------------------
Eval num_timesteps=52000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -1           |
| time/                   |              |
|    total_timesteps      | 52000        |
| train/                  |              |
|    approx_kl            | 0.0011637713 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.79        |
|    explained_variance   | 0.527        |
|    learning_rate        | 3.06e-05     |
|    loss                 | 0.00502      |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.00109     |
|    std                  | 0.976        |
|    value_loss           | 0.00135      |
------------------------------------------
Eval num_timesteps=53000, episode_reward=-0.63 +/- 0.48
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.634   |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
New best mean reward!
------------------------------
| time/              |       |
|    fps             | 598   |
|    iterations      | 26    |
|    time_elapsed    | 88    |
|    total_timesteps | 53248 |
------------------------------
Eval num_timesteps=54000, episode_reward=-0.88 +/- 0.24
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.882       |
| time/                   |              |
|    total_timesteps      | 54000        |
| train/                  |              |
|    approx_kl            | 0.0030403929 |
|    clip_fraction        | 0.00264      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.79        |
|    explained_variance   | 0.19         |
|    learning_rate        | 3.06e-05     |
|    loss                 | -0.0131      |
|    n_updates            | 260          |
|    policy_gradient_loss | -0.00255     |
|    std                  | 0.975        |
|    value_loss           | 0.00132      |
------------------------------------------
Eval num_timesteps=55000, episode_reward=-0.73 +/- 0.53
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.733   |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 598   |
|    iterations      | 27    |
|    time_elapsed    | 92    |
|    total_timesteps | 55296 |
------------------------------
Eval num_timesteps=56000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -1          |
| time/                   |             |
|    total_timesteps      | 56000       |
| train/                  |             |
|    approx_kl            | 0.003756592 |
|    clip_fraction        | 0.00942     |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.79       |
|    explained_variance   | 0.0489      |
|    learning_rate        | 3.07e-05    |
|    loss                 | 0.00419     |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.00291    |
|    std                  | 0.973       |
|    value_loss           | 0.00185     |
-----------------------------------------
Eval num_timesteps=57000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 598   |
|    iterations      | 28    |
|    time_elapsed    | 95    |
|    total_timesteps | 57344 |
------------------------------
Eval num_timesteps=58000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -1           |
| time/                   |              |
|    total_timesteps      | 58000        |
| train/                  |              |
|    approx_kl            | 0.0040685358 |
|    clip_fraction        | 0.0148       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.78        |
|    explained_variance   | 0.648        |
|    learning_rate        | 3.07e-05     |
|    loss                 | -0.00406     |
|    n_updates            | 280          |
|    policy_gradient_loss | -0.00354     |
|    std                  | 0.974        |
|    value_loss           | 0.00102      |
------------------------------------------
box reached target
Eval num_timesteps=59000, episode_reward=0.85 +/- 2.41
Episode length: 298.60 +/- 2.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 299      |
|    mean_reward     | 0.847    |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
New best mean reward!
------------------------------
| time/              |       |
|    fps             | 598   |
|    iterations      | 29    |
|    time_elapsed    | 99    |
|    total_timesteps | 59392 |
------------------------------
Eval num_timesteps=60000, episode_reward=-0.71 +/- 0.59
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.707      |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.004801589 |
|    clip_fraction        | 0.0137      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.78       |
|    explained_variance   | 0.635       |
|    learning_rate        | 3.07e-05    |
|    loss                 | 0.004       |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.00332    |
|    std                  | 0.972       |
|    value_loss           | 0.00184     |
-----------------------------------------
Eval num_timesteps=61000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 598   |
|    iterations      | 30    |
|    time_elapsed    | 102   |
|    total_timesteps | 61440 |
------------------------------
box reached target
box reached target
Eval num_timesteps=62000, episode_reward=1.61 +/- 3.20
Episode length: 283.80 +/- 24.05
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 284        |
|    mean_reward          | 1.61       |
| time/                   |            |
|    total_timesteps      | 62000      |
| train/                  |            |
|    approx_kl            | 0.00412902 |
|    clip_fraction        | 0.0154     |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.78      |
|    explained_variance   | 0.534      |
|    learning_rate        | 3.07e-05   |
|    loss                 | 9.04e-05   |
|    n_updates            | 300        |
|    policy_gradient_loss | -0.00279   |
|    std                  | 0.97       |
|    value_loss           | 0.00142    |
----------------------------------------
New best mean reward!
Eval num_timesteps=63000, episode_reward=-0.74 +/- 0.51
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.745   |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 598   |
|    iterations      | 31    |
|    time_elapsed    | 105   |
|    total_timesteps | 63488 |
------------------------------
Eval num_timesteps=64000, episode_reward=-0.84 +/- 0.32
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.84        |
| time/                   |              |
|    total_timesteps      | 64000        |
| train/                  |              |
|    approx_kl            | 0.0023275306 |
|    clip_fraction        | 0.00181      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.77        |
|    explained_variance   | 0.723        |
|    learning_rate        | 3.08e-05     |
|    loss                 | -0.00354     |
|    n_updates            | 310          |
|    policy_gradient_loss | -0.00175     |
|    std                  | 0.967        |
|    value_loss           | 0.00157      |
------------------------------------------
Eval num_timesteps=65000, episode_reward=-0.77 +/- 0.46
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.769   |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 598   |
|    iterations      | 32    |
|    time_elapsed    | 109   |
|    total_timesteps | 65536 |
------------------------------
box reached target
Eval num_timesteps=66000, episode_reward=-0.36 +/- 0.61
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.363       |
| time/                   |              |
|    total_timesteps      | 66000        |
| train/                  |              |
|    approx_kl            | 0.0026086562 |
|    clip_fraction        | 0.00435      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.77        |
|    explained_variance   | 0.535        |
|    learning_rate        | 3.08e-05     |
|    loss                 | -0.00402     |
|    n_updates            | 320          |
|    policy_gradient_loss | -0.00286     |
|    std                  | 0.965        |
|    value_loss           | 0.00289      |
------------------------------------------
box reached target
Eval num_timesteps=67000, episode_reward=0.66 +/- 2.34
Episode length: 287.20 +/- 25.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | 0.665    |
| time/              |          |
|    total_timesteps | 67000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 599   |
|    iterations      | 33    |
|    time_elapsed    | 112   |
|    total_timesteps | 67584 |
------------------------------
Eval num_timesteps=68000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -1           |
| time/                   |              |
|    total_timesteps      | 68000        |
| train/                  |              |
|    approx_kl            | 0.0052599637 |
|    clip_fraction        | 0.0194       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.77        |
|    explained_variance   | 0.293        |
|    learning_rate        | 3.08e-05     |
|    loss                 | -0.00658     |
|    n_updates            | 330          |
|    policy_gradient_loss | -0.00459     |
|    std                  | 0.963        |
|    value_loss           | 0.0818       |
------------------------------------------
box reached target
Eval num_timesteps=69000, episode_reward=0.32 +/- 2.65
Episode length: 294.40 +/- 11.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 294      |
|    mean_reward     | 0.323    |
| time/              |          |
|    total_timesteps | 69000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 599   |
|    iterations      | 34    |
|    time_elapsed    | 116   |
|    total_timesteps | 69632 |
------------------------------
Eval num_timesteps=70000, episode_reward=-0.70 +/- 0.60
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.699      |
| time/                   |             |
|    total_timesteps      | 70000       |
| train/                  |             |
|    approx_kl            | 0.002990114 |
|    clip_fraction        | 0.0041      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.76       |
|    explained_variance   | 0.86        |
|    learning_rate        | 3.08e-05    |
|    loss                 | 0.0161      |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.00233    |
|    std                  | 0.961       |
|    value_loss           | 0.007       |
-----------------------------------------
Eval num_timesteps=71000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 71000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 599   |
|    iterations      | 35    |
|    time_elapsed    | 119   |
|    total_timesteps | 71680 |
------------------------------
Eval num_timesteps=72000, episode_reward=-0.56 +/- 0.64
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.564       |
| time/                   |              |
|    total_timesteps      | 72000        |
| train/                  |              |
|    approx_kl            | 0.0012669788 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.76        |
|    explained_variance   | 0.832        |
|    learning_rate        | 3.09e-05     |
|    loss                 | -0.00702     |
|    n_updates            | 350          |
|    policy_gradient_loss | -0.000953    |
|    std                  | 0.96         |
|    value_loss           | 0.00315      |
------------------------------------------
Eval num_timesteps=73000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 73000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 599   |
|    iterations      | 36    |
|    time_elapsed    | 123   |
|    total_timesteps | 73728 |
------------------------------
Eval num_timesteps=74000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -1           |
| time/                   |              |
|    total_timesteps      | 74000        |
| train/                  |              |
|    approx_kl            | 0.0024329354 |
|    clip_fraction        | 0.00513      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.76        |
|    explained_variance   | 0.483        |
|    learning_rate        | 3.09e-05     |
|    loss                 | -0.00611     |
|    n_updates            | 360          |
|    policy_gradient_loss | -0.002       |
|    std                  | 0.961        |
|    value_loss           | 0.00292      |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=75000, episode_reward=1.51 +/- 3.08
Episode length: 281.20 +/- 30.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 281      |
|    mean_reward     | 1.51     |
| time/              |          |
|    total_timesteps | 75000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 599   |
|    iterations      | 37    |
|    time_elapsed    | 126   |
|    total_timesteps | 75776 |
------------------------------
box reached target
Eval num_timesteps=76000, episode_reward=0.69 +/- 2.34
Episode length: 290.40 +/- 19.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 290          |
|    mean_reward          | 0.691        |
| time/                   |              |
|    total_timesteps      | 76000        |
| train/                  |              |
|    approx_kl            | 0.0021730731 |
|    clip_fraction        | 0.00161      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.76        |
|    explained_variance   | 0.203        |
|    learning_rate        | 3.09e-05     |
|    loss                 | -0.0136      |
|    n_updates            | 370          |
|    policy_gradient_loss | -0.00193     |
|    std                  | 0.959        |
|    value_loss           | 0.00383      |
------------------------------------------
Eval num_timesteps=77000, episode_reward=-0.82 +/- 0.36
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.822   |
| time/              |          |
|    total_timesteps | 77000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 599   |
|    iterations      | 38    |
|    time_elapsed    | 129   |
|    total_timesteps | 77824 |
------------------------------
Eval num_timesteps=78000, episode_reward=-0.43 +/- 0.71
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.43       |
| time/                   |             |
|    total_timesteps      | 78000       |
| train/                  |             |
|    approx_kl            | 0.003587321 |
|    clip_fraction        | 0.00581     |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.75       |
|    explained_variance   | 0.812       |
|    learning_rate        | 3.09e-05    |
|    loss                 | 0.00824     |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.00229    |
|    std                  | 0.958       |
|    value_loss           | 0.00224     |
-----------------------------------------
box reached target
Eval num_timesteps=79000, episode_reward=0.26 +/- 2.51
Episode length: 282.80 +/- 34.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 283      |
|    mean_reward     | 0.257    |
| time/              |          |
|    total_timesteps | 79000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 598   |
|    iterations      | 39    |
|    time_elapsed    | 133   |
|    total_timesteps | 79872 |
------------------------------
box reached target
Eval num_timesteps=80000, episode_reward=0.43 +/- 2.44
Episode length: 295.00 +/- 10.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 295          |
|    mean_reward          | 0.433        |
| time/                   |              |
|    total_timesteps      | 80000        |
| train/                  |              |
|    approx_kl            | 0.0016752884 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.75        |
|    explained_variance   | 0.637        |
|    learning_rate        | 3.1e-05      |
|    loss                 | 0.00138      |
|    n_updates            | 390          |
|    policy_gradient_loss | -0.00116     |
|    std                  | 0.956        |
|    value_loss           | 0.00104      |
------------------------------------------
Eval num_timesteps=81000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 81000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 598   |
|    iterations      | 40    |
|    time_elapsed    | 136   |
|    total_timesteps | 81920 |
------------------------------
Eval num_timesteps=82000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -1           |
| time/                   |              |
|    total_timesteps      | 82000        |
| train/                  |              |
|    approx_kl            | 0.0036490075 |
|    clip_fraction        | 0.00659      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.75        |
|    explained_variance   | 0.429        |
|    learning_rate        | 3.1e-05      |
|    loss                 | 0.00139      |
|    n_updates            | 400          |
|    policy_gradient_loss | -0.00283     |
|    std                  | 0.954        |
|    value_loss           | 0.00189      |
------------------------------------------
Eval num_timesteps=83000, episode_reward=-0.74 +/- 0.52
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.738   |
| time/              |          |
|    total_timesteps | 83000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 598   |
|    iterations      | 41    |
|    time_elapsed    | 140   |
|    total_timesteps | 83968 |
------------------------------
box reached target
Eval num_timesteps=84000, episode_reward=0.25 +/- 2.50
Episode length: 287.00 +/- 26.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 287          |
|    mean_reward          | 0.251        |
| time/                   |              |
|    total_timesteps      | 84000        |
| train/                  |              |
|    approx_kl            | 0.0015221133 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.75        |
|    explained_variance   | 0.498        |
|    learning_rate        | 3.1e-05      |
|    loss                 | 0.00499      |
|    n_updates            | 410          |
|    policy_gradient_loss | -0.00131     |
|    std                  | 0.956        |
|    value_loss           | 0.00153      |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=85000, episode_reward=1.50 +/- 3.06
Episode length: 277.00 +/- 33.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 277      |
|    mean_reward     | 1.5      |
| time/              |          |
|    total_timesteps | 85000    |
---------------------------------
Eval num_timesteps=86000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 596   |
|    iterations      | 42    |
|    time_elapsed    | 144   |
|    total_timesteps | 86016 |
------------------------------
Eval num_timesteps=87000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -1          |
| time/                   |             |
|    total_timesteps      | 87000       |
| train/                  |             |
|    approx_kl            | 0.003461297 |
|    clip_fraction        | 0.00557     |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.75       |
|    explained_variance   | 0.74        |
|    learning_rate        | 3.1e-05     |
|    loss                 | 0.0037      |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.00243    |
|    std                  | 0.955       |
|    value_loss           | 0.00159     |
-----------------------------------------
box reached target
Eval num_timesteps=88000, episode_reward=0.65 +/- 2.40
Episode length: 291.80 +/- 16.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 292      |
|    mean_reward     | 0.654    |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 596   |
|    iterations      | 43    |
|    time_elapsed    | 147   |
|    total_timesteps | 88064 |
------------------------------
Eval num_timesteps=89000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -1           |
| time/                   |              |
|    total_timesteps      | 89000        |
| train/                  |              |
|    approx_kl            | 0.0045490013 |
|    clip_fraction        | 0.0195       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.75        |
|    explained_variance   | 0.833        |
|    learning_rate        | 3.11e-05     |
|    loss                 | 0.00388      |
|    n_updates            | 430          |
|    policy_gradient_loss | -0.00428     |
|    std                  | 0.955        |
|    value_loss           | 0.00112      |
------------------------------------------
box reached target
Eval num_timesteps=90000, episode_reward=0.98 +/- 2.20
Episode length: 280.00 +/- 40.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 280      |
|    mean_reward     | 0.981    |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 597   |
|    iterations      | 44    |
|    time_elapsed    | 150   |
|    total_timesteps | 90112 |
------------------------------
box reached target
box reached target
Eval num_timesteps=91000, episode_reward=1.48 +/- 3.04
Episode length: 267.80 +/- 43.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 268         |
|    mean_reward          | 1.48        |
| time/                   |             |
|    total_timesteps      | 91000       |
| train/                  |             |
|    approx_kl            | 0.006592052 |
|    clip_fraction        | 0.04        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.75       |
|    explained_variance   | 0.884       |
|    learning_rate        | 3.11e-05    |
|    loss                 | 0.00107     |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.00504    |
|    std                  | 0.956       |
|    value_loss           | 0.000391    |
-----------------------------------------
box reached target
Eval num_timesteps=92000, episode_reward=0.60 +/- 2.45
Episode length: 281.60 +/- 36.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 282      |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 597   |
|    iterations      | 45    |
|    time_elapsed    | 154   |
|    total_timesteps | 92160 |
------------------------------
box reached target
Eval num_timesteps=93000, episode_reward=0.29 +/- 2.58
Episode length: 286.40 +/- 27.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 286          |
|    mean_reward          | 0.29         |
| time/                   |              |
|    total_timesteps      | 93000        |
| train/                  |              |
|    approx_kl            | 0.0031758277 |
|    clip_fraction        | 0.00444      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.74        |
|    explained_variance   | 0.874        |
|    learning_rate        | 3.11e-05     |
|    loss                 | -0.00329     |
|    n_updates            | 450          |
|    policy_gradient_loss | -0.00195     |
|    std                  | 0.953        |
|    value_loss           | 0.000952     |
------------------------------------------
Eval num_timesteps=94000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 597   |
|    iterations      | 46    |
|    time_elapsed    | 157   |
|    total_timesteps | 94208 |
------------------------------
box reached target
Eval num_timesteps=95000, episode_reward=0.30 +/- 2.61
Episode length: 285.80 +/- 28.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 286         |
|    mean_reward          | 0.303       |
| time/                   |             |
|    total_timesteps      | 95000       |
| train/                  |             |
|    approx_kl            | 0.004119303 |
|    clip_fraction        | 0.015       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.74       |
|    explained_variance   | 0.679       |
|    learning_rate        | 3.11e-05    |
|    loss                 | -0.00368    |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.0032     |
|    std                  | 0.95        |
|    value_loss           | 0.00159     |
-----------------------------------------
box reached target
Eval num_timesteps=96000, episode_reward=0.39 +/- 2.51
Episode length: 274.00 +/- 52.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 274      |
|    mean_reward     | 0.388    |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 598   |
|    iterations      | 47    |
|    time_elapsed    | 160   |
|    total_timesteps | 96256 |
------------------------------
box reached target
Eval num_timesteps=97000, episode_reward=0.54 +/- 2.35
Episode length: 286.20 +/- 27.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 286          |
|    mean_reward          | 0.538        |
| time/                   |              |
|    total_timesteps      | 97000        |
| train/                  |              |
|    approx_kl            | 0.0039965687 |
|    clip_fraction        | 0.0235       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.74        |
|    explained_variance   | 0.872        |
|    learning_rate        | 3.12e-05     |
|    loss                 | -0.00661     |
|    n_updates            | 470          |
|    policy_gradient_loss | -0.00367     |
|    std                  | 0.95         |
|    value_loss           | 0.00115      |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=98000, episode_reward=0.88 +/- 2.30
Episode length: 282.40 +/- 35.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 282      |
|    mean_reward     | 0.884    |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 598   |
|    iterations      | 48    |
|    time_elapsed    | 164   |
|    total_timesteps | 98304 |
------------------------------
Eval num_timesteps=99000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -1          |
| time/                   |             |
|    total_timesteps      | 99000       |
| train/                  |             |
|    approx_kl            | 0.002133809 |
|    clip_fraction        | 0.000684    |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.73       |
|    explained_variance   | 0.283       |
|    learning_rate        | 3.12e-05    |
|    loss                 | 0.00908     |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.00123    |
|    std                  | 0.949       |
|    value_loss           | 0.0752      |
-----------------------------------------
box reached target
Eval num_timesteps=100000, episode_reward=0.52 +/- 2.51
Episode length: 285.00 +/- 30.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 285      |
|    mean_reward     | 0.524    |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 598    |
|    iterations      | 49     |
|    time_elapsed    | 167    |
|    total_timesteps | 100352 |
-------------------------------
box reached target
Eval num_timesteps=101000, episode_reward=0.50 +/- 2.36
Episode length: 273.80 +/- 52.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 274          |
|    mean_reward          | 0.503        |
| time/                   |              |
|    total_timesteps      | 101000       |
| train/                  |              |
|    approx_kl            | 0.0041591395 |
|    clip_fraction        | 0.0179       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.73        |
|    explained_variance   | 0.883        |
|    learning_rate        | 3.12e-05     |
|    loss                 | -0.00337     |
|    n_updates            | 490          |
|    policy_gradient_loss | -0.00354     |
|    std                  | 0.948        |
|    value_loss           | 0.00153      |
------------------------------------------
Eval num_timesteps=102000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 102000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 599    |
|    iterations      | 50     |
|    time_elapsed    | 170    |
|    total_timesteps | 102400 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=103000, episode_reward=1.56 +/- 3.13
Episode length: 266.60 +/- 40.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 267          |
|    mean_reward          | 1.56         |
| time/                   |              |
|    total_timesteps      | 103000       |
| train/                  |              |
|    approx_kl            | 0.0020719625 |
|    clip_fraction        | 0.00337      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.73        |
|    explained_variance   | 0.814        |
|    learning_rate        | 3.12e-05     |
|    loss                 | -0.0144      |
|    n_updates            | 500          |
|    policy_gradient_loss | -0.00325     |
|    std                  | 0.947        |
|    value_loss           | 0.00127      |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=104000, episode_reward=0.26 +/- 2.52
Episode length: 276.00 +/- 48.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 276      |
|    mean_reward     | 0.262    |
| time/              |          |
|    total_timesteps | 104000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 599    |
|    iterations      | 51     |
|    time_elapsed    | 174    |
|    total_timesteps | 104448 |
-------------------------------
box reached target
Eval num_timesteps=105000, episode_reward=0.36 +/- 2.52
Episode length: 271.80 +/- 56.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 272          |
|    mean_reward          | 0.356        |
| time/                   |              |
|    total_timesteps      | 105000       |
| train/                  |              |
|    approx_kl            | 0.0013365657 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.73        |
|    explained_variance   | -0.0752      |
|    learning_rate        | 3.13e-05     |
|    loss                 | 0.00963      |
|    n_updates            | 510          |
|    policy_gradient_loss | -0.000949    |
|    std                  | 0.947        |
|    value_loss           | 0.1          |
------------------------------------------
box reached target
Eval num_timesteps=106000, episode_reward=0.48 +/- 2.48
Episode length: 273.00 +/- 54.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 273      |
|    mean_reward     | 0.482    |
| time/              |          |
|    total_timesteps | 106000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 600    |
|    iterations      | 52     |
|    time_elapsed    | 177    |
|    total_timesteps | 106496 |
-------------------------------
Eval num_timesteps=107000, episode_reward=-0.78 +/- 0.45
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.776       |
| time/                   |              |
|    total_timesteps      | 107000       |
| train/                  |              |
|    approx_kl            | 0.0011257767 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.73        |
|    explained_variance   | 0.894        |
|    learning_rate        | 3.13e-05     |
|    loss                 | -0.0123      |
|    n_updates            | 520          |
|    policy_gradient_loss | -0.00174     |
|    std                  | 0.946        |
|    value_loss           | 0.00209      |
------------------------------------------
Eval num_timesteps=108000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 108000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 600    |
|    iterations      | 53     |
|    time_elapsed    | 180    |
|    total_timesteps | 108544 |
-------------------------------
Eval num_timesteps=109000, episode_reward=-0.52 +/- 0.67
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.516       |
| time/                   |              |
|    total_timesteps      | 109000       |
| train/                  |              |
|    approx_kl            | 0.0040368782 |
|    clip_fraction        | 0.02         |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.72        |
|    explained_variance   | 0.597        |
|    learning_rate        | 3.13e-05     |
|    loss                 | -0.00632     |
|    n_updates            | 530          |
|    policy_gradient_loss | -0.0044      |
|    std                  | 0.944        |
|    value_loss           | 0.00252      |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=110000, episode_reward=0.24 +/- 2.49
Episode length: 277.20 +/- 45.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 277      |
|    mean_reward     | 0.244    |
| time/              |          |
|    total_timesteps | 110000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 600    |
|    iterations      | 54     |
|    time_elapsed    | 184    |
|    total_timesteps | 110592 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=111000, episode_reward=1.50 +/- 3.07
Episode length: 254.00 +/- 56.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 254          |
|    mean_reward          | 1.5          |
| time/                   |              |
|    total_timesteps      | 111000       |
| train/                  |              |
|    approx_kl            | 0.0012690283 |
|    clip_fraction        | 0.000928     |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.72        |
|    explained_variance   | 0.287        |
|    learning_rate        | 3.13e-05     |
|    loss                 | 0.103        |
|    n_updates            | 540          |
|    policy_gradient_loss | -0.00195     |
|    std                  | 0.941        |
|    value_loss           | 0.0745       |
------------------------------------------
box reached target
Eval num_timesteps=112000, episode_reward=0.26 +/- 2.52
Episode length: 275.00 +/- 50.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 275      |
|    mean_reward     | 0.259    |
| time/              |          |
|    total_timesteps | 112000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 600    |
|    iterations      | 55     |
|    time_elapsed    | 187    |
|    total_timesteps | 112640 |
-------------------------------
Eval num_timesteps=113000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -1           |
| time/                   |              |
|    total_timesteps      | 113000       |
| train/                  |              |
|    approx_kl            | 0.0024931543 |
|    clip_fraction        | 0.00112      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.71        |
|    explained_variance   | 0.88         |
|    learning_rate        | 3.14e-05     |
|    loss                 | -0.0223      |
|    n_updates            | 550          |
|    policy_gradient_loss | -0.00144     |
|    std                  | 0.94         |
|    value_loss           | 0.000933     |
------------------------------------------
box reached target
Eval num_timesteps=114000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 114000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 600    |
|    iterations      | 56     |
|    time_elapsed    | 190    |
|    total_timesteps | 114688 |
-------------------------------
box reached target
Eval num_timesteps=115000, episode_reward=0.25 +/- 2.51
Episode length: 279.60 +/- 40.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 280         |
|    mean_reward          | 0.253       |
| time/                   |             |
|    total_timesteps      | 115000      |
| train/                  |             |
|    approx_kl            | 0.000828586 |
|    clip_fraction        | 0.000146    |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.71       |
|    explained_variance   | 0.532       |
|    learning_rate        | 3.14e-05    |
|    loss                 | 0.00464     |
|    n_updates            | 560         |
|    policy_gradient_loss | -0.000358   |
|    std                  | 0.939       |
|    value_loss           | 0.0579      |
-----------------------------------------
box reached target
Eval num_timesteps=116000, episode_reward=0.43 +/- 2.36
Episode length: 280.60 +/- 38.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 281      |
|    mean_reward     | 0.428    |
| time/              |          |
|    total_timesteps | 116000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 601    |
|    iterations      | 57     |
|    time_elapsed    | 194    |
|    total_timesteps | 116736 |
-------------------------------
Eval num_timesteps=117000, episode_reward=-0.12 +/- 0.78
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.122      |
| time/                   |             |
|    total_timesteps      | 117000      |
| train/                  |             |
|    approx_kl            | 0.004589243 |
|    clip_fraction        | 0.0132      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.71       |
|    explained_variance   | 0.859       |
|    learning_rate        | 3.14e-05    |
|    loss                 | -0.00203    |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.00259    |
|    std                  | 0.937       |
|    value_loss           | 0.00299     |
-----------------------------------------
Eval num_timesteps=118000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 118000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 601    |
|    iterations      | 58     |
|    time_elapsed    | 197    |
|    total_timesteps | 118784 |
-------------------------------
box reached target
Eval num_timesteps=119000, episode_reward=0.23 +/- 2.46
Episode length: 284.60 +/- 30.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 285          |
|    mean_reward          | 0.232        |
| time/                   |              |
|    total_timesteps      | 119000       |
| train/                  |              |
|    approx_kl            | 0.0040102233 |
|    clip_fraction        | 0.00835      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.71        |
|    explained_variance   | 0.817        |
|    learning_rate        | 3.14e-05     |
|    loss                 | 0.00706      |
|    n_updates            | 580          |
|    policy_gradient_loss | -0.00318     |
|    std                  | 0.938        |
|    value_loss           | 0.00136      |
------------------------------------------
box reached target
Eval num_timesteps=120000, episode_reward=-0.54 +/- 0.62
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.541   |
| time/              |          |
|    total_timesteps | 120000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 601    |
|    iterations      | 59     |
|    time_elapsed    | 200    |
|    total_timesteps | 120832 |
-------------------------------
Eval num_timesteps=121000, episode_reward=-0.49 +/- 0.63
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.489       |
| time/                   |              |
|    total_timesteps      | 121000       |
| train/                  |              |
|    approx_kl            | 0.0018158192 |
|    clip_fraction        | 0.00146      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.71        |
|    explained_variance   | -0.442       |
|    learning_rate        | 3.14e-05     |
|    loss                 | 0.0185       |
|    n_updates            | 590          |
|    policy_gradient_loss | -0.00119     |
|    std                  | 0.937        |
|    value_loss           | 0.106        |
------------------------------------------
Eval num_timesteps=122000, episode_reward=-0.80 +/- 0.40
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.798   |
| time/              |          |
|    total_timesteps | 122000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 601    |
|    iterations      | 60     |
|    time_elapsed    | 204    |
|    total_timesteps | 122880 |
-------------------------------
box reached target
Eval num_timesteps=123000, episode_reward=0.38 +/- 2.50
Episode length: 278.60 +/- 42.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 279          |
|    mean_reward          | 0.383        |
| time/                   |              |
|    total_timesteps      | 123000       |
| train/                  |              |
|    approx_kl            | 0.0032255428 |
|    clip_fraction        | 0.0105       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.71        |
|    explained_variance   | 0.852        |
|    learning_rate        | 3.15e-05     |
|    loss                 | 0.00714      |
|    n_updates            | 600          |
|    policy_gradient_loss | -0.00277     |
|    std                  | 0.937        |
|    value_loss           | 0.00149      |
------------------------------------------
Eval num_timesteps=124000, episode_reward=-0.77 +/- 0.45
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.774   |
| time/              |          |
|    total_timesteps | 124000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 601    |
|    iterations      | 61     |
|    time_elapsed    | 207    |
|    total_timesteps | 124928 |
-------------------------------
box reached target
Eval num_timesteps=125000, episode_reward=0.50 +/- 2.38
Episode length: 277.20 +/- 45.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 277          |
|    mean_reward          | 0.5          |
| time/                   |              |
|    total_timesteps      | 125000       |
| train/                  |              |
|    approx_kl            | 0.0051882295 |
|    clip_fraction        | 0.0174       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.71        |
|    explained_variance   | 0.473        |
|    learning_rate        | 3.15e-05     |
|    loss                 | 0.0056       |
|    n_updates            | 610          |
|    policy_gradient_loss | -0.00241     |
|    std                  | 0.936        |
|    value_loss           | 0.00103      |
------------------------------------------
Eval num_timesteps=126000, episode_reward=-0.69 +/- 0.61
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.693   |
| time/              |          |
|    total_timesteps | 126000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 601    |
|    iterations      | 62     |
|    time_elapsed    | 211    |
|    total_timesteps | 126976 |
-------------------------------
box reached target
Eval num_timesteps=127000, episode_reward=0.52 +/- 2.53
Episode length: 273.80 +/- 52.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 274         |
|    mean_reward          | 0.516       |
| time/                   |             |
|    total_timesteps      | 127000      |
| train/                  |             |
|    approx_kl            | 0.003288633 |
|    clip_fraction        | 0.0115      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.71       |
|    explained_variance   | 0.857       |
|    learning_rate        | 3.15e-05    |
|    loss                 | 0.00807     |
|    n_updates            | 620         |
|    policy_gradient_loss | -0.0029     |
|    std                  | 0.939       |
|    value_loss           | 0.000724    |
-----------------------------------------
box reached target
Eval num_timesteps=128000, episode_reward=0.25 +/- 2.50
Episode length: 279.40 +/- 41.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 279      |
|    mean_reward     | 0.252    |
| time/              |          |
|    total_timesteps | 128000   |
---------------------------------
Eval num_timesteps=129000, episode_reward=-0.85 +/- 0.29
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.854   |
| time/              |          |
|    total_timesteps | 129000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 600    |
|    iterations      | 63     |
|    time_elapsed    | 214    |
|    total_timesteps | 129024 |
-------------------------------
box reached target
Eval num_timesteps=130000, episode_reward=0.25 +/- 2.50
Episode length: 283.40 +/- 33.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 283          |
|    mean_reward          | 0.251        |
| time/                   |              |
|    total_timesteps      | 130000       |
| train/                  |              |
|    approx_kl            | 0.0053788796 |
|    clip_fraction        | 0.0439       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.71        |
|    explained_variance   | 0.866        |
|    learning_rate        | 3.15e-05     |
|    loss                 | -0.0175      |
|    n_updates            | 630          |
|    policy_gradient_loss | -0.00474     |
|    std                  | 0.936        |
|    value_loss           | 0.00113      |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=131000, episode_reward=2.78 +/- 3.09
Episode length: 244.60 +/- 51.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 245      |
|    mean_reward     | 2.78     |
| time/              |          |
|    total_timesteps | 131000   |
---------------------------------
New best mean reward!
-------------------------------
| time/              |        |
|    fps             | 600    |
|    iterations      | 64     |
|    time_elapsed    | 218    |
|    total_timesteps | 131072 |
-------------------------------
Eval num_timesteps=132000, episode_reward=-0.71 +/- 0.58
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.71        |
| time/                   |              |
|    total_timesteps      | 132000       |
| train/                  |              |
|    approx_kl            | 0.0026289697 |
|    clip_fraction        | 0.0042       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.7         |
|    explained_variance   | 0.924        |
|    learning_rate        | 3.16e-05     |
|    loss                 | -0.00326     |
|    n_updates            | 640          |
|    policy_gradient_loss | -0.00147     |
|    std                  | 0.935        |
|    value_loss           | 0.00124      |
------------------------------------------
Eval num_timesteps=133000, episode_reward=-0.71 +/- 0.57
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.715   |
| time/              |          |
|    total_timesteps | 133000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 600    |
|    iterations      | 65     |
|    time_elapsed    | 221    |
|    total_timesteps | 133120 |
-------------------------------
box reached target
Eval num_timesteps=134000, episode_reward=-0.98 +/- 0.03
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.984       |
| time/                   |              |
|    total_timesteps      | 134000       |
| train/                  |              |
|    approx_kl            | 0.0044223303 |
|    clip_fraction        | 0.0284       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.7         |
|    explained_variance   | 0.793        |
|    learning_rate        | 3.16e-05     |
|    loss                 | -0.0136      |
|    n_updates            | 650          |
|    policy_gradient_loss | -0.00344     |
|    std                  | 0.934        |
|    value_loss           | 0.000928     |
------------------------------------------
Eval num_timesteps=135000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 135000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 600    |
|    iterations      | 66     |
|    time_elapsed    | 224    |
|    total_timesteps | 135168 |
-------------------------------
Eval num_timesteps=136000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -1           |
| time/                   |              |
|    total_timesteps      | 136000       |
| train/                  |              |
|    approx_kl            | 0.0018120399 |
|    clip_fraction        | 0.00137      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.7         |
|    explained_variance   | 0.214        |
|    learning_rate        | 3.16e-05     |
|    loss                 | 0.0144       |
|    n_updates            | 660          |
|    policy_gradient_loss | -0.00109     |
|    std                  | 0.935        |
|    value_loss           | 0.0736       |
------------------------------------------
box reached target
Eval num_timesteps=137000, episode_reward=0.53 +/- 2.45
Episode length: 276.40 +/- 47.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 276      |
|    mean_reward     | 0.529    |
| time/              |          |
|    total_timesteps | 137000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 600    |
|    iterations      | 67     |
|    time_elapsed    | 228    |
|    total_timesteps | 137216 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=138000, episode_reward=0.61 +/- 2.44
Episode length: 276.20 +/- 47.60
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 276           |
|    mean_reward          | 0.613         |
| time/                   |               |
|    total_timesteps      | 138000        |
| train/                  |               |
|    approx_kl            | 0.00037596343 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.7          |
|    explained_variance   | 0.838         |
|    learning_rate        | 3.16e-05      |
|    loss                 | -0.000871     |
|    n_updates            | 670           |
|    policy_gradient_loss | -0.000303     |
|    std                  | 0.936         |
|    value_loss           | 0.0111        |
-------------------------------------------
box reached target
Eval num_timesteps=139000, episode_reward=0.89 +/- 2.34
Episode length: 278.80 +/- 42.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 279      |
|    mean_reward     | 0.888    |
| time/              |          |
|    total_timesteps | 139000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 601    |
|    iterations      | 68     |
|    time_elapsed    | 231    |
|    total_timesteps | 139264 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=140000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 300        |
|    mean_reward          | -1         |
| time/                   |            |
|    total_timesteps      | 140000     |
| train/                  |            |
|    approx_kl            | 0.00175494 |
|    clip_fraction        | 0.00464    |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.7       |
|    explained_variance   | 0.53       |
|    learning_rate        | 3.17e-05   |
|    loss                 | -0.00112   |
|    n_updates            | 680        |
|    policy_gradient_loss | -0.0026    |
|    std                  | 0.933      |
|    value_loss           | 0.0569     |
----------------------------------------
Eval num_timesteps=141000, episode_reward=-0.32 +/- 0.63
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.322   |
| time/              |          |
|    total_timesteps | 141000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 601    |
|    iterations      | 69     |
|    time_elapsed    | 235    |
|    total_timesteps | 141312 |
-------------------------------
box reached target
Eval num_timesteps=142000, episode_reward=0.25 +/- 2.51
Episode length: 280.80 +/- 38.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 281           |
|    mean_reward          | 0.254         |
| time/                   |               |
|    total_timesteps      | 142000        |
| train/                  |               |
|    approx_kl            | 0.00027906883 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.7          |
|    explained_variance   | 0.494         |
|    learning_rate        | 3.17e-05      |
|    loss                 | 0.141         |
|    n_updates            | 690           |
|    policy_gradient_loss | -8.89e-05     |
|    std                  | 0.932         |
|    value_loss           | 0.171         |
-------------------------------------------
box reached target
Eval num_timesteps=143000, episode_reward=-0.58 +/- 0.79
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.583   |
| time/              |          |
|    total_timesteps | 143000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 601    |
|    iterations      | 70     |
|    time_elapsed    | 238    |
|    total_timesteps | 143360 |
-------------------------------
Eval num_timesteps=144000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -1          |
| time/                   |             |
|    total_timesteps      | 144000      |
| train/                  |             |
|    approx_kl            | 0.004056977 |
|    clip_fraction        | 0.0106      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.7        |
|    explained_variance   | 0.147       |
|    learning_rate        | 3.17e-05    |
|    loss                 | 0.09        |
|    n_updates            | 700         |
|    policy_gradient_loss | -0.00241    |
|    std                  | 0.93        |
|    value_loss           | 0.121       |
-----------------------------------------
box reached target
Eval num_timesteps=145000, episode_reward=0.85 +/- 2.30
Episode length: 277.00 +/- 46.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 277      |
|    mean_reward     | 0.855    |
| time/              |          |
|    total_timesteps | 145000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 601    |
|    iterations      | 71     |
|    time_elapsed    | 241    |
|    total_timesteps | 145408 |
-------------------------------
Eval num_timesteps=146000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -1          |
| time/                   |             |
|    total_timesteps      | 146000      |
| train/                  |             |
|    approx_kl            | 0.004403876 |
|    clip_fraction        | 0.0263      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.69       |
|    explained_variance   | 0.785       |
|    learning_rate        | 3.17e-05    |
|    loss                 | -0.0152     |
|    n_updates            | 710         |
|    policy_gradient_loss | -0.00609    |
|    std                  | 0.928       |
|    value_loss           | 0.00303     |
-----------------------------------------
box reached target
Eval num_timesteps=147000, episode_reward=-0.94 +/- 0.13
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.935   |
| time/              |          |
|    total_timesteps | 147000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 601    |
|    iterations      | 72     |
|    time_elapsed    | 245    |
|    total_timesteps | 147456 |
-------------------------------
Eval num_timesteps=148000, episode_reward=-0.87 +/- 0.27
Episode length: 300.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 300           |
|    mean_reward          | -0.866        |
| time/                   |               |
|    total_timesteps      | 148000        |
| train/                  |               |
|    approx_kl            | 0.00040004504 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.69         |
|    explained_variance   | -0.142        |
|    learning_rate        | 3.18e-05      |
|    loss                 | 0.153         |
|    n_updates            | 720           |
|    policy_gradient_loss | -0.00016      |
|    std                  | 0.927         |
|    value_loss           | 0.0995        |
-------------------------------------------
Eval num_timesteps=149000, episode_reward=-0.47 +/- 0.70
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.467   |
| time/              |          |
|    total_timesteps | 149000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 601    |
|    iterations      | 73     |
|    time_elapsed    | 248    |
|    total_timesteps | 149504 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=150000, episode_reward=1.97 +/- 2.84
Episode length: 257.80 +/- 52.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 258          |
|    mean_reward          | 1.97         |
| time/                   |              |
|    total_timesteps      | 150000       |
| train/                  |              |
|    approx_kl            | 0.0016924542 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.68        |
|    explained_variance   | 0.834        |
|    learning_rate        | 3.18e-05     |
|    loss                 | -0.0167      |
|    n_updates            | 730          |
|    policy_gradient_loss | -0.00186     |
|    std                  | 0.926        |
|    value_loss           | 0.00475      |
------------------------------------------
box reached target
Eval num_timesteps=151000, episode_reward=0.29 +/- 2.57
Episode length: 280.80 +/- 38.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 281      |
|    mean_reward     | 0.286    |
| time/              |          |
|    total_timesteps | 151000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 601    |
|    iterations      | 74     |
|    time_elapsed    | 251    |
|    total_timesteps | 151552 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=152000, episode_reward=0.41 +/- 2.73
Episode length: 299.60 +/- 0.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | 0.412        |
| time/                   |              |
|    total_timesteps      | 152000       |
| train/                  |              |
|    approx_kl            | 0.0031612022 |
|    clip_fraction        | 0.017        |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.68        |
|    explained_variance   | 0.77         |
|    learning_rate        | 3.18e-05     |
|    loss                 | -0.000917    |
|    n_updates            | 740          |
|    policy_gradient_loss | -0.0029      |
|    std                  | 0.926        |
|    value_loss           | 0.00258      |
------------------------------------------
box reached target
Eval num_timesteps=153000, episode_reward=1.20 +/- 2.28
Episode length: 295.40 +/- 9.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 295      |
|    mean_reward     | 1.2      |
| time/              |          |
|    total_timesteps | 153000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 601    |
|    iterations      | 75     |
|    time_elapsed    | 255    |
|    total_timesteps | 153600 |
-------------------------------
box reached target
Eval num_timesteps=154000, episode_reward=0.76 +/- 2.29
Episode length: 280.80 +/- 38.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 281          |
|    mean_reward          | 0.761        |
| time/                   |              |
|    total_timesteps      | 154000       |
| train/                  |              |
|    approx_kl            | 0.0013269879 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.68        |
|    explained_variance   | 0.327        |
|    learning_rate        | 3.18e-05     |
|    loss                 | 0.0179       |
|    n_updates            | 750          |
|    policy_gradient_loss | -0.000934    |
|    std                  | 0.924        |
|    value_loss           | 0.0726       |
------------------------------------------
Eval num_timesteps=155000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 155000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 602    |
|    iterations      | 76     |
|    time_elapsed    | 258    |
|    total_timesteps | 155648 |
-------------------------------
Eval num_timesteps=156000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -1           |
| time/                   |              |
|    total_timesteps      | 156000       |
| train/                  |              |
|    approx_kl            | 0.0010642504 |
|    clip_fraction        | 0.00239      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.68        |
|    explained_variance   | 0.848        |
|    learning_rate        | 3.19e-05     |
|    loss                 | 0.00462      |
|    n_updates            | 760          |
|    policy_gradient_loss | -0.00114     |
|    std                  | 0.923        |
|    value_loss           | 0.00166      |
------------------------------------------
box reached target
Eval num_timesteps=157000, episode_reward=0.52 +/- 2.58
Episode length: 283.00 +/- 34.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 283      |
|    mean_reward     | 0.521    |
| time/              |          |
|    total_timesteps | 157000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 602    |
|    iterations      | 77     |
|    time_elapsed    | 261    |
|    total_timesteps | 157696 |
-------------------------------
Eval num_timesteps=158000, episode_reward=-0.47 +/- 0.65
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.474       |
| time/                   |              |
|    total_timesteps      | 158000       |
| train/                  |              |
|    approx_kl            | 0.0021761996 |
|    clip_fraction        | 0.00293      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.68        |
|    explained_variance   | 0.848        |
|    learning_rate        | 3.19e-05     |
|    loss                 | 0.0044       |
|    n_updates            | 770          |
|    policy_gradient_loss | -0.00229     |
|    std                  | 0.923        |
|    value_loss           | 0.00332      |
------------------------------------------
Eval num_timesteps=159000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 159000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 602    |
|    iterations      | 78     |
|    time_elapsed    | 265    |
|    total_timesteps | 159744 |
-------------------------------
Eval num_timesteps=160000, episode_reward=-0.77 +/- 0.46
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.772      |
| time/                   |             |
|    total_timesteps      | 160000      |
| train/                  |             |
|    approx_kl            | 0.000887291 |
|    clip_fraction        | 0.000342    |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.68       |
|    explained_variance   | 0.623       |
|    learning_rate        | 3.19e-05    |
|    loss                 | 0.0841      |
|    n_updates            | 780         |
|    policy_gradient_loss | -0.000517   |
|    std                  | 0.923       |
|    value_loss           | 0.0572      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=161000, episode_reward=2.03 +/- 2.87
Episode length: 275.40 +/- 33.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 275      |
|    mean_reward     | 2.03     |
| time/              |          |
|    total_timesteps | 161000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 602    |
|    iterations      | 79     |
|    time_elapsed    | 268    |
|    total_timesteps | 161792 |
-------------------------------
Eval num_timesteps=162000, episode_reward=-0.68 +/- 0.64
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.68        |
| time/                   |              |
|    total_timesteps      | 162000       |
| train/                  |              |
|    approx_kl            | 0.0048208656 |
|    clip_fraction        | 0.0114       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.68        |
|    explained_variance   | 0.933        |
|    learning_rate        | 3.19e-05     |
|    loss                 | -0.0216      |
|    n_updates            | 790          |
|    policy_gradient_loss | -0.00434     |
|    std                  | 0.923        |
|    value_loss           | 0.00288      |
------------------------------------------
box reached target
Eval num_timesteps=163000, episode_reward=0.52 +/- 2.56
Episode length: 296.00 +/- 8.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 296      |
|    mean_reward     | 0.515    |
| time/              |          |
|    total_timesteps | 163000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 602    |
|    iterations      | 80     |
|    time_elapsed    | 272    |
|    total_timesteps | 163840 |
-------------------------------
Eval num_timesteps=164000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -1           |
| time/                   |              |
|    total_timesteps      | 164000       |
| train/                  |              |
|    approx_kl            | 0.0034080255 |
|    clip_fraction        | 0.0139       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.68        |
|    explained_variance   | 0.772        |
|    learning_rate        | 3.2e-05      |
|    loss                 | -0.000119    |
|    n_updates            | 800          |
|    policy_gradient_loss | -0.00295     |
|    std                  | 0.921        |
|    value_loss           | 0.0013       |
------------------------------------------
Eval num_timesteps=165000, episode_reward=-0.30 +/- 0.61
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.296   |
| time/              |          |
|    total_timesteps | 165000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 602    |
|    iterations      | 81     |
|    time_elapsed    | 275    |
|    total_timesteps | 165888 |
-------------------------------
Eval num_timesteps=166000, episode_reward=-0.27 +/- 0.75
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.265       |
| time/                   |              |
|    total_timesteps      | 166000       |
| train/                  |              |
|    approx_kl            | 0.0038549355 |
|    clip_fraction        | 0.0146       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.67        |
|    explained_variance   | 0.915        |
|    learning_rate        | 3.2e-05      |
|    loss                 | 0.00965      |
|    n_updates            | 810          |
|    policy_gradient_loss | -0.00326     |
|    std                  | 0.921        |
|    value_loss           | 0.00157      |
------------------------------------------
Eval num_timesteps=167000, episode_reward=-0.75 +/- 0.50
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.749   |
| time/              |          |
|    total_timesteps | 167000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 602    |
|    iterations      | 82     |
|    time_elapsed    | 278    |
|    total_timesteps | 167936 |
-------------------------------
Eval num_timesteps=168000, episode_reward=-0.66 +/- 0.49
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.658       |
| time/                   |              |
|    total_timesteps      | 168000       |
| train/                  |              |
|    approx_kl            | 0.0034444835 |
|    clip_fraction        | 0.00459      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.67        |
|    explained_variance   | 0.868        |
|    learning_rate        | 3.2e-05      |
|    loss                 | 0.00827      |
|    n_updates            | 820          |
|    policy_gradient_loss | -0.00322     |
|    std                  | 0.921        |
|    value_loss           | 0.00143      |
------------------------------------------
Eval num_timesteps=169000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 169000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 602    |
|    iterations      | 83     |
|    time_elapsed    | 282    |
|    total_timesteps | 169984 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=170000, episode_reward=1.56 +/- 3.14
Episode length: 267.40 +/- 40.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 267          |
|    mean_reward          | 1.56         |
| time/                   |              |
|    total_timesteps      | 170000       |
| train/                  |              |
|    approx_kl            | 0.0042272345 |
|    clip_fraction        | 0.0317       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.67        |
|    explained_variance   | 0.906        |
|    learning_rate        | 3.2e-05      |
|    loss                 | -0.0205      |
|    n_updates            | 830          |
|    policy_gradient_loss | -0.00447     |
|    std                  | 0.918        |
|    value_loss           | 0.00347      |
------------------------------------------
Eval num_timesteps=171000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 171000   |
---------------------------------
Eval num_timesteps=172000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 172000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 601    |
|    iterations      | 84     |
|    time_elapsed    | 286    |
|    total_timesteps | 172032 |
-------------------------------
box reached target
Eval num_timesteps=173000, episode_reward=-0.74 +/- 0.36
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.743      |
| time/                   |             |
|    total_timesteps      | 173000      |
| train/                  |             |
|    approx_kl            | 0.004638492 |
|    clip_fraction        | 0.0217      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.67       |
|    explained_variance   | 0.898       |
|    learning_rate        | 3.21e-05    |
|    loss                 | 0.00048     |
|    n_updates            | 840         |
|    policy_gradient_loss | -0.0029     |
|    std                  | 0.92        |
|    value_loss           | 0.00136     |
-----------------------------------------
box reached target
Eval num_timesteps=174000, episode_reward=0.39 +/- 2.39
Episode length: 275.40 +/- 49.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 275      |
|    mean_reward     | 0.395    |
| time/              |          |
|    total_timesteps | 174000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 601    |
|    iterations      | 85     |
|    time_elapsed    | 289    |
|    total_timesteps | 174080 |
-------------------------------
box reached target
Eval num_timesteps=175000, episode_reward=0.58 +/- 2.39
Episode length: 275.40 +/- 49.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 275          |
|    mean_reward          | 0.582        |
| time/                   |              |
|    total_timesteps      | 175000       |
| train/                  |              |
|    approx_kl            | 0.0024934378 |
|    clip_fraction        | 0.00449      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.67        |
|    explained_variance   | 0.555        |
|    learning_rate        | 3.21e-05     |
|    loss                 | 0.0114       |
|    n_updates            | 850          |
|    policy_gradient_loss | -0.00286     |
|    std                  | 0.918        |
|    value_loss           | 0.0481       |
------------------------------------------
box reached target
Eval num_timesteps=176000, episode_reward=0.27 +/- 2.55
Episode length: 274.20 +/- 51.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 274      |
|    mean_reward     | 0.273    |
| time/              |          |
|    total_timesteps | 176000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 601    |
|    iterations      | 86     |
|    time_elapsed    | 292    |
|    total_timesteps | 176128 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=177000, episode_reward=1.48 +/- 3.04
Episode length: 257.60 +/- 52.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 258          |
|    mean_reward          | 1.48         |
| time/                   |              |
|    total_timesteps      | 177000       |
| train/                  |              |
|    approx_kl            | 0.0028037913 |
|    clip_fraction        | 0.00581      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.67        |
|    explained_variance   | 0.761        |
|    learning_rate        | 3.21e-05     |
|    loss                 | 0.00171      |
|    n_updates            | 860          |
|    policy_gradient_loss | -0.00199     |
|    std                  | 0.917        |
|    value_loss           | 0.0034       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=178000, episode_reward=0.59 +/- 2.40
Episode length: 277.40 +/- 45.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 277      |
|    mean_reward     | 0.585    |
| time/              |          |
|    total_timesteps | 178000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 601    |
|    iterations      | 87     |
|    time_elapsed    | 296    |
|    total_timesteps | 178176 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=179000, episode_reward=1.56 +/- 3.14
Episode length: 251.00 +/- 60.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 251         |
|    mean_reward          | 1.56        |
| time/                   |             |
|    total_timesteps      | 179000      |
| train/                  |             |
|    approx_kl            | 0.001913659 |
|    clip_fraction        | 0.000684    |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.66       |
|    explained_variance   | -0.277      |
|    learning_rate        | 3.21e-05    |
|    loss                 | 0.0566      |
|    n_updates            | 870         |
|    policy_gradient_loss | -0.00127    |
|    std                  | 0.914       |
|    value_loss           | 0.101       |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=180000, episode_reward=2.73 +/- 3.05
Episode length: 235.20 +/- 53.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 235      |
|    mean_reward     | 2.73     |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 602    |
|    iterations      | 88     |
|    time_elapsed    | 299    |
|    total_timesteps | 180224 |
-------------------------------
box reached target
Eval num_timesteps=181000, episode_reward=-0.48 +/- 0.45
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.476       |
| time/                   |              |
|    total_timesteps      | 181000       |
| train/                  |              |
|    approx_kl            | 0.0039611207 |
|    clip_fraction        | 0.00947      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.66        |
|    explained_variance   | 0.878        |
|    learning_rate        | 3.22e-05     |
|    loss                 | -0.00933     |
|    n_updates            | 880          |
|    policy_gradient_loss | -0.00423     |
|    std                  | 0.913        |
|    value_loss           | 0.00069      |
------------------------------------------
Eval num_timesteps=182000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 182000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 602    |
|    iterations      | 89     |
|    time_elapsed    | 302    |
|    total_timesteps | 182272 |
-------------------------------
Eval num_timesteps=183000, episode_reward=-0.43 +/- 0.71
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.425       |
| time/                   |              |
|    total_timesteps      | 183000       |
| train/                  |              |
|    approx_kl            | 0.0022539054 |
|    clip_fraction        | 0.00469      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.65        |
|    explained_variance   | 0.585        |
|    learning_rate        | 3.22e-05     |
|    loss                 | -0.0181      |
|    n_updates            | 890          |
|    policy_gradient_loss | -0.00287     |
|    std                  | 0.911        |
|    value_loss           | 0.0478       |
------------------------------------------
Eval num_timesteps=184000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 184000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 602    |
|    iterations      | 90     |
|    time_elapsed    | 306    |
|    total_timesteps | 184320 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=185000, episode_reward=1.53 +/- 3.09
Episode length: 258.20 +/- 51.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 258          |
|    mean_reward          | 1.53         |
| time/                   |              |
|    total_timesteps      | 185000       |
| train/                  |              |
|    approx_kl            | 0.0050354516 |
|    clip_fraction        | 0.0206       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.65        |
|    explained_variance   | 0.762        |
|    learning_rate        | 3.22e-05     |
|    loss                 | 0.024        |
|    n_updates            | 900          |
|    policy_gradient_loss | -0.00467     |
|    std                  | 0.91         |
|    value_loss           | 0.00262      |
------------------------------------------
box reached target
Eval num_timesteps=186000, episode_reward=0.27 +/- 2.53
Episode length: 277.00 +/- 46.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 277      |
|    mean_reward     | 0.266    |
| time/              |          |
|    total_timesteps | 186000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 602    |
|    iterations      | 91     |
|    time_elapsed    | 309    |
|    total_timesteps | 186368 |
-------------------------------
Eval num_timesteps=187000, episode_reward=-0.90 +/- 0.20
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.902       |
| time/                   |              |
|    total_timesteps      | 187000       |
| train/                  |              |
|    approx_kl            | 0.0017354115 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.65        |
|    explained_variance   | 0.496        |
|    learning_rate        | 3.22e-05     |
|    loss                 | 0.0123       |
|    n_updates            | 910          |
|    policy_gradient_loss | -0.00149     |
|    std                  | 0.91         |
|    value_loss           | 0.067        |
------------------------------------------
Eval num_timesteps=188000, episode_reward=-0.73 +/- 0.54
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.731   |
| time/              |          |
|    total_timesteps | 188000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 602    |
|    iterations      | 92     |
|    time_elapsed    | 312    |
|    total_timesteps | 188416 |
-------------------------------
box reached target
Eval num_timesteps=189000, episode_reward=0.26 +/- 2.52
Episode length: 272.40 +/- 55.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 272         |
|    mean_reward          | 0.258       |
| time/                   |             |
|    total_timesteps      | 189000      |
| train/                  |             |
|    approx_kl            | 0.005602218 |
|    clip_fraction        | 0.0251      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.65       |
|    explained_variance   | 0.837       |
|    learning_rate        | 3.23e-05    |
|    loss                 | -0.0141     |
|    n_updates            | 920         |
|    policy_gradient_loss | -0.00584    |
|    std                  | 0.909       |
|    value_loss           | 0.00125     |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=190000, episode_reward=1.74 +/- 2.87
Episode length: 253.00 +/- 57.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 253      |
|    mean_reward     | 1.74     |
| time/              |          |
|    total_timesteps | 190000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 602    |
|    iterations      | 93     |
|    time_elapsed    | 315    |
|    total_timesteps | 190464 |
-------------------------------
Eval num_timesteps=191000, episode_reward=-0.73 +/- 0.54
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.732       |
| time/                   |              |
|    total_timesteps      | 191000       |
| train/                  |              |
|    approx_kl            | 0.0019193428 |
|    clip_fraction        | 0.0022       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.65        |
|    explained_variance   | 0.799        |
|    learning_rate        | 3.23e-05     |
|    loss                 | 0.00592      |
|    n_updates            | 930          |
|    policy_gradient_loss | -0.00168     |
|    std                  | 0.908        |
|    value_loss           | 0.0018       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=192000, episode_reward=0.23 +/- 2.46
Episode length: 276.20 +/- 47.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 276      |
|    mean_reward     | 0.23     |
| time/              |          |
|    total_timesteps | 192000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 602    |
|    iterations      | 94     |
|    time_elapsed    | 319    |
|    total_timesteps | 192512 |
-------------------------------
Eval num_timesteps=193000, episode_reward=-0.80 +/- 0.40
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.801       |
| time/                   |              |
|    total_timesteps      | 193000       |
| train/                  |              |
|    approx_kl            | 0.0014454455 |
|    clip_fraction        | 0.000928     |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.64        |
|    explained_variance   | -0.000147    |
|    learning_rate        | 3.23e-05     |
|    loss                 | 0.0336       |
|    n_updates            | 940          |
|    policy_gradient_loss | -0.00131     |
|    std                  | 0.906        |
|    value_loss           | 0.0871       |
------------------------------------------
box reached target
Eval num_timesteps=194000, episode_reward=-0.69 +/- 0.62
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.69    |
| time/              |          |
|    total_timesteps | 194000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 602    |
|    iterations      | 95     |
|    time_elapsed    | 322    |
|    total_timesteps | 194560 |
-------------------------------
Eval num_timesteps=195000, episode_reward=-0.69 +/- 0.63
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.686       |
| time/                   |              |
|    total_timesteps      | 195000       |
| train/                  |              |
|    approx_kl            | 0.0014739747 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.64        |
|    explained_variance   | 0.568        |
|    learning_rate        | 3.23e-05     |
|    loss                 | 0.00474      |
|    n_updates            | 950          |
|    policy_gradient_loss | -0.000822    |
|    std                  | 0.904        |
|    value_loss           | 0.0588       |
------------------------------------------
Eval num_timesteps=196000, episode_reward=-0.96 +/- 0.08
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.962   |
| time/              |          |
|    total_timesteps | 196000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 602    |
|    iterations      | 96     |
|    time_elapsed    | 326    |
|    total_timesteps | 196608 |
-------------------------------
box reached target
Eval num_timesteps=197000, episode_reward=0.61 +/- 2.56
Episode length: 291.60 +/- 16.80
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 292        |
|    mean_reward          | 0.611      |
| time/                   |            |
|    total_timesteps      | 197000     |
| train/                  |            |
|    approx_kl            | 0.00353625 |
|    clip_fraction        | 0.00786    |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.64      |
|    explained_variance   | 0.839      |
|    learning_rate        | 3.24e-05   |
|    loss                 | -0.0157    |
|    n_updates            | 960        |
|    policy_gradient_loss | -0.00328   |
|    std                  | 0.905      |
|    value_loss           | 0.00109    |
----------------------------------------
box reached target
Eval num_timesteps=198000, episode_reward=0.94 +/- 2.29
Episode length: 282.40 +/- 35.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 282      |
|    mean_reward     | 0.945    |
| time/              |          |
|    total_timesteps | 198000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 603    |
|    iterations      | 97     |
|    time_elapsed    | 329    |
|    total_timesteps | 198656 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=199000, episode_reward=0.29 +/- 2.58
Episode length: 281.40 +/- 37.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 281          |
|    mean_reward          | 0.289        |
| time/                   |              |
|    total_timesteps      | 199000       |
| train/                  |              |
|    approx_kl            | 0.0049601113 |
|    clip_fraction        | 0.0355       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.64        |
|    explained_variance   | 0.703        |
|    learning_rate        | 3.24e-05     |
|    loss                 | -0.0397      |
|    n_updates            | 970          |
|    policy_gradient_loss | -0.00565     |
|    std                  | 0.904        |
|    value_loss           | 0.00165      |
------------------------------------------
box reached target
Eval num_timesteps=200000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 200000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 603    |
|    iterations      | 98     |
|    time_elapsed    | 332    |
|    total_timesteps | 200704 |
-------------------------------
Eval num_timesteps=201000, episode_reward=-0.81 +/- 0.38
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.81        |
| time/                   |              |
|    total_timesteps      | 201000       |
| train/                  |              |
|    approx_kl            | 0.0041348897 |
|    clip_fraction        | 0.012        |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.63        |
|    explained_variance   | 0.381        |
|    learning_rate        | 3.24e-05     |
|    loss                 | 0.129        |
|    n_updates            | 980          |
|    policy_gradient_loss | -0.00135     |
|    std                  | 0.901        |
|    value_loss           | 0.113        |
------------------------------------------
box reached target
Eval num_timesteps=202000, episode_reward=0.49 +/- 2.43
Episode length: 273.40 +/- 53.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 273      |
|    mean_reward     | 0.491    |
| time/              |          |
|    total_timesteps | 202000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 603    |
|    iterations      | 99     |
|    time_elapsed    | 336    |
|    total_timesteps | 202752 |
-------------------------------
Eval num_timesteps=203000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -1          |
| time/                   |             |
|    total_timesteps      | 203000      |
| train/                  |             |
|    approx_kl            | 0.003033591 |
|    clip_fraction        | 0.0102      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.63       |
|    explained_variance   | 0.843       |
|    learning_rate        | 3.24e-05    |
|    loss                 | 0.00717     |
|    n_updates            | 990         |
|    policy_gradient_loss | -0.00391    |
|    std                  | 0.899       |
|    value_loss           | 0.00691     |
-----------------------------------------
box reached target
Eval num_timesteps=204000, episode_reward=-0.68 +/- 0.39
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.681   |
| time/              |          |
|    total_timesteps | 204000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 603    |
|    iterations      | 100    |
|    time_elapsed    | 339    |
|    total_timesteps | 204800 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=205000, episode_reward=1.75 +/- 2.93
Episode length: 244.80 +/- 67.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 245          |
|    mean_reward          | 1.75         |
| time/                   |              |
|    total_timesteps      | 205000       |
| train/                  |              |
|    approx_kl            | 0.0030544093 |
|    clip_fraction        | 0.00444      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.62        |
|    explained_variance   | 0.344        |
|    learning_rate        | 3.25e-05     |
|    loss                 | 0.00254      |
|    n_updates            | 1000         |
|    policy_gradient_loss | -0.00146     |
|    std                  | 0.896        |
|    value_loss           | 0.0647       |
------------------------------------------
box reached target
Eval num_timesteps=206000, episode_reward=-0.67 +/- 0.66
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.669   |
| time/              |          |
|    total_timesteps | 206000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 603    |
|    iterations      | 101    |
|    time_elapsed    | 342    |
|    total_timesteps | 206848 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=207000, episode_reward=1.58 +/- 3.16
Episode length: 255.60 +/- 55.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 256          |
|    mean_reward          | 1.58         |
| time/                   |              |
|    total_timesteps      | 207000       |
| train/                  |              |
|    approx_kl            | 0.0032499405 |
|    clip_fraction        | 0.00913      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.62        |
|    explained_variance   | 0.702        |
|    learning_rate        | 3.25e-05     |
|    loss                 | 0.0991       |
|    n_updates            | 1010         |
|    policy_gradient_loss | -0.00311     |
|    std                  | 0.896        |
|    value_loss           | 0.0436       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=208000, episode_reward=1.82 +/- 2.88
Episode length: 264.60 +/- 43.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 265      |
|    mean_reward     | 1.82     |
| time/              |          |
|    total_timesteps | 208000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 603    |
|    iterations      | 102    |
|    time_elapsed    | 346    |
|    total_timesteps | 208896 |
-------------------------------
Eval num_timesteps=209000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -1           |
| time/                   |              |
|    total_timesteps      | 209000       |
| train/                  |              |
|    approx_kl            | 0.0018097474 |
|    clip_fraction        | 0.00156      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.62        |
|    explained_variance   | 0.789        |
|    learning_rate        | 3.25e-05     |
|    loss                 | 0.0136       |
|    n_updates            | 1020         |
|    policy_gradient_loss | -0.0011      |
|    std                  | 0.896        |
|    value_loss           | 0.00139      |
------------------------------------------
box reached target
Eval num_timesteps=210000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 210000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 603    |
|    iterations      | 103    |
|    time_elapsed    | 349    |
|    total_timesteps | 210944 |
-------------------------------
Eval num_timesteps=211000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -1          |
| time/                   |             |
|    total_timesteps      | 211000      |
| train/                  |             |
|    approx_kl            | 0.003087902 |
|    clip_fraction        | 0.00879     |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.62       |
|    explained_variance   | 0.466       |
|    learning_rate        | 3.25e-05    |
|    loss                 | -0.00617    |
|    n_updates            | 1030        |
|    policy_gradient_loss | -0.00206    |
|    std                  | 0.896       |
|    value_loss           | 0.0667      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=212000, episode_reward=1.76 +/- 2.92
Episode length: 262.20 +/- 50.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 262      |
|    mean_reward     | 1.76     |
| time/              |          |
|    total_timesteps | 212000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 603    |
|    iterations      | 104    |
|    time_elapsed    | 352    |
|    total_timesteps | 212992 |
-------------------------------
box reached target
Eval num_timesteps=213000, episode_reward=0.27 +/- 2.54
Episode length: 269.80 +/- 60.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 270         |
|    mean_reward          | 0.268       |
| time/                   |             |
|    total_timesteps      | 213000      |
| train/                  |             |
|    approx_kl            | 0.003037492 |
|    clip_fraction        | 0.00635     |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.62       |
|    explained_variance   | -0.193      |
|    learning_rate        | 3.26e-05    |
|    loss                 | 0.0441      |
|    n_updates            | 1040        |
|    policy_gradient_loss | -0.00119    |
|    std                  | 0.895       |
|    value_loss           | 0.106       |
-----------------------------------------
box reached target
Eval num_timesteps=214000, episode_reward=-0.85 +/- 0.29
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.853   |
| time/              |          |
|    total_timesteps | 214000   |
---------------------------------
box reached target
Eval num_timesteps=215000, episode_reward=0.46 +/- 2.38
Episode length: 275.00 +/- 50.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 275      |
|    mean_reward     | 0.464    |
| time/              |          |
|    total_timesteps | 215000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 602    |
|    iterations      | 105    |
|    time_elapsed    | 356    |
|    total_timesteps | 215040 |
-------------------------------
box reached target
Eval num_timesteps=216000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -1          |
| time/                   |             |
|    total_timesteps      | 216000      |
| train/                  |             |
|    approx_kl            | 0.002685323 |
|    clip_fraction        | 0.00181     |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.61       |
|    explained_variance   | 0.579       |
|    learning_rate        | 3.26e-05    |
|    loss                 | -0.0235     |
|    n_updates            | 1050        |
|    policy_gradient_loss | -0.000919   |
|    std                  | 0.892       |
|    value_loss           | 0.0547      |
-----------------------------------------
box reached target
Eval num_timesteps=217000, episode_reward=-0.75 +/- 0.51
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.746   |
| time/              |          |
|    total_timesteps | 217000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 602    |
|    iterations      | 106    |
|    time_elapsed    | 360    |
|    total_timesteps | 217088 |
-------------------------------
Eval num_timesteps=218000, episode_reward=-0.73 +/- 0.53
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.733       |
| time/                   |              |
|    total_timesteps      | 218000       |
| train/                  |              |
|    approx_kl            | 0.0022433682 |
|    clip_fraction        | 0.00781      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.61        |
|    explained_variance   | 0.419        |
|    learning_rate        | 3.26e-05     |
|    loss                 | 0.124        |
|    n_updates            | 1060         |
|    policy_gradient_loss | -0.00254     |
|    std                  | 0.889        |
|    value_loss           | 0.113        |
------------------------------------------
box reached target
Eval num_timesteps=219000, episode_reward=0.59 +/- 2.48
Episode length: 275.20 +/- 49.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 275      |
|    mean_reward     | 0.589    |
| time/              |          |
|    total_timesteps | 219000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 602    |
|    iterations      | 107    |
|    time_elapsed    | 363    |
|    total_timesteps | 219136 |
-------------------------------
Eval num_timesteps=220000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -1          |
| time/                   |             |
|    total_timesteps      | 220000      |
| train/                  |             |
|    approx_kl            | 0.002636863 |
|    clip_fraction        | 0.0084      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.6        |
|    explained_variance   | 0.933       |
|    learning_rate        | 3.26e-05    |
|    loss                 | -0.0161     |
|    n_updates            | 1070        |
|    policy_gradient_loss | -0.00225    |
|    std                  | 0.888       |
|    value_loss           | 0.00157     |
-----------------------------------------
Eval num_timesteps=221000, episode_reward=-0.64 +/- 0.72
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.641   |
| time/              |          |
|    total_timesteps | 221000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 602    |
|    iterations      | 108    |
|    time_elapsed    | 366    |
|    total_timesteps | 221184 |
-------------------------------
box reached target
Eval num_timesteps=222000, episode_reward=0.27 +/- 2.54
Episode length: 276.00 +/- 48.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 276          |
|    mean_reward          | 0.269        |
| time/                   |              |
|    total_timesteps      | 222000       |
| train/                  |              |
|    approx_kl            | 0.0038929263 |
|    clip_fraction        | 0.026        |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.6         |
|    explained_variance   | 0.815        |
|    learning_rate        | 3.27e-05     |
|    loss                 | 0.0166       |
|    n_updates            | 1080         |
|    policy_gradient_loss | -0.00362     |
|    std                  | 0.888        |
|    value_loss           | 0.000605     |
------------------------------------------
Eval num_timesteps=223000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 223000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 603    |
|    iterations      | 109    |
|    time_elapsed    | 370    |
|    total_timesteps | 223232 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=224000, episode_reward=2.76 +/- 3.07
Episode length: 222.80 +/- 63.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 223          |
|    mean_reward          | 2.76         |
| time/                   |              |
|    total_timesteps      | 224000       |
| train/                  |              |
|    approx_kl            | 0.0044449433 |
|    clip_fraction        | 0.0215       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.6         |
|    explained_variance   | 0.933        |
|    learning_rate        | 3.27e-05     |
|    loss                 | -0.0189      |
|    n_updates            | 1090         |
|    policy_gradient_loss | -0.00394     |
|    std                  | 0.887        |
|    value_loss           | 0.00216      |
------------------------------------------
box reached target
Eval num_timesteps=225000, episode_reward=0.53 +/- 2.43
Episode length: 278.80 +/- 42.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 279      |
|    mean_reward     | 0.526    |
| time/              |          |
|    total_timesteps | 225000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 603    |
|    iterations      | 110    |
|    time_elapsed    | 373    |
|    total_timesteps | 225280 |
-------------------------------
box reached target
Eval num_timesteps=226000, episode_reward=0.25 +/- 2.50
Episode length: 273.40 +/- 53.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 273         |
|    mean_reward          | 0.248       |
| time/                   |             |
|    total_timesteps      | 226000      |
| train/                  |             |
|    approx_kl            | 0.003578105 |
|    clip_fraction        | 0.0158      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.6        |
|    explained_variance   | 0.766       |
|    learning_rate        | 3.27e-05    |
|    loss                 | -0.017      |
|    n_updates            | 1100        |
|    policy_gradient_loss | -0.00294    |
|    std                  | 0.886       |
|    value_loss           | 0.00136     |
-----------------------------------------
box reached target
Eval num_timesteps=227000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 227000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 603    |
|    iterations      | 111    |
|    time_elapsed    | 376    |
|    total_timesteps | 227328 |
-------------------------------
Eval num_timesteps=228000, episode_reward=-0.71 +/- 0.59
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.707       |
| time/                   |              |
|    total_timesteps      | 228000       |
| train/                  |              |
|    approx_kl            | 0.0020906474 |
|    clip_fraction        | 0.00376      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.6         |
|    explained_variance   | 0.589        |
|    learning_rate        | 3.27e-05     |
|    loss                 | -0.0102      |
|    n_updates            | 1110         |
|    policy_gradient_loss | -0.00185     |
|    std                  | 0.885        |
|    value_loss           | 0.0477       |
------------------------------------------
box reached target
Eval num_timesteps=229000, episode_reward=0.26 +/- 2.52
Episode length: 275.20 +/- 49.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 275      |
|    mean_reward     | 0.259    |
| time/              |          |
|    total_timesteps | 229000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 603    |
|    iterations      | 112    |
|    time_elapsed    | 380    |
|    total_timesteps | 229376 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=230000, episode_reward=0.30 +/- 2.60
Episode length: 272.20 +/- 55.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 272          |
|    mean_reward          | 0.3          |
| time/                   |              |
|    total_timesteps      | 230000       |
| train/                  |              |
|    approx_kl            | 0.0013845501 |
|    clip_fraction        | 0.0022       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.59        |
|    explained_variance   | 0.441        |
|    learning_rate        | 3.28e-05     |
|    loss                 | 0.000859     |
|    n_updates            | 1120         |
|    policy_gradient_loss | -0.00111     |
|    std                  | 0.884        |
|    value_loss           | 0.0732       |
------------------------------------------
box reached target
Eval num_timesteps=231000, episode_reward=0.65 +/- 2.56
Episode length: 282.00 +/- 36.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 282      |
|    mean_reward     | 0.651    |
| time/              |          |
|    total_timesteps | 231000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 603    |
|    iterations      | 113    |
|    time_elapsed    | 383    |
|    total_timesteps | 231424 |
-------------------------------
box reached target
Eval num_timesteps=232000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -1          |
| time/                   |             |
|    total_timesteps      | 232000      |
| train/                  |             |
|    approx_kl            | 0.004368754 |
|    clip_fraction        | 0.0185      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.59       |
|    explained_variance   | 0.723       |
|    learning_rate        | 3.28e-05    |
|    loss                 | 0.00417     |
|    n_updates            | 1130        |
|    policy_gradient_loss | -0.00202    |
|    std                  | 0.883       |
|    value_loss           | 0.0786      |
-----------------------------------------
box reached target
Eval num_timesteps=233000, episode_reward=0.22 +/- 2.44
Episode length: 277.20 +/- 45.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 277      |
|    mean_reward     | 0.218    |
| time/              |          |
|    total_timesteps | 233000   |
---------------------------------
box reached target
box reached target
-------------------------------
| time/              |        |
|    fps             | 603    |
|    iterations      | 114    |
|    time_elapsed    | 386    |
|    total_timesteps | 233472 |
-------------------------------
box reached target
Eval num_timesteps=234000, episode_reward=0.31 +/- 2.62
Episode length: 293.60 +/- 12.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 294         |
|    mean_reward          | 0.312       |
| time/                   |             |
|    total_timesteps      | 234000      |
| train/                  |             |
|    approx_kl            | 0.000812513 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.59       |
|    explained_variance   | 0.712       |
|    learning_rate        | 3.28e-05    |
|    loss                 | 0.00186     |
|    n_updates            | 1140        |
|    policy_gradient_loss | -0.000372   |
|    std                  | 0.881       |
|    value_loss           | 0.13        |
-----------------------------------------
Eval num_timesteps=235000, episode_reward=-0.84 +/- 0.33
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.837   |
| time/              |          |
|    total_timesteps | 235000   |
---------------------------------
box reached target
box reached target
-------------------------------
| time/              |        |
|    fps             | 603    |
|    iterations      | 115    |
|    time_elapsed    | 390    |
|    total_timesteps | 235520 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=236000, episode_reward=1.78 +/- 2.87
Episode length: 260.00 +/- 53.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 260         |
|    mean_reward          | 1.78        |
| time/                   |             |
|    total_timesteps      | 236000      |
| train/                  |             |
|    approx_kl            | 0.002339547 |
|    clip_fraction        | 0.0022      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.58       |
|    explained_variance   | 0.763       |
|    learning_rate        | 3.28e-05    |
|    loss                 | 0.0274      |
|    n_updates            | 1150        |
|    policy_gradient_loss | -0.00142    |
|    std                  | 0.881       |
|    value_loss           | 0.0759      |
-----------------------------------------
box reached target
Eval num_timesteps=237000, episode_reward=0.46 +/- 2.38
Episode length: 275.00 +/- 50.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 275      |
|    mean_reward     | 0.457    |
| time/              |          |
|    total_timesteps | 237000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 604    |
|    iterations      | 116    |
|    time_elapsed    | 393    |
|    total_timesteps | 237568 |
-------------------------------
Eval num_timesteps=238000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -1           |
| time/                   |              |
|    total_timesteps      | 238000       |
| train/                  |              |
|    approx_kl            | 0.0048196153 |
|    clip_fraction        | 0.0264       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.58        |
|    explained_variance   | 0.868        |
|    learning_rate        | 3.29e-05     |
|    loss                 | 0.00301      |
|    n_updates            | 1160         |
|    policy_gradient_loss | -0.00431     |
|    std                  | 0.881        |
|    value_loss           | 0.00207      |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=239000, episode_reward=0.43 +/- 2.45
Episode length: 278.00 +/- 44.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | 0.429    |
| time/              |          |
|    total_timesteps | 239000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 604    |
|    iterations      | 117    |
|    time_elapsed    | 396    |
|    total_timesteps | 239616 |
-------------------------------
box reached target
Eval num_timesteps=240000, episode_reward=0.24 +/- 2.49
Episode length: 272.80 +/- 54.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 273          |
|    mean_reward          | 0.244        |
| time/                   |              |
|    total_timesteps      | 240000       |
| train/                  |              |
|    approx_kl            | 0.0043999525 |
|    clip_fraction        | 0.00679      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.58        |
|    explained_variance   | 0.734        |
|    learning_rate        | 3.29e-05     |
|    loss                 | -0.00956     |
|    n_updates            | 1170         |
|    policy_gradient_loss | -0.00234     |
|    std                  | 0.88         |
|    value_loss           | 0.0721       |
------------------------------------------
box reached target
Eval num_timesteps=241000, episode_reward=-0.25 +/- 0.73
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.249   |
| time/              |          |
|    total_timesteps | 241000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 604    |
|    iterations      | 118    |
|    time_elapsed    | 399    |
|    total_timesteps | 241664 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=242000, episode_reward=2.85 +/- 3.15
Episode length: 244.80 +/- 53.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 245         |
|    mean_reward          | 2.85        |
| time/                   |             |
|    total_timesteps      | 242000      |
| train/                  |             |
|    approx_kl            | 0.002332041 |
|    clip_fraction        | 0.00322     |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.58       |
|    explained_variance   | 0.622       |
|    learning_rate        | 3.29e-05    |
|    loss                 | 0.0044      |
|    n_updates            | 1180        |
|    policy_gradient_loss | -0.00152    |
|    std                  | 0.877       |
|    value_loss           | 0.0458      |
-----------------------------------------
New best mean reward!
Eval num_timesteps=243000, episode_reward=-0.67 +/- 0.67
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.667   |
| time/              |          |
|    total_timesteps | 243000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 604    |
|    iterations      | 119    |
|    time_elapsed    | 403    |
|    total_timesteps | 243712 |
-------------------------------
box reached target
Eval num_timesteps=244000, episode_reward=0.51 +/- 2.41
Episode length: 277.60 +/- 44.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 278          |
|    mean_reward          | 0.506        |
| time/                   |              |
|    total_timesteps      | 244000       |
| train/                  |              |
|    approx_kl            | 0.0036637103 |
|    clip_fraction        | 0.0112       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.58        |
|    explained_variance   | 0.881        |
|    learning_rate        | 3.29e-05     |
|    loss                 | -0.00601     |
|    n_updates            | 1190         |
|    policy_gradient_loss | -0.00234     |
|    std                  | 0.878        |
|    value_loss           | 0.00666      |
------------------------------------------
box reached target
Eval num_timesteps=245000, episode_reward=-0.39 +/- 0.75
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.39    |
| time/              |          |
|    total_timesteps | 245000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 604    |
|    iterations      | 120    |
|    time_elapsed    | 406    |
|    total_timesteps | 245760 |
-------------------------------
Eval num_timesteps=246000, episode_reward=-0.77 +/- 0.45
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.773       |
| time/                   |              |
|    total_timesteps      | 246000       |
| train/                  |              |
|    approx_kl            | 0.0036963015 |
|    clip_fraction        | 0.00664      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.58        |
|    explained_variance   | 0.827        |
|    learning_rate        | 3.29e-05     |
|    loss                 | 0.00158      |
|    n_updates            | 1200         |
|    policy_gradient_loss | -0.00193     |
|    std                  | 0.877        |
|    value_loss           | 0.0268       |
------------------------------------------
box reached target
Eval num_timesteps=247000, episode_reward=-0.72 +/- 0.44
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.717   |
| time/              |          |
|    total_timesteps | 247000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 604    |
|    iterations      | 121    |
|    time_elapsed    | 409    |
|    total_timesteps | 247808 |
-------------------------------
Eval num_timesteps=248000, episode_reward=-0.91 +/- 0.18
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.909      |
| time/                   |             |
|    total_timesteps      | 248000      |
| train/                  |             |
|    approx_kl            | 0.005314308 |
|    clip_fraction        | 0.0222      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.57       |
|    explained_variance   | 0.851       |
|    learning_rate        | 3.3e-05     |
|    loss                 | -0.00282    |
|    n_updates            | 1210        |
|    policy_gradient_loss | -0.00383    |
|    std                  | 0.875       |
|    value_loss           | 0.0283      |
-----------------------------------------
box reached target
Eval num_timesteps=249000, episode_reward=0.26 +/- 2.51
Episode length: 271.60 +/- 56.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 272      |
|    mean_reward     | 0.257    |
| time/              |          |
|    total_timesteps | 249000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 604    |
|    iterations      | 122    |
|    time_elapsed    | 413    |
|    total_timesteps | 249856 |
-------------------------------
Eval num_timesteps=250000, episode_reward=-0.70 +/- 0.61
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.695       |
| time/                   |              |
|    total_timesteps      | 250000       |
| train/                  |              |
|    approx_kl            | 0.0010262086 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.57        |
|    explained_variance   | 0.743        |
|    learning_rate        | 3.3e-05      |
|    loss                 | -0.00296     |
|    n_updates            | 1220         |
|    policy_gradient_loss | -0.000941    |
|    std                  | 0.875        |
|    value_loss           | 0.00321      |
------------------------------------------
Eval num_timesteps=251000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 251000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 604    |
|    iterations      | 123    |
|    time_elapsed    | 416    |
|    total_timesteps | 251904 |
-------------------------------
Eval num_timesteps=252000, episode_reward=-0.76 +/- 0.48
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.761       |
| time/                   |              |
|    total_timesteps      | 252000       |
| train/                  |              |
|    approx_kl            | 0.0030136253 |
|    clip_fraction        | 0.0145       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.57        |
|    explained_variance   | 0.846        |
|    learning_rate        | 3.3e-05      |
|    loss                 | 0.023        |
|    n_updates            | 1230         |
|    policy_gradient_loss | -0.00281     |
|    std                  | 0.875        |
|    value_loss           | 0.0444       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=253000, episode_reward=0.33 +/- 2.46
Episode length: 276.80 +/- 46.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 277      |
|    mean_reward     | 0.331    |
| time/              |          |
|    total_timesteps | 253000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 604    |
|    iterations      | 124    |
|    time_elapsed    | 419    |
|    total_timesteps | 253952 |
-------------------------------
Eval num_timesteps=254000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -1          |
| time/                   |             |
|    total_timesteps      | 254000      |
| train/                  |             |
|    approx_kl            | 0.001923861 |
|    clip_fraction        | 0.00664     |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.57       |
|    explained_variance   | 0.927       |
|    learning_rate        | 3.3e-05     |
|    loss                 | -0.00185    |
|    n_updates            | 1240        |
|    policy_gradient_loss | -0.00234    |
|    std                  | 0.875       |
|    value_loss           | 0.0179      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=255000, episode_reward=1.58 +/- 3.16
Episode length: 252.80 +/- 58.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 253      |
|    mean_reward     | 1.58     |
| time/              |          |
|    total_timesteps | 255000   |
---------------------------------
Eval num_timesteps=256000, episode_reward=-0.77 +/- 0.45
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.774   |
| time/              |          |
|    total_timesteps | 256000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 604    |
|    iterations      | 125    |
|    time_elapsed    | 423    |
|    total_timesteps | 256000 |
-------------------------------
Eval num_timesteps=257000, episode_reward=-0.78 +/- 0.44
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.78       |
| time/                   |             |
|    total_timesteps      | 257000      |
| train/                  |             |
|    approx_kl            | 0.004769716 |
|    clip_fraction        | 0.0288      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.57       |
|    explained_variance   | 0.823       |
|    learning_rate        | 3.31e-05    |
|    loss                 | -0.00769    |
|    n_updates            | 1250        |
|    policy_gradient_loss | -0.00491    |
|    std                  | 0.876       |
|    value_loss           | 0.00601     |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=258000, episode_reward=0.23 +/- 2.46
Episode length: 274.60 +/- 50.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 275      |
|    mean_reward     | 0.232    |
| time/              |          |
|    total_timesteps | 258000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 604    |
|    iterations      | 126    |
|    time_elapsed    | 427    |
|    total_timesteps | 258048 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=259000, episode_reward=0.27 +/- 2.53
Episode length: 272.00 +/- 56.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 272        |
|    mean_reward          | 0.267      |
| time/                   |            |
|    total_timesteps      | 259000     |
| train/                  |            |
|    approx_kl            | 0.00187453 |
|    clip_fraction        | 0.00308    |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.57      |
|    explained_variance   | 0.412      |
|    learning_rate        | 3.31e-05   |
|    loss                 | 0.00246    |
|    n_updates            | 1260       |
|    policy_gradient_loss | -0.00133   |
|    std                  | 0.875      |
|    value_loss           | 0.0602     |
----------------------------------------
box reached target
Eval num_timesteps=260000, episode_reward=0.28 +/- 2.57
Episode length: 271.40 +/- 57.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 271      |
|    mean_reward     | 0.283    |
| time/              |          |
|    total_timesteps | 260000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 604    |
|    iterations      | 127    |
|    time_elapsed    | 430    |
|    total_timesteps | 260096 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=261000, episode_reward=1.61 +/- 3.20
Episode length: 267.20 +/- 46.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 267          |
|    mean_reward          | 1.61         |
| time/                   |              |
|    total_timesteps      | 261000       |
| train/                  |              |
|    approx_kl            | 0.0009565457 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.57        |
|    explained_variance   | 0.656        |
|    learning_rate        | 3.31e-05     |
|    loss                 | -6.36e-05    |
|    n_updates            | 1270         |
|    policy_gradient_loss | -0.000901    |
|    std                  | 0.873        |
|    value_loss           | 0.0464       |
------------------------------------------
box reached target
Eval num_timesteps=262000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 262000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 604    |
|    iterations      | 128    |
|    time_elapsed    | 433    |
|    total_timesteps | 262144 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=263000, episode_reward=0.26 +/- 2.51
Episode length: 274.60 +/- 50.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 275          |
|    mean_reward          | 0.257        |
| time/                   |              |
|    total_timesteps      | 263000       |
| train/                  |              |
|    approx_kl            | 0.0019063882 |
|    clip_fraction        | 0.00371      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.56        |
|    explained_variance   | 0.696        |
|    learning_rate        | 3.31e-05     |
|    loss                 | -0.00217     |
|    n_updates            | 1280         |
|    policy_gradient_loss | -0.00198     |
|    std                  | 0.872        |
|    value_loss           | 0.0427       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=264000, episode_reward=1.52 +/- 3.08
Episode length: 256.00 +/- 53.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 256      |
|    mean_reward     | 1.52     |
| time/              |          |
|    total_timesteps | 264000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 604    |
|    iterations      | 129    |
|    time_elapsed    | 436    |
|    total_timesteps | 264192 |
-------------------------------
Eval num_timesteps=265000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -1           |
| time/                   |              |
|    total_timesteps      | 265000       |
| train/                  |              |
|    approx_kl            | 0.0014907429 |
|    clip_fraction        | 0.00166      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.56        |
|    explained_variance   | 0.617        |
|    learning_rate        | 3.32e-05     |
|    loss                 | 0.00823      |
|    n_updates            | 1290         |
|    policy_gradient_loss | -0.00174     |
|    std                  | 0.87         |
|    value_loss           | 0.0973       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=266000, episode_reward=0.25 +/- 2.51
Episode length: 274.00 +/- 52.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 274      |
|    mean_reward     | 0.254    |
| time/              |          |
|    total_timesteps | 266000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 604    |
|    iterations      | 130    |
|    time_elapsed    | 440    |
|    total_timesteps | 266240 |
-------------------------------
box reached target
Eval num_timesteps=267000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -1           |
| time/                   |              |
|    total_timesteps      | 267000       |
| train/                  |              |
|    approx_kl            | 0.0024554532 |
|    clip_fraction        | 0.0021       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.56        |
|    explained_variance   | 0.922        |
|    learning_rate        | 3.32e-05     |
|    loss                 | 0.0223       |
|    n_updates            | 1300         |
|    policy_gradient_loss | -0.00164     |
|    std                  | 0.87         |
|    value_loss           | 0.0297       |
------------------------------------------
Eval num_timesteps=268000, episode_reward=-0.78 +/- 0.44
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.781   |
| time/              |          |
|    total_timesteps | 268000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 604    |
|    iterations      | 131    |
|    time_elapsed    | 443    |
|    total_timesteps | 268288 |
-------------------------------
box reached target
Eval num_timesteps=269000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -1           |
| time/                   |              |
|    total_timesteps      | 269000       |
| train/                  |              |
|    approx_kl            | 0.0019133782 |
|    clip_fraction        | 0.00122      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.56        |
|    explained_variance   | 0.442        |
|    learning_rate        | 3.32e-05     |
|    loss                 | 0.0673       |
|    n_updates            | 1310         |
|    policy_gradient_loss | -0.000822    |
|    std                  | 0.867        |
|    value_loss           | 0.0484       |
------------------------------------------
box reached target
Eval num_timesteps=270000, episode_reward=0.56 +/- 2.48
Episode length: 281.20 +/- 37.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 281      |
|    mean_reward     | 0.555    |
| time/              |          |
|    total_timesteps | 270000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 605    |
|    iterations      | 132    |
|    time_elapsed    | 446    |
|    total_timesteps | 270336 |
-------------------------------
box reached target
Eval num_timesteps=271000, episode_reward=1.15 +/- 2.15
Episode length: 271.40 +/- 57.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 271        |
|    mean_reward          | 1.15       |
| time/                   |            |
|    total_timesteps      | 271000     |
| train/                  |            |
|    approx_kl            | 0.00319418 |
|    clip_fraction        | 0.0122     |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.55      |
|    explained_variance   | 0.796      |
|    learning_rate        | 3.32e-05   |
|    loss                 | 0.00157    |
|    n_updates            | 1320       |
|    policy_gradient_loss | -0.00145   |
|    std                  | 0.864      |
|    value_loss           | 0.0631     |
----------------------------------------
box reached target
box reached target
Eval num_timesteps=272000, episode_reward=0.27 +/- 2.55
Episode length: 278.80 +/- 42.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 279      |
|    mean_reward     | 0.273    |
| time/              |          |
|    total_timesteps | 272000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 605    |
|    iterations      | 133    |
|    time_elapsed    | 450    |
|    total_timesteps | 272384 |
-------------------------------
Eval num_timesteps=273000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -1          |
| time/                   |             |
|    total_timesteps      | 273000      |
| train/                  |             |
|    approx_kl            | 0.003974339 |
|    clip_fraction        | 0.00991     |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.55       |
|    explained_variance   | 0.847       |
|    learning_rate        | 3.33e-05    |
|    loss                 | -0.0059     |
|    n_updates            | 1330        |
|    policy_gradient_loss | -0.002      |
|    std                  | 0.865       |
|    value_loss           | 0.0429      |
-----------------------------------------
box reached target
Eval num_timesteps=274000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 274000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 605    |
|    iterations      | 134    |
|    time_elapsed    | 453    |
|    total_timesteps | 274432 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=275000, episode_reward=1.81 +/- 3.08
Episode length: 267.80 +/- 48.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 268          |
|    mean_reward          | 1.81         |
| time/                   |              |
|    total_timesteps      | 275000       |
| train/                  |              |
|    approx_kl            | 0.0048364103 |
|    clip_fraction        | 0.0204       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.55        |
|    explained_variance   | 0.884        |
|    learning_rate        | 3.33e-05     |
|    loss                 | 0.0116       |
|    n_updates            | 1340         |
|    policy_gradient_loss | -0.00293     |
|    std                  | 0.864        |
|    value_loss           | 0.0289       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=276000, episode_reward=1.60 +/- 3.19
Episode length: 260.80 +/- 49.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 261      |
|    mean_reward     | 1.6      |
| time/              |          |
|    total_timesteps | 276000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 605    |
|    iterations      | 135    |
|    time_elapsed    | 456    |
|    total_timesteps | 276480 |
-------------------------------
box reached target
Eval num_timesteps=277000, episode_reward=0.44 +/- 2.41
Episode length: 274.20 +/- 51.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 274          |
|    mean_reward          | 0.44         |
| time/                   |              |
|    total_timesteps      | 277000       |
| train/                  |              |
|    approx_kl            | 0.0021460704 |
|    clip_fraction        | 0.00464      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.54        |
|    explained_variance   | 0.699        |
|    learning_rate        | 3.33e-05     |
|    loss                 | -0.00927     |
|    n_updates            | 1350         |
|    policy_gradient_loss | -0.00123     |
|    std                  | 0.863        |
|    value_loss           | 0.00639      |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=278000, episode_reward=0.28 +/- 2.56
Episode length: 287.40 +/- 25.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | 0.282    |
| time/              |          |
|    total_timesteps | 278000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 605    |
|    iterations      | 136    |
|    time_elapsed    | 460    |
|    total_timesteps | 278528 |
-------------------------------
Eval num_timesteps=279000, episode_reward=-0.70 +/- 0.59
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.704      |
| time/                   |             |
|    total_timesteps      | 279000      |
| train/                  |             |
|    approx_kl            | 0.004731171 |
|    clip_fraction        | 0.0331      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.54       |
|    explained_variance   | 0.658       |
|    learning_rate        | 3.33e-05    |
|    loss                 | 0.00604     |
|    n_updates            | 1360        |
|    policy_gradient_loss | -0.0038     |
|    std                  | 0.86        |
|    value_loss           | 0.0899      |
-----------------------------------------
box reached target
Eval num_timesteps=280000, episode_reward=0.27 +/- 2.54
Episode length: 273.40 +/- 53.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 273      |
|    mean_reward     | 0.271    |
| time/              |          |
|    total_timesteps | 280000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 605    |
|    iterations      | 137    |
|    time_elapsed    | 463    |
|    total_timesteps | 280576 |
-------------------------------
Eval num_timesteps=281000, episode_reward=-0.79 +/- 0.42
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.789       |
| time/                   |              |
|    total_timesteps      | 281000       |
| train/                  |              |
|    approx_kl            | 0.0040454804 |
|    clip_fraction        | 0.0252       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.53        |
|    explained_variance   | 0.891        |
|    learning_rate        | 3.34e-05     |
|    loss                 | 0.00562      |
|    n_updates            | 1370         |
|    policy_gradient_loss | -0.00234     |
|    std                  | 0.858        |
|    value_loss           | 0.0286       |
------------------------------------------
Eval num_timesteps=282000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 282000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 605    |
|    iterations      | 138    |
|    time_elapsed    | 466    |
|    total_timesteps | 282624 |
-------------------------------
Eval num_timesteps=283000, episode_reward=-0.46 +/- 0.69
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.455       |
| time/                   |              |
|    total_timesteps      | 283000       |
| train/                  |              |
|    approx_kl            | 0.0076000392 |
|    clip_fraction        | 0.0549       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.53        |
|    explained_variance   | 0.849        |
|    learning_rate        | 3.34e-05     |
|    loss                 | 0.0174       |
|    n_updates            | 1380         |
|    policy_gradient_loss | -0.00627     |
|    std                  | 0.858        |
|    value_loss           | 0.00183      |
------------------------------------------
Eval num_timesteps=284000, episode_reward=-0.96 +/- 0.08
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.958   |
| time/              |          |
|    total_timesteps | 284000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 605    |
|    iterations      | 139    |
|    time_elapsed    | 470    |
|    total_timesteps | 284672 |
-------------------------------
box reached target
Eval num_timesteps=285000, episode_reward=0.28 +/- 2.56
Episode length: 278.20 +/- 43.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 278          |
|    mean_reward          | 0.28         |
| time/                   |              |
|    total_timesteps      | 285000       |
| train/                  |              |
|    approx_kl            | 0.0015860527 |
|    clip_fraction        | 0.00103      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.53        |
|    explained_variance   | 0.653        |
|    learning_rate        | 3.34e-05     |
|    loss                 | 0.00818      |
|    n_updates            | 1390         |
|    policy_gradient_loss | -0.000528    |
|    std                  | 0.857        |
|    value_loss           | 0.0418       |
------------------------------------------
box reached target
Eval num_timesteps=286000, episode_reward=-0.79 +/- 0.41
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.793   |
| time/              |          |
|    total_timesteps | 286000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 605    |
|    iterations      | 140    |
|    time_elapsed    | 473    |
|    total_timesteps | 286720 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=287000, episode_reward=1.63 +/- 3.22
Episode length: 265.20 +/- 45.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 265          |
|    mean_reward          | 1.63         |
| time/                   |              |
|    total_timesteps      | 287000       |
| train/                  |              |
|    approx_kl            | 0.0028484883 |
|    clip_fraction        | 0.00366      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.53        |
|    explained_variance   | 0.913        |
|    learning_rate        | 3.34e-05     |
|    loss                 | 0.00121      |
|    n_updates            | 1400         |
|    policy_gradient_loss | -0.00162     |
|    std                  | 0.857        |
|    value_loss           | 0.0403       |
------------------------------------------
Eval num_timesteps=288000, episode_reward=-0.75 +/- 0.49
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.755   |
| time/              |          |
|    total_timesteps | 288000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 605    |
|    iterations      | 141    |
|    time_elapsed    | 476    |
|    total_timesteps | 288768 |
-------------------------------
Eval num_timesteps=289000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -1           |
| time/                   |              |
|    total_timesteps      | 289000       |
| train/                  |              |
|    approx_kl            | 0.0026022857 |
|    clip_fraction        | 0.0125       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.53        |
|    explained_variance   | 0.402        |
|    learning_rate        | 3.35e-05     |
|    loss                 | 0.0195       |
|    n_updates            | 1410         |
|    policy_gradient_loss | -0.00426     |
|    std                  | 0.855        |
|    value_loss           | 0.0554       |
------------------------------------------
box reached target
Eval num_timesteps=290000, episode_reward=0.29 +/- 2.57
Episode length: 274.60 +/- 50.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 275      |
|    mean_reward     | 0.287    |
| time/              |          |
|    total_timesteps | 290000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 605    |
|    iterations      | 142    |
|    time_elapsed    | 479    |
|    total_timesteps | 290816 |
-------------------------------
Eval num_timesteps=291000, episode_reward=-0.63 +/- 0.74
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.631       |
| time/                   |              |
|    total_timesteps      | 291000       |
| train/                  |              |
|    approx_kl            | 0.0024056593 |
|    clip_fraction        | 0.00479      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.52        |
|    explained_variance   | 0.874        |
|    learning_rate        | 3.35e-05     |
|    loss                 | -0.000976    |
|    n_updates            | 1420         |
|    policy_gradient_loss | -0.0017      |
|    std                  | 0.854        |
|    value_loss           | 0.00217      |
------------------------------------------
box reached target
Eval num_timesteps=292000, episode_reward=0.29 +/- 2.58
Episode length: 275.40 +/- 49.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 275      |
|    mean_reward     | 0.292    |
| time/              |          |
|    total_timesteps | 292000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 606    |
|    iterations      | 143    |
|    time_elapsed    | 483    |
|    total_timesteps | 292864 |
-------------------------------
Eval num_timesteps=293000, episode_reward=-0.43 +/- 0.55
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.427       |
| time/                   |              |
|    total_timesteps      | 293000       |
| train/                  |              |
|    approx_kl            | 0.0058321753 |
|    clip_fraction        | 0.0418       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.52        |
|    explained_variance   | 0.806        |
|    learning_rate        | 3.35e-05     |
|    loss                 | -0.0106      |
|    n_updates            | 1430         |
|    policy_gradient_loss | -0.00466     |
|    std                  | 0.855        |
|    value_loss           | 0.00244      |
------------------------------------------
Eval num_timesteps=294000, episode_reward=-0.49 +/- 0.65
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.493   |
| time/              |          |
|    total_timesteps | 294000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 606    |
|    iterations      | 144    |
|    time_elapsed    | 486    |
|    total_timesteps | 294912 |
-------------------------------
Eval num_timesteps=295000, episode_reward=-0.42 +/- 0.71
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.421       |
| time/                   |              |
|    total_timesteps      | 295000       |
| train/                  |              |
|    approx_kl            | 0.0026764376 |
|    clip_fraction        | 0.0234       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.52        |
|    explained_variance   | 0.932        |
|    learning_rate        | 3.35e-05     |
|    loss                 | 0.0128       |
|    n_updates            | 1440         |
|    policy_gradient_loss | -0.00356     |
|    std                  | 0.854        |
|    value_loss           | 0.00476      |
------------------------------------------
box reached target
Eval num_timesteps=296000, episode_reward=0.28 +/- 2.55
Episode length: 286.40 +/- 27.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 286      |
|    mean_reward     | 0.276    |
| time/              |          |
|    total_timesteps | 296000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 606    |
|    iterations      | 145    |
|    time_elapsed    | 489    |
|    total_timesteps | 296960 |
-------------------------------
box reached target
Eval num_timesteps=297000, episode_reward=0.76 +/- 2.47
Episode length: 282.20 +/- 35.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 282         |
|    mean_reward          | 0.755       |
| time/                   |             |
|    total_timesteps      | 297000      |
| train/                  |             |
|    approx_kl            | 0.003563056 |
|    clip_fraction        | 0.0205      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.52       |
|    explained_variance   | 0.933       |
|    learning_rate        | 3.36e-05    |
|    loss                 | 0.00853     |
|    n_updates            | 1450        |
|    policy_gradient_loss | -0.00421    |
|    std                  | 0.853       |
|    value_loss           | 0.00254     |
-----------------------------------------
Eval num_timesteps=298000, episode_reward=-0.92 +/- 0.15
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.924   |
| time/              |          |
|    total_timesteps | 298000   |
---------------------------------
box reached target
Eval num_timesteps=299000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 299000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 605    |
|    iterations      | 146    |
|    time_elapsed    | 493    |
|    total_timesteps | 299008 |
-------------------------------
Eval num_timesteps=300000, episode_reward=-0.43 +/- 0.70
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.425       |
| time/                   |              |
|    total_timesteps      | 300000       |
| train/                  |              |
|    approx_kl            | 0.0040262695 |
|    clip_fraction        | 0.0102       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.52        |
|    explained_variance   | 0.745        |
|    learning_rate        | 3.36e-05     |
|    loss                 | 0.0186       |
|    n_updates            | 1460         |
|    policy_gradient_loss | -0.00218     |
|    std                  | 0.853        |
|    value_loss           | 0.0545       |
------------------------------------------
Eval num_timesteps=301000, episode_reward=-0.66 +/- 0.68
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.66    |
| time/              |          |
|    total_timesteps | 301000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 605    |
|    iterations      | 147    |
|    time_elapsed    | 497    |
|    total_timesteps | 301056 |
-------------------------------
Eval num_timesteps=302000, episode_reward=-0.90 +/- 0.20
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.899      |
| time/                   |             |
|    total_timesteps      | 302000      |
| train/                  |             |
|    approx_kl            | 0.002816665 |
|    clip_fraction        | 0.0142      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.52       |
|    explained_variance   | 0.908       |
|    learning_rate        | 3.36e-05    |
|    loss                 | -0.0142     |
|    n_updates            | 1470        |
|    policy_gradient_loss | -0.00205    |
|    std                  | 0.853       |
|    value_loss           | 0.0089      |
-----------------------------------------
box reached target
Eval num_timesteps=303000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 303000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 605    |
|    iterations      | 148    |
|    time_elapsed    | 500    |
|    total_timesteps | 303104 |
-------------------------------
box reached target
Eval num_timesteps=304000, episode_reward=0.34 +/- 2.56
Episode length: 275.40 +/- 49.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 275          |
|    mean_reward          | 0.339        |
| time/                   |              |
|    total_timesteps      | 304000       |
| train/                  |              |
|    approx_kl            | 0.0040708305 |
|    clip_fraction        | 0.0207       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.52        |
|    explained_variance   | 0.853        |
|    learning_rate        | 3.36e-05     |
|    loss                 | -0.0214      |
|    n_updates            | 1480         |
|    policy_gradient_loss | -0.00247     |
|    std                  | 0.852        |
|    value_loss           | 0.0217       |
------------------------------------------
box reached target
Eval num_timesteps=305000, episode_reward=-0.86 +/- 0.29
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.856   |
| time/              |          |
|    total_timesteps | 305000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 605    |
|    iterations      | 149    |
|    time_elapsed    | 503    |
|    total_timesteps | 305152 |
-------------------------------
Eval num_timesteps=306000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -1           |
| time/                   |              |
|    total_timesteps      | 306000       |
| train/                  |              |
|    approx_kl            | 0.0019344315 |
|    clip_fraction        | 0.0022       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.52        |
|    explained_variance   | 0.837        |
|    learning_rate        | 3.37e-05     |
|    loss                 | 0.0293       |
|    n_updates            | 1490         |
|    policy_gradient_loss | -0.00149     |
|    std                  | 0.85         |
|    value_loss           | 0.0195       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=307000, episode_reward=1.55 +/- 3.12
Episode length: 273.20 +/- 33.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 273      |
|    mean_reward     | 1.55     |
| time/              |          |
|    total_timesteps | 307000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 605    |
|    iterations      | 150    |
|    time_elapsed    | 507    |
|    total_timesteps | 307200 |
-------------------------------
box reached target
Eval num_timesteps=308000, episode_reward=0.29 +/- 2.58
Episode length: 273.60 +/- 52.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 274          |
|    mean_reward          | 0.289        |
| time/                   |              |
|    total_timesteps      | 308000       |
| train/                  |              |
|    approx_kl            | 0.0038731657 |
|    clip_fraction        | 0.0205       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.51        |
|    explained_variance   | 0.513        |
|    learning_rate        | 3.37e-05     |
|    loss                 | 0.0863       |
|    n_updates            | 1500         |
|    policy_gradient_loss | -0.00223     |
|    std                  | 0.85         |
|    value_loss           | 0.0691       |
------------------------------------------
box reached target
Eval num_timesteps=309000, episode_reward=0.28 +/- 2.56
Episode length: 274.00 +/- 52.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 274      |
|    mean_reward     | 0.278    |
| time/              |          |
|    total_timesteps | 309000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 605    |
|    iterations      | 151    |
|    time_elapsed    | 510    |
|    total_timesteps | 309248 |
-------------------------------
Eval num_timesteps=310000, episode_reward=-0.73 +/- 0.55
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.726      |
| time/                   |             |
|    total_timesteps      | 310000      |
| train/                  |             |
|    approx_kl            | 0.004130154 |
|    clip_fraction        | 0.0156      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.51       |
|    explained_variance   | 0.925       |
|    learning_rate        | 3.37e-05    |
|    loss                 | -0.0062     |
|    n_updates            | 1510        |
|    policy_gradient_loss | -0.00273    |
|    std                  | 0.849       |
|    value_loss           | 0.0073      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=311000, episode_reward=0.62 +/- 2.47
Episode length: 274.60 +/- 50.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 275      |
|    mean_reward     | 0.624    |
| time/              |          |
|    total_timesteps | 311000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 606    |
|    iterations      | 152    |
|    time_elapsed    | 513    |
|    total_timesteps | 311296 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=312000, episode_reward=1.91 +/- 2.95
Episode length: 261.20 +/- 50.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 261         |
|    mean_reward          | 1.91        |
| time/                   |             |
|    total_timesteps      | 312000      |
| train/                  |             |
|    approx_kl            | 0.003791765 |
|    clip_fraction        | 0.00747     |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.51       |
|    explained_variance   | 0.921       |
|    learning_rate        | 3.37e-05    |
|    loss                 | -0.00163    |
|    n_updates            | 1520        |
|    policy_gradient_loss | -0.00244    |
|    std                  | 0.85        |
|    value_loss           | 0.0235      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=313000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 313000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 606    |
|    iterations      | 153    |
|    time_elapsed    | 516    |
|    total_timesteps | 313344 |
-------------------------------
box reached target
Eval num_timesteps=314000, episode_reward=0.93 +/- 2.33
Episode length: 273.40 +/- 53.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 273          |
|    mean_reward          | 0.93         |
| time/                   |              |
|    total_timesteps      | 314000       |
| train/                  |              |
|    approx_kl            | 0.0037731065 |
|    clip_fraction        | 0.0267       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.51        |
|    explained_variance   | 0.53         |
|    learning_rate        | 3.38e-05     |
|    loss                 | 0.127        |
|    n_updates            | 1530         |
|    policy_gradient_loss | -0.00385     |
|    std                  | 0.848        |
|    value_loss           | 0.118        |
------------------------------------------
box reached target
Eval num_timesteps=315000, episode_reward=0.29 +/- 2.58
Episode length: 279.60 +/- 40.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 280      |
|    mean_reward     | 0.29     |
| time/              |          |
|    total_timesteps | 315000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 606    |
|    iterations      | 154    |
|    time_elapsed    | 520    |
|    total_timesteps | 315392 |
-------------------------------
box reached target
Eval num_timesteps=316000, episode_reward=-0.46 +/- 0.70
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.462      |
| time/                   |             |
|    total_timesteps      | 316000      |
| train/                  |             |
|    approx_kl            | 0.003390701 |
|    clip_fraction        | 0.0114      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.51       |
|    explained_variance   | 0.828       |
|    learning_rate        | 3.38e-05    |
|    loss                 | 0.00831     |
|    n_updates            | 1540        |
|    policy_gradient_loss | -0.00271    |
|    std                  | 0.848       |
|    value_loss           | 0.00301     |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=317000, episode_reward=0.60 +/- 2.44
Episode length: 273.20 +/- 53.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 273      |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 317000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 606    |
|    iterations      | 155    |
|    time_elapsed    | 523    |
|    total_timesteps | 317440 |
-------------------------------
Eval num_timesteps=318000, episode_reward=-0.69 +/- 0.61
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.694      |
| time/                   |             |
|    total_timesteps      | 318000      |
| train/                  |             |
|    approx_kl            | 0.003543717 |
|    clip_fraction        | 0.00781     |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.51       |
|    explained_variance   | 0.812       |
|    learning_rate        | 3.38e-05    |
|    loss                 | 0.0404      |
|    n_updates            | 1550        |
|    policy_gradient_loss | -0.00184    |
|    std                  | 0.847       |
|    value_loss           | 0.0492      |
-----------------------------------------
box reached target
Eval num_timesteps=319000, episode_reward=-0.88 +/- 0.24
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.881   |
| time/              |          |
|    total_timesteps | 319000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 606    |
|    iterations      | 156    |
|    time_elapsed    | 526    |
|    total_timesteps | 319488 |
-------------------------------
Eval num_timesteps=320000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -1           |
| time/                   |              |
|    total_timesteps      | 320000       |
| train/                  |              |
|    approx_kl            | 0.0013834286 |
|    clip_fraction        | 0.00283      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.5         |
|    explained_variance   | 0.718        |
|    learning_rate        | 3.38e-05     |
|    loss                 | -0.00513     |
|    n_updates            | 1560         |
|    policy_gradient_loss | -0.00289     |
|    std                  | 0.844        |
|    value_loss           | 0.0334       |
------------------------------------------
box reached target
Eval num_timesteps=321000, episode_reward=0.28 +/- 2.55
Episode length: 273.20 +/- 53.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 273      |
|    mean_reward     | 0.277    |
| time/              |          |
|    total_timesteps | 321000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 606    |
|    iterations      | 157    |
|    time_elapsed    | 530    |
|    total_timesteps | 321536 |
-------------------------------
box reached target
Eval num_timesteps=322000, episode_reward=0.51 +/- 2.45
Episode length: 272.60 +/- 54.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 273          |
|    mean_reward          | 0.515        |
| time/                   |              |
|    total_timesteps      | 322000       |
| train/                  |              |
|    approx_kl            | 0.0048940377 |
|    clip_fraction        | 0.0329       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.5         |
|    explained_variance   | 0.827        |
|    learning_rate        | 3.39e-05     |
|    loss                 | 0.0117       |
|    n_updates            | 1570         |
|    policy_gradient_loss | -0.00497     |
|    std                  | 0.842        |
|    value_loss           | 0.00354      |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=323000, episode_reward=0.54 +/- 2.51
Episode length: 276.60 +/- 46.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 277      |
|    mean_reward     | 0.544    |
| time/              |          |
|    total_timesteps | 323000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 606    |
|    iterations      | 158    |
|    time_elapsed    | 533    |
|    total_timesteps | 323584 |
-------------------------------
box reached target
Eval num_timesteps=324000, episode_reward=0.27 +/- 2.54
Episode length: 281.00 +/- 38.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 281         |
|    mean_reward          | 0.272       |
| time/                   |             |
|    total_timesteps      | 324000      |
| train/                  |             |
|    approx_kl            | 0.002201979 |
|    clip_fraction        | 0.00498     |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.49       |
|    explained_variance   | 0.864       |
|    learning_rate        | 3.39e-05    |
|    loss                 | 0.0451      |
|    n_updates            | 1580        |
|    policy_gradient_loss | -0.00162    |
|    std                  | 0.842       |
|    value_loss           | 0.0475      |
-----------------------------------------
box reached target
Eval num_timesteps=325000, episode_reward=0.57 +/- 2.43
Episode length: 280.20 +/- 39.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 280      |
|    mean_reward     | 0.567    |
| time/              |          |
|    total_timesteps | 325000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 606    |
|    iterations      | 159    |
|    time_elapsed    | 536    |
|    total_timesteps | 325632 |
-------------------------------
Eval num_timesteps=326000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -1           |
| time/                   |              |
|    total_timesteps      | 326000       |
| train/                  |              |
|    approx_kl            | 0.0035597254 |
|    clip_fraction        | 0.0132       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.49        |
|    explained_variance   | 0.913        |
|    learning_rate        | 3.39e-05     |
|    loss                 | -0.0135      |
|    n_updates            | 1590         |
|    policy_gradient_loss | -0.00333     |
|    std                  | 0.843        |
|    value_loss           | 0.00563      |
------------------------------------------
box reached target
Eval num_timesteps=327000, episode_reward=0.28 +/- 2.56
Episode length: 272.40 +/- 55.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 272      |
|    mean_reward     | 0.281    |
| time/              |          |
|    total_timesteps | 327000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 606    |
|    iterations      | 160    |
|    time_elapsed    | 540    |
|    total_timesteps | 327680 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=328000, episode_reward=1.84 +/- 2.97
Episode length: 258.80 +/- 50.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 259         |
|    mean_reward          | 1.84        |
| time/                   |             |
|    total_timesteps      | 328000      |
| train/                  |             |
|    approx_kl            | 0.002503742 |
|    clip_fraction        | 0.00454     |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.49       |
|    explained_variance   | 0.779       |
|    learning_rate        | 3.39e-05    |
|    loss                 | -0.00198    |
|    n_updates            | 1600        |
|    policy_gradient_loss | -0.00163    |
|    std                  | 0.841       |
|    value_loss           | 0.00404     |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=329000, episode_reward=1.74 +/- 3.03
Episode length: 258.00 +/- 54.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 258      |
|    mean_reward     | 1.74     |
| time/              |          |
|    total_timesteps | 329000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 606    |
|    iterations      | 161    |
|    time_elapsed    | 543    |
|    total_timesteps | 329728 |
-------------------------------
box reached target
Eval num_timesteps=330000, episode_reward=0.88 +/- 2.36
Episode length: 273.80 +/- 52.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 274          |
|    mean_reward          | 0.876        |
| time/                   |              |
|    total_timesteps      | 330000       |
| train/                  |              |
|    approx_kl            | 0.0026032708 |
|    clip_fraction        | 0.00615      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.49        |
|    explained_variance   | 0.818        |
|    learning_rate        | 3.4e-05      |
|    loss                 | 0.000883     |
|    n_updates            | 1610         |
|    policy_gradient_loss | -0.00255     |
|    std                  | 0.842        |
|    value_loss           | 0.0289       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=331000, episode_reward=-0.69 +/- 0.62
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.689   |
| time/              |          |
|    total_timesteps | 331000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 606    |
|    iterations      | 162    |
|    time_elapsed    | 546    |
|    total_timesteps | 331776 |
-------------------------------
Eval num_timesteps=332000, episode_reward=-0.42 +/- 0.73
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.419      |
| time/                   |             |
|    total_timesteps      | 332000      |
| train/                  |             |
|    approx_kl            | 0.004939804 |
|    clip_fraction        | 0.028       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.49       |
|    explained_variance   | 0.746       |
|    learning_rate        | 3.4e-05     |
|    loss                 | 0.0513      |
|    n_updates            | 1620        |
|    policy_gradient_loss | -0.00305    |
|    std                  | 0.84        |
|    value_loss           | 0.0927      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=333000, episode_reward=1.53 +/- 3.10
Episode length: 252.40 +/- 58.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 252      |
|    mean_reward     | 1.53     |
| time/              |          |
|    total_timesteps | 333000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 606    |
|    iterations      | 163    |
|    time_elapsed    | 550    |
|    total_timesteps | 333824 |
-------------------------------
Eval num_timesteps=334000, episode_reward=-0.91 +/- 0.18
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.911       |
| time/                   |              |
|    total_timesteps      | 334000       |
| train/                  |              |
|    approx_kl            | 0.0025214036 |
|    clip_fraction        | 0.00713      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.49        |
|    explained_variance   | 0.872        |
|    learning_rate        | 3.4e-05      |
|    loss                 | 0.00952      |
|    n_updates            | 1630         |
|    policy_gradient_loss | -0.00229     |
|    std                  | 0.841        |
|    value_loss           | 0.0231       |
------------------------------------------
box reached target
Eval num_timesteps=335000, episode_reward=0.29 +/- 2.58
Episode length: 276.00 +/- 48.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 276      |
|    mean_reward     | 0.288    |
| time/              |          |
|    total_timesteps | 335000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 606    |
|    iterations      | 164    |
|    time_elapsed    | 553    |
|    total_timesteps | 335872 |
-------------------------------
Eval num_timesteps=336000, episode_reward=-0.67 +/- 0.65
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.673       |
| time/                   |              |
|    total_timesteps      | 336000       |
| train/                  |              |
|    approx_kl            | 0.0038870245 |
|    clip_fraction        | 0.0245       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.49        |
|    explained_variance   | 0.789        |
|    learning_rate        | 3.4e-05      |
|    loss                 | 0.0125       |
|    n_updates            | 1640         |
|    policy_gradient_loss | -0.00531     |
|    std                  | 0.84         |
|    value_loss           | 0.00709      |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=337000, episode_reward=0.48 +/- 2.39
Episode length: 279.80 +/- 40.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 280      |
|    mean_reward     | 0.477    |
| time/              |          |
|    total_timesteps | 337000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 607    |
|    iterations      | 165    |
|    time_elapsed    | 556    |
|    total_timesteps | 337920 |
-------------------------------
Eval num_timesteps=338000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -1          |
| time/                   |             |
|    total_timesteps      | 338000      |
| train/                  |             |
|    approx_kl            | 0.004048949 |
|    clip_fraction        | 0.0237      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.49       |
|    explained_variance   | 0.932       |
|    learning_rate        | 3.41e-05    |
|    loss                 | -0.0141     |
|    n_updates            | 1650        |
|    policy_gradient_loss | -0.0037     |
|    std                  | 0.841       |
|    value_loss           | 0.015       |
-----------------------------------------
box reached target
Eval num_timesteps=339000, episode_reward=0.37 +/- 2.41
Episode length: 280.00 +/- 40.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 280      |
|    mean_reward     | 0.37     |
| time/              |          |
|    total_timesteps | 339000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 607    |
|    iterations      | 166    |
|    time_elapsed    | 559    |
|    total_timesteps | 339968 |
-------------------------------
box reached target
Eval num_timesteps=340000, episode_reward=0.27 +/- 2.53
Episode length: 284.40 +/- 31.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 284         |
|    mean_reward          | 0.266       |
| time/                   |             |
|    total_timesteps      | 340000      |
| train/                  |             |
|    approx_kl            | 0.003626993 |
|    clip_fraction        | 0.0181      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.49       |
|    explained_variance   | 0.933       |
|    learning_rate        | 3.41e-05    |
|    loss                 | -0.00864    |
|    n_updates            | 1660        |
|    policy_gradient_loss | -0.0035     |
|    std                  | 0.839       |
|    value_loss           | 0.0199      |
-----------------------------------------
box reached target
Eval num_timesteps=341000, episode_reward=-0.42 +/- 0.72
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.415   |
| time/              |          |
|    total_timesteps | 341000   |
---------------------------------
box reached target
box reached target
Eval num_timesteps=342000, episode_reward=1.49 +/- 3.05
Episode length: 257.00 +/- 52.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 257      |
|    mean_reward     | 1.49     |
| time/              |          |
|    total_timesteps | 342000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 606    |
|    iterations      | 167    |
|    time_elapsed    | 563    |
|    total_timesteps | 342016 |
-------------------------------
box reached target
Eval num_timesteps=343000, episode_reward=0.25 +/- 2.49
Episode length: 287.60 +/- 24.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 0.247       |
| time/                   |             |
|    total_timesteps      | 343000      |
| train/                  |             |
|    approx_kl            | 0.001816165 |
|    clip_fraction        | 0.00786     |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.49       |
|    explained_variance   | 0.676       |
|    learning_rate        | 3.41e-05    |
|    loss                 | 0.00474     |
|    n_updates            | 1670        |
|    policy_gradient_loss | -0.00255    |
|    std                  | 0.839       |
|    value_loss           | 0.0591      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=344000, episode_reward=0.47 +/- 2.52
Episode length: 284.60 +/- 30.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 285      |
|    mean_reward     | 0.466    |
| time/              |          |
|    total_timesteps | 344000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 606    |
|    iterations      | 168    |
|    time_elapsed    | 567    |
|    total_timesteps | 344064 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=345000, episode_reward=1.52 +/- 3.09
Episode length: 262.20 +/- 47.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 262          |
|    mean_reward          | 1.52         |
| time/                   |              |
|    total_timesteps      | 345000       |
| train/                  |              |
|    approx_kl            | 0.0019910685 |
|    clip_fraction        | 0.0041       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.48        |
|    explained_variance   | 0.735        |
|    learning_rate        | 3.41e-05     |
|    loss                 | 0.0418       |
|    n_updates            | 1680         |
|    policy_gradient_loss | -0.0012      |
|    std                  | 0.837        |
|    value_loss           | 0.139        |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=346000, episode_reward=1.85 +/- 3.05
Episode length: 271.80 +/- 39.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 272      |
|    mean_reward     | 1.85     |
| time/              |          |
|    total_timesteps | 346000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 606    |
|    iterations      | 169    |
|    time_elapsed    | 570    |
|    total_timesteps | 346112 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=347000, episode_reward=0.62 +/- 2.56
Episode length: 280.00 +/- 40.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 280         |
|    mean_reward          | 0.616       |
| time/                   |             |
|    total_timesteps      | 347000      |
| train/                  |             |
|    approx_kl            | 0.004753829 |
|    clip_fraction        | 0.0182      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.48       |
|    explained_variance   | 0.809       |
|    learning_rate        | 3.42e-05    |
|    loss                 | -0.0186     |
|    n_updates            | 1690        |
|    policy_gradient_loss | -0.00324    |
|    std                  | 0.837       |
|    value_loss           | 0.015       |
-----------------------------------------
box reached target
Eval num_timesteps=348000, episode_reward=0.58 +/- 2.44
Episode length: 275.60 +/- 48.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 276      |
|    mean_reward     | 0.582    |
| time/              |          |
|    total_timesteps | 348000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 606    |
|    iterations      | 170    |
|    time_elapsed    | 573    |
|    total_timesteps | 348160 |
-------------------------------
Eval num_timesteps=349000, episode_reward=-0.92 +/- 0.17
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.916      |
| time/                   |             |
|    total_timesteps      | 349000      |
| train/                  |             |
|    approx_kl            | 0.002960602 |
|    clip_fraction        | 0.00894     |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.48       |
|    explained_variance   | 0.933       |
|    learning_rate        | 3.42e-05    |
|    loss                 | 0.0263      |
|    n_updates            | 1700        |
|    policy_gradient_loss | -0.00123    |
|    std                  | 0.836       |
|    value_loss           | 0.034       |
-----------------------------------------
box reached target
Eval num_timesteps=350000, episode_reward=-0.68 +/- 0.41
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.677   |
| time/              |          |
|    total_timesteps | 350000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 606    |
|    iterations      | 171    |
|    time_elapsed    | 576    |
|    total_timesteps | 350208 |
-------------------------------
Eval num_timesteps=351000, episode_reward=-0.80 +/- 0.41
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.795       |
| time/                   |              |
|    total_timesteps      | 351000       |
| train/                  |              |
|    approx_kl            | 0.0031063966 |
|    clip_fraction        | 0.0142       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.48        |
|    explained_variance   | 0.63         |
|    learning_rate        | 3.42e-05     |
|    loss                 | 0.0577       |
|    n_updates            | 1710         |
|    policy_gradient_loss | -0.00392     |
|    std                  | 0.836        |
|    value_loss           | 0.0773       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=352000, episode_reward=1.66 +/- 2.92
Episode length: 259.60 +/- 49.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 260      |
|    mean_reward     | 1.66     |
| time/              |          |
|    total_timesteps | 352000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 607    |
|    iterations      | 172    |
|    time_elapsed    | 580    |
|    total_timesteps | 352256 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=353000, episode_reward=0.73 +/- 2.36
Episode length: 286.40 +/- 27.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 286         |
|    mean_reward          | 0.728       |
| time/                   |             |
|    total_timesteps      | 353000      |
| train/                  |             |
|    approx_kl            | 0.006700866 |
|    clip_fraction        | 0.0487      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.48       |
|    explained_variance   | 0.825       |
|    learning_rate        | 3.42e-05    |
|    loss                 | 0.0215      |
|    n_updates            | 1720        |
|    policy_gradient_loss | -0.00786    |
|    std                  | 0.837       |
|    value_loss           | 0.0128      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=354000, episode_reward=0.27 +/- 2.53
Episode length: 273.80 +/- 52.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 274      |
|    mean_reward     | 0.266    |
| time/              |          |
|    total_timesteps | 354000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 607    |
|    iterations      | 173    |
|    time_elapsed    | 583    |
|    total_timesteps | 354304 |
-------------------------------
box reached target
Eval num_timesteps=355000, episode_reward=-0.79 +/- 0.43
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.785       |
| time/                   |              |
|    total_timesteps      | 355000       |
| train/                  |              |
|    approx_kl            | 0.0018100425 |
|    clip_fraction        | 0.00288      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.48        |
|    explained_variance   | 0.805        |
|    learning_rate        | 3.43e-05     |
|    loss                 | -0.00591     |
|    n_updates            | 1730         |
|    policy_gradient_loss | -0.0018      |
|    std                  | 0.835        |
|    value_loss           | 0.0991       |
------------------------------------------
box reached target
Eval num_timesteps=356000, episode_reward=-0.07 +/- 0.76
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.068   |
| time/              |          |
|    total_timesteps | 356000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 607    |
|    iterations      | 174    |
|    time_elapsed    | 586    |
|    total_timesteps | 356352 |
-------------------------------
box reached target
Eval num_timesteps=357000, episode_reward=0.89 +/- 2.23
Episode length: 279.20 +/- 41.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 279          |
|    mean_reward          | 0.891        |
| time/                   |              |
|    total_timesteps      | 357000       |
| train/                  |              |
|    approx_kl            | 0.0031615766 |
|    clip_fraction        | 0.0139       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.48        |
|    explained_variance   | 0.938        |
|    learning_rate        | 3.43e-05     |
|    loss                 | 0.0206       |
|    n_updates            | 1740         |
|    policy_gradient_loss | -0.00229     |
|    std                  | 0.834        |
|    value_loss           | 0.0456       |
------------------------------------------
box reached target
Eval num_timesteps=358000, episode_reward=0.61 +/- 2.45
Episode length: 281.00 +/- 38.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 281      |
|    mean_reward     | 0.609    |
| time/              |          |
|    total_timesteps | 358000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 607    |
|    iterations      | 175    |
|    time_elapsed    | 590    |
|    total_timesteps | 358400 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=359000, episode_reward=2.07 +/- 2.59
Episode length: 262.20 +/- 46.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 262          |
|    mean_reward          | 2.07         |
| time/                   |              |
|    total_timesteps      | 359000       |
| train/                  |              |
|    approx_kl            | 0.0033125342 |
|    clip_fraction        | 0.0119       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.47        |
|    explained_variance   | 0.838        |
|    learning_rate        | 3.43e-05     |
|    loss                 | -0.000717    |
|    n_updates            | 1750         |
|    policy_gradient_loss | -0.00321     |
|    std                  | 0.834        |
|    value_loss           | 0.00654      |
------------------------------------------
box reached target
Eval num_timesteps=360000, episode_reward=-0.22 +/- 0.74
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.22    |
| time/              |          |
|    total_timesteps | 360000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 607    |
|    iterations      | 176    |
|    time_elapsed    | 593    |
|    total_timesteps | 360448 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=361000, episode_reward=3.04 +/- 3.00
Episode length: 254.00 +/- 38.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 254         |
|    mean_reward          | 3.04        |
| time/                   |             |
|    total_timesteps      | 361000      |
| train/                  |             |
|    approx_kl            | 0.004534276 |
|    clip_fraction        | 0.0143      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.47       |
|    explained_variance   | 0.864       |
|    learning_rate        | 3.43e-05    |
|    loss                 | 0.0134      |
|    n_updates            | 1760        |
|    policy_gradient_loss | -0.00286    |
|    std                  | 0.834       |
|    value_loss           | 0.0507      |
-----------------------------------------
New best mean reward!
Eval num_timesteps=362000, episode_reward=-0.64 +/- 0.72
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.638   |
| time/              |          |
|    total_timesteps | 362000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 607    |
|    iterations      | 177    |
|    time_elapsed    | 596    |
|    total_timesteps | 362496 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=363000, episode_reward=2.13 +/- 2.62
Episode length: 259.80 +/- 49.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 260          |
|    mean_reward          | 2.13         |
| time/                   |              |
|    total_timesteps      | 363000       |
| train/                  |              |
|    approx_kl            | 0.0037178562 |
|    clip_fraction        | 0.0137       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.47        |
|    explained_variance   | 0.902        |
|    learning_rate        | 3.43e-05     |
|    loss                 | 0.00495      |
|    n_updates            | 1770         |
|    policy_gradient_loss | -0.00208     |
|    std                  | 0.834        |
|    value_loss           | 0.00628      |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=364000, episode_reward=0.97 +/- 2.23
Episode length: 277.80 +/- 44.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | 0.966    |
| time/              |          |
|    total_timesteps | 364000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 607    |
|    iterations      | 178    |
|    time_elapsed    | 599    |
|    total_timesteps | 364544 |
-------------------------------
Eval num_timesteps=365000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -1           |
| time/                   |              |
|    total_timesteps      | 365000       |
| train/                  |              |
|    approx_kl            | 0.0032526725 |
|    clip_fraction        | 0.00576      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.47        |
|    explained_variance   | 0.892        |
|    learning_rate        | 3.44e-05     |
|    loss                 | 0.0743       |
|    n_updates            | 1780         |
|    policy_gradient_loss | -0.00127     |
|    std                  | 0.831        |
|    value_loss           | 0.0605       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=366000, episode_reward=3.10 +/- 2.73
Episode length: 246.20 +/- 45.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 246      |
|    mean_reward     | 3.1      |
| time/              |          |
|    total_timesteps | 366000   |
---------------------------------
New best mean reward!
-------------------------------
| time/              |        |
|    fps             | 607    |
|    iterations      | 179    |
|    time_elapsed    | 603    |
|    total_timesteps | 366592 |
-------------------------------
Eval num_timesteps=367000, episode_reward=-0.86 +/- 0.28
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.862      |
| time/                   |             |
|    total_timesteps      | 367000      |
| train/                  |             |
|    approx_kl            | 0.003209969 |
|    clip_fraction        | 0.021       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.47       |
|    explained_variance   | 0.786       |
|    learning_rate        | 3.44e-05    |
|    loss                 | -0.0191     |
|    n_updates            | 1790        |
|    policy_gradient_loss | -0.00221    |
|    std                  | 0.832       |
|    value_loss           | 0.00494     |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=368000, episode_reward=2.76 +/- 3.07
Episode length: 251.20 +/- 45.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | 2.76     |
| time/              |          |
|    total_timesteps | 368000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 607    |
|    iterations      | 180    |
|    time_elapsed    | 606    |
|    total_timesteps | 368640 |
-------------------------------
Eval num_timesteps=369000, episode_reward=-0.71 +/- 0.58
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.709      |
| time/                   |             |
|    total_timesteps      | 369000      |
| train/                  |             |
|    approx_kl            | 0.004051809 |
|    clip_fraction        | 0.0184      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.47       |
|    explained_variance   | 0.734       |
|    learning_rate        | 3.44e-05    |
|    loss                 | -0.0071     |
|    n_updates            | 1800        |
|    policy_gradient_loss | -0.00328    |
|    std                  | 0.832       |
|    value_loss           | 0.00129     |
-----------------------------------------
Eval num_timesteps=370000, episode_reward=-0.91 +/- 0.19
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.906   |
| time/              |          |
|    total_timesteps | 370000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 607    |
|    iterations      | 181    |
|    time_elapsed    | 609    |
|    total_timesteps | 370688 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=371000, episode_reward=1.77 +/- 2.88
Episode length: 256.40 +/- 53.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 256        |
|    mean_reward          | 1.77       |
| time/                   |            |
|    total_timesteps      | 371000     |
| train/                  |            |
|    approx_kl            | 0.00484237 |
|    clip_fraction        | 0.04       |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.47      |
|    explained_variance   | 0.883      |
|    learning_rate        | 3.44e-05   |
|    loss                 | 0.00447    |
|    n_updates            | 1810       |
|    policy_gradient_loss | -0.00471   |
|    std                  | 0.832      |
|    value_loss           | 0.00503    |
----------------------------------------
box reached target
Eval num_timesteps=372000, episode_reward=0.47 +/- 2.65
Episode length: 285.40 +/- 29.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 285      |
|    mean_reward     | 0.473    |
| time/              |          |
|    total_timesteps | 372000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 608    |
|    iterations      | 182    |
|    time_elapsed    | 612    |
|    total_timesteps | 372736 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=373000, episode_reward=1.89 +/- 2.95
Episode length: 260.20 +/- 48.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 260          |
|    mean_reward          | 1.89         |
| time/                   |              |
|    total_timesteps      | 373000       |
| train/                  |              |
|    approx_kl            | 0.0070240414 |
|    clip_fraction        | 0.0358       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.47        |
|    explained_variance   | 0.808        |
|    learning_rate        | 3.45e-05     |
|    loss                 | 0.0142       |
|    n_updates            | 1820         |
|    policy_gradient_loss | -0.00505     |
|    std                  | 0.829        |
|    value_loss           | 0.05         |
------------------------------------------
Eval num_timesteps=374000, episode_reward=-0.19 +/- 0.72
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.195   |
| time/              |          |
|    total_timesteps | 374000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 608    |
|    iterations      | 183    |
|    time_elapsed    | 616    |
|    total_timesteps | 374784 |
-------------------------------
box reached target
Eval num_timesteps=375000, episode_reward=0.71 +/- 2.39
Episode length: 285.00 +/- 30.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 285         |
|    mean_reward          | 0.706       |
| time/                   |             |
|    total_timesteps      | 375000      |
| train/                  |             |
|    approx_kl            | 0.003769692 |
|    clip_fraction        | 0.00972     |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.46       |
|    explained_variance   | 0.946       |
|    learning_rate        | 3.45e-05    |
|    loss                 | 0.0218      |
|    n_updates            | 1830        |
|    policy_gradient_loss | -0.00309    |
|    std                  | 0.828       |
|    value_loss           | 0.0226      |
-----------------------------------------
box reached target
Eval num_timesteps=376000, episode_reward=0.80 +/- 2.56
Episode length: 286.60 +/- 26.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 376000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 608    |
|    iterations      | 184    |
|    time_elapsed    | 619    |
|    total_timesteps | 376832 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=377000, episode_reward=2.74 +/- 3.06
Episode length: 236.20 +/- 54.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 236         |
|    mean_reward          | 2.74        |
| time/                   |             |
|    total_timesteps      | 377000      |
| train/                  |             |
|    approx_kl            | 0.005973615 |
|    clip_fraction        | 0.0486      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.46       |
|    explained_variance   | 0.93        |
|    learning_rate        | 3.45e-05    |
|    loss                 | 0.00127     |
|    n_updates            | 1840        |
|    policy_gradient_loss | -0.00665    |
|    std                  | 0.827       |
|    value_loss           | 0.00224     |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=378000, episode_reward=1.79 +/- 2.96
Episode length: 262.20 +/- 46.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 262      |
|    mean_reward     | 1.79     |
| time/              |          |
|    total_timesteps | 378000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 608    |
|    iterations      | 185    |
|    time_elapsed    | 622    |
|    total_timesteps | 378880 |
-------------------------------
Eval num_timesteps=379000, episode_reward=-0.79 +/- 0.28
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.793       |
| time/                   |              |
|    total_timesteps      | 379000       |
| train/                  |              |
|    approx_kl            | 0.0027300476 |
|    clip_fraction        | 0.00752      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.46        |
|    explained_variance   | 0.865        |
|    learning_rate        | 3.45e-05     |
|    loss                 | 0.0748       |
|    n_updates            | 1850         |
|    policy_gradient_loss | -0.00241     |
|    std                  | 0.826        |
|    value_loss           | 0.0486       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=380000, episode_reward=1.86 +/- 2.93
Episode length: 258.20 +/- 51.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 258      |
|    mean_reward     | 1.86     |
| time/              |          |
|    total_timesteps | 380000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 608    |
|    iterations      | 186    |
|    time_elapsed    | 625    |
|    total_timesteps | 380928 |
-------------------------------
Eval num_timesteps=381000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -1           |
| time/                   |              |
|    total_timesteps      | 381000       |
| train/                  |              |
|    approx_kl            | 0.0037773661 |
|    clip_fraction        | 0.013        |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.45        |
|    explained_variance   | 0.788        |
|    learning_rate        | 3.46e-05     |
|    loss                 | 0.0258       |
|    n_updates            | 1860         |
|    policy_gradient_loss | -0.00293     |
|    std                  | 0.823        |
|    value_loss           | 0.0835       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=382000, episode_reward=0.59 +/- 2.42
Episode length: 283.20 +/- 33.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 283      |
|    mean_reward     | 0.587    |
| time/              |          |
|    total_timesteps | 382000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 608    |
|    iterations      | 187    |
|    time_elapsed    | 629    |
|    total_timesteps | 382976 |
-------------------------------
Eval num_timesteps=383000, episode_reward=-0.79 +/- 0.42
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.791       |
| time/                   |              |
|    total_timesteps      | 383000       |
| train/                  |              |
|    approx_kl            | 0.0016064357 |
|    clip_fraction        | 0.00405      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.45        |
|    explained_variance   | 0.74         |
|    learning_rate        | 3.46e-05     |
|    loss                 | 0.0112       |
|    n_updates            | 1870         |
|    policy_gradient_loss | -0.000869    |
|    std                  | 0.822        |
|    value_loss           | 0.137        |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=384000, episode_reward=3.05 +/- 2.78
Episode length: 236.20 +/- 56.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 236      |
|    mean_reward     | 3.05     |
| time/              |          |
|    total_timesteps | 384000   |
---------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=385000, episode_reward=0.87 +/- 2.29
Episode length: 278.60 +/- 42.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 279      |
|    mean_reward     | 0.87     |
| time/              |          |
|    total_timesteps | 385000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 608    |
|    iterations      | 188    |
|    time_elapsed    | 633    |
|    total_timesteps | 385024 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=386000, episode_reward=1.23 +/- 2.17
Episode length: 274.60 +/- 50.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 275          |
|    mean_reward          | 1.23         |
| time/                   |              |
|    total_timesteps      | 386000       |
| train/                  |              |
|    approx_kl            | 0.0016936131 |
|    clip_fraction        | 0.002        |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.44        |
|    explained_variance   | 0.865        |
|    learning_rate        | 3.46e-05     |
|    loss                 | 0.0102       |
|    n_updates            | 1880         |
|    policy_gradient_loss | -0.000703    |
|    std                  | 0.82         |
|    value_loss           | 0.101        |
------------------------------------------
box reached target
Eval num_timesteps=387000, episode_reward=0.48 +/- 2.51
Episode length: 284.40 +/- 31.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 284      |
|    mean_reward     | 0.484    |
| time/              |          |
|    total_timesteps | 387000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 608    |
|    iterations      | 189    |
|    time_elapsed    | 636    |
|    total_timesteps | 387072 |
-------------------------------
box reached target
Eval num_timesteps=388000, episode_reward=-0.98 +/- 0.04
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.979       |
| time/                   |              |
|    total_timesteps      | 388000       |
| train/                  |              |
|    approx_kl            | 0.0048063975 |
|    clip_fraction        | 0.0199       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.44        |
|    explained_variance   | 0.847        |
|    learning_rate        | 3.46e-05     |
|    loss                 | 0.0148       |
|    n_updates            | 1890         |
|    policy_gradient_loss | -0.00321     |
|    std                  | 0.821        |
|    value_loss           | 0.0322       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=389000, episode_reward=1.11 +/- 2.13
Episode length: 279.80 +/- 40.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 280      |
|    mean_reward     | 1.11     |
| time/              |          |
|    total_timesteps | 389000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 608    |
|    iterations      | 190    |
|    time_elapsed    | 639    |
|    total_timesteps | 389120 |
-------------------------------
box reached target
Eval num_timesteps=390000, episode_reward=-0.04 +/- 0.61
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.0429      |
| time/                   |              |
|    total_timesteps      | 390000       |
| train/                  |              |
|    approx_kl            | 0.0012489494 |
|    clip_fraction        | 0.00112      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.44        |
|    explained_variance   | 0.72         |
|    learning_rate        | 3.47e-05     |
|    loss                 | 0.0502       |
|    n_updates            | 1900         |
|    policy_gradient_loss | -0.000915    |
|    std                  | 0.82         |
|    value_loss           | 0.155        |
------------------------------------------
box reached target
Eval num_timesteps=391000, episode_reward=0.69 +/- 2.50
Episode length: 295.00 +/- 10.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 295      |
|    mean_reward     | 0.689    |
| time/              |          |
|    total_timesteps | 391000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 608    |
|    iterations      | 191    |
|    time_elapsed    | 643    |
|    total_timesteps | 391168 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=392000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -1           |
| time/                   |              |
|    total_timesteps      | 392000       |
| train/                  |              |
|    approx_kl            | 0.0025751959 |
|    clip_fraction        | 0.00454      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.44        |
|    explained_variance   | 0.806        |
|    learning_rate        | 3.47e-05     |
|    loss                 | 0.0104       |
|    n_updates            | 1910         |
|    policy_gradient_loss | -0.00226     |
|    std                  | 0.818        |
|    value_loss           | 0.0478       |
------------------------------------------
box reached target
Eval num_timesteps=393000, episode_reward=0.36 +/- 2.42
Episode length: 272.40 +/- 55.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 272      |
|    mean_reward     | 0.357    |
| time/              |          |
|    total_timesteps | 393000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 608    |
|    iterations      | 192    |
|    time_elapsed    | 646    |
|    total_timesteps | 393216 |
-------------------------------
Eval num_timesteps=394000, episode_reward=-0.27 +/- 0.73
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.269      |
| time/                   |             |
|    total_timesteps      | 394000      |
| train/                  |             |
|    approx_kl            | 0.003947087 |
|    clip_fraction        | 0.013       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.43       |
|    explained_variance   | 0.85        |
|    learning_rate        | 3.47e-05    |
|    loss                 | 0.0217      |
|    n_updates            | 1920        |
|    policy_gradient_loss | -0.00271    |
|    std                  | 0.817       |
|    value_loss           | 0.0712      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=395000, episode_reward=2.83 +/- 3.13
Episode length: 251.00 +/- 43.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | 2.83     |
| time/              |          |
|    total_timesteps | 395000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 608    |
|    iterations      | 193    |
|    time_elapsed    | 649    |
|    total_timesteps | 395264 |
-------------------------------
box reached target
Eval num_timesteps=396000, episode_reward=0.31 +/- 2.62
Episode length: 287.60 +/- 24.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 288          |
|    mean_reward          | 0.309        |
| time/                   |              |
|    total_timesteps      | 396000       |
| train/                  |              |
|    approx_kl            | 0.0029895785 |
|    clip_fraction        | 0.00532      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.43        |
|    explained_variance   | 0.906        |
|    learning_rate        | 3.47e-05     |
|    loss                 | 0.0195       |
|    n_updates            | 1930         |
|    policy_gradient_loss | -0.00259     |
|    std                  | 0.816        |
|    value_loss           | 0.0341       |
------------------------------------------
box reached target
Eval num_timesteps=397000, episode_reward=0.61 +/- 2.30
Episode length: 276.40 +/- 47.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 276      |
|    mean_reward     | 0.609    |
| time/              |          |
|    total_timesteps | 397000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 608    |
|    iterations      | 194    |
|    time_elapsed    | 652    |
|    total_timesteps | 397312 |
-------------------------------
box reached target
Eval num_timesteps=398000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -1           |
| time/                   |              |
|    total_timesteps      | 398000       |
| train/                  |              |
|    approx_kl            | 0.0017961104 |
|    clip_fraction        | 0.00259      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.43        |
|    explained_variance   | 0.835        |
|    learning_rate        | 3.48e-05     |
|    loss                 | 0.0021       |
|    n_updates            | 1940         |
|    policy_gradient_loss | -0.00153     |
|    std                  | 0.815        |
|    value_loss           | 0.0101       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=399000, episode_reward=1.54 +/- 3.03
Episode length: 263.60 +/- 45.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 264      |
|    mean_reward     | 1.54     |
| time/              |          |
|    total_timesteps | 399000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 608    |
|    iterations      | 195    |
|    time_elapsed    | 656    |
|    total_timesteps | 399360 |
-------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=400000, episode_reward=2.79 +/- 3.10
Episode length: 242.20 +/- 47.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 242          |
|    mean_reward          | 2.79         |
| time/                   |              |
|    total_timesteps      | 400000       |
| train/                  |              |
|    approx_kl            | 0.0044309297 |
|    clip_fraction        | 0.0255       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.43        |
|    explained_variance   | 0.724        |
|    learning_rate        | 3.48e-05     |
|    loss                 | 0.0256       |
|    n_updates            | 1950         |
|    policy_gradient_loss | -0.00294     |
|    std                  | 0.815        |
|    value_loss           | 0.102        |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=401000, episode_reward=2.88 +/- 2.75
Episode length: 236.60 +/- 52.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 237      |
|    mean_reward     | 2.88     |
| time/              |          |
|    total_timesteps | 401000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 608    |
|    iterations      | 196    |
|    time_elapsed    | 659    |
|    total_timesteps | 401408 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=402000, episode_reward=1.93 +/- 2.68
Episode length: 252.00 +/- 60.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 252          |
|    mean_reward          | 1.93         |
| time/                   |              |
|    total_timesteps      | 402000       |
| train/                  |              |
|    approx_kl            | 0.0027693566 |
|    clip_fraction        | 0.0176       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.43        |
|    explained_variance   | 0.8          |
|    learning_rate        | 3.48e-05     |
|    loss                 | -0.0224      |
|    n_updates            | 1960         |
|    policy_gradient_loss | -0.00505     |
|    std                  | 0.813        |
|    value_loss           | 0.0574       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=403000, episode_reward=1.50 +/- 3.06
Episode length: 256.80 +/- 54.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 257      |
|    mean_reward     | 1.5      |
| time/              |          |
|    total_timesteps | 403000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 609    |
|    iterations      | 197    |
|    time_elapsed    | 662    |
|    total_timesteps | 403456 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=404000, episode_reward=1.51 +/- 3.17
Episode length: 263.20 +/- 49.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 263          |
|    mean_reward          | 1.51         |
| time/                   |              |
|    total_timesteps      | 404000       |
| train/                  |              |
|    approx_kl            | 0.0024854024 |
|    clip_fraction        | 0.00952      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.42        |
|    explained_variance   | 0.858        |
|    learning_rate        | 3.48e-05     |
|    loss                 | 0.0178       |
|    n_updates            | 1970         |
|    policy_gradient_loss | -0.00196     |
|    std                  | 0.813        |
|    value_loss           | 0.0832       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=405000, episode_reward=2.69 +/- 3.03
Episode length: 243.00 +/- 51.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 243      |
|    mean_reward     | 2.69     |
| time/              |          |
|    total_timesteps | 405000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 609    |
|    iterations      | 198    |
|    time_elapsed    | 665    |
|    total_timesteps | 405504 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=406000, episode_reward=1.80 +/- 2.93
Episode length: 256.60 +/- 55.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 257          |
|    mean_reward          | 1.8          |
| time/                   |              |
|    total_timesteps      | 406000       |
| train/                  |              |
|    approx_kl            | 0.0016327517 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.42        |
|    explained_variance   | 0.696        |
|    learning_rate        | 3.49e-05     |
|    loss                 | 0.027        |
|    n_updates            | 1980         |
|    policy_gradient_loss | -0.00134     |
|    std                  | 0.814        |
|    value_loss           | 0.11         |
------------------------------------------
box reached target
Eval num_timesteps=407000, episode_reward=-0.64 +/- 0.54
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.643   |
| time/              |          |
|    total_timesteps | 407000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 609    |
|    iterations      | 199    |
|    time_elapsed    | 668    |
|    total_timesteps | 407552 |
-------------------------------
Eval num_timesteps=408000, episode_reward=-0.46 +/- 0.70
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.455      |
| time/                   |             |
|    total_timesteps      | 408000      |
| train/                  |             |
|    approx_kl            | 0.004723886 |
|    clip_fraction        | 0.0343      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.42       |
|    explained_variance   | 0.956       |
|    learning_rate        | 3.49e-05    |
|    loss                 | 0.0209      |
|    n_updates            | 1990        |
|    policy_gradient_loss | -0.00628    |
|    std                  | 0.812       |
|    value_loss           | 0.0165      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=409000, episode_reward=2.14 +/- 2.76
Episode length: 282.40 +/- 23.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 282      |
|    mean_reward     | 2.14     |
| time/              |          |
|    total_timesteps | 409000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 609    |
|    iterations      | 200    |
|    time_elapsed    | 672    |
|    total_timesteps | 409600 |
-------------------------------
box reached target
Eval num_timesteps=410000, episode_reward=-0.23 +/- 0.65
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.225      |
| time/                   |             |
|    total_timesteps      | 410000      |
| train/                  |             |
|    approx_kl            | 0.003222051 |
|    clip_fraction        | 0.0171      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.42       |
|    explained_variance   | 0.828       |
|    learning_rate        | 3.49e-05    |
|    loss                 | 0.0764      |
|    n_updates            | 2000        |
|    policy_gradient_loss | -0.00379    |
|    std                  | 0.812       |
|    value_loss           | 0.101       |
-----------------------------------------
Eval num_timesteps=411000, episode_reward=-0.82 +/- 0.19
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.82    |
| time/              |          |
|    total_timesteps | 411000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 609    |
|    iterations      | 201    |
|    time_elapsed    | 675    |
|    total_timesteps | 411648 |
-------------------------------
Eval num_timesteps=412000, episode_reward=-0.86 +/- 0.17
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.861       |
| time/                   |              |
|    total_timesteps      | 412000       |
| train/                  |              |
|    approx_kl            | 0.0046902206 |
|    clip_fraction        | 0.0227       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.42        |
|    explained_variance   | 0.891        |
|    learning_rate        | 3.49e-05     |
|    loss                 | -0.000256    |
|    n_updates            | 2010         |
|    policy_gradient_loss | -0.00393     |
|    std                  | 0.812        |
|    value_loss           | 0.0383       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=413000, episode_reward=2.99 +/- 2.75
Episode length: 225.20 +/- 62.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 225      |
|    mean_reward     | 2.99     |
| time/              |          |
|    total_timesteps | 413000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 609    |
|    iterations      | 202    |
|    time_elapsed    | 678    |
|    total_timesteps | 413696 |
-------------------------------
box reached target
Eval num_timesteps=414000, episode_reward=0.73 +/- 2.36
Episode length: 277.60 +/- 44.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 278         |
|    mean_reward          | 0.733       |
| time/                   |             |
|    total_timesteps      | 414000      |
| train/                  |             |
|    approx_kl            | 0.003627522 |
|    clip_fraction        | 0.0218      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.42       |
|    explained_variance   | 0.893       |
|    learning_rate        | 3.5e-05     |
|    loss                 | 0.0221      |
|    n_updates            | 2020        |
|    policy_gradient_loss | -0.0047     |
|    std                  | 0.813       |
|    value_loss           | 0.0321      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=415000, episode_reward=1.51 +/- 3.08
Episode length: 257.80 +/- 53.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 258      |
|    mean_reward     | 1.51     |
| time/              |          |
|    total_timesteps | 415000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 609    |
|    iterations      | 203    |
|    time_elapsed    | 681    |
|    total_timesteps | 415744 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=416000, episode_reward=1.96 +/- 2.80
Episode length: 259.20 +/- 50.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 259          |
|    mean_reward          | 1.96         |
| time/                   |              |
|    total_timesteps      | 416000       |
| train/                  |              |
|    approx_kl            | 0.0040968386 |
|    clip_fraction        | 0.0167       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.42        |
|    explained_variance   | 0.795        |
|    learning_rate        | 3.5e-05      |
|    loss                 | -0.00599     |
|    n_updates            | 2030         |
|    policy_gradient_loss | -0.0043      |
|    std                  | 0.812        |
|    value_loss           | 0.0263       |
------------------------------------------
box reached target
Eval num_timesteps=417000, episode_reward=-0.67 +/- 0.70
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.674   |
| time/              |          |
|    total_timesteps | 417000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 609    |
|    iterations      | 204    |
|    time_elapsed    | 685    |
|    total_timesteps | 417792 |
-------------------------------
Eval num_timesteps=418000, episode_reward=-0.49 +/- 0.55
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.488      |
| time/                   |             |
|    total_timesteps      | 418000      |
| train/                  |             |
|    approx_kl            | 0.003650999 |
|    clip_fraction        | 0.0136      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.42       |
|    explained_variance   | 0.873       |
|    learning_rate        | 3.5e-05     |
|    loss                 | -0.000927   |
|    n_updates            | 2040        |
|    policy_gradient_loss | -0.00251    |
|    std                  | 0.812       |
|    value_loss           | 0.0525      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=419000, episode_reward=1.69 +/- 2.94
Episode length: 252.60 +/- 58.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 253      |
|    mean_reward     | 1.69     |
| time/              |          |
|    total_timesteps | 419000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 609    |
|    iterations      | 205    |
|    time_elapsed    | 688    |
|    total_timesteps | 419840 |
-------------------------------
box reached target
Eval num_timesteps=420000, episode_reward=0.77 +/- 2.22
Episode length: 275.60 +/- 48.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 276         |
|    mean_reward          | 0.772       |
| time/                   |             |
|    total_timesteps      | 420000      |
| train/                  |             |
|    approx_kl            | 0.002082224 |
|    clip_fraction        | 0.00439     |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.42       |
|    explained_variance   | 0.851       |
|    learning_rate        | 3.5e-05     |
|    loss                 | 0.0191      |
|    n_updates            | 2050        |
|    policy_gradient_loss | -0.00228    |
|    std                  | 0.809       |
|    value_loss           | 0.069       |
-----------------------------------------
box reached target
Eval num_timesteps=421000, episode_reward=0.62 +/- 2.51
Episode length: 282.80 +/- 34.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 283      |
|    mean_reward     | 0.623    |
| time/              |          |
|    total_timesteps | 421000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 609    |
|    iterations      | 206    |
|    time_elapsed    | 691    |
|    total_timesteps | 421888 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=422000, episode_reward=2.70 +/- 3.02
Episode length: 227.40 +/- 59.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 227         |
|    mean_reward          | 2.7         |
| time/                   |             |
|    total_timesteps      | 422000      |
| train/                  |             |
|    approx_kl            | 0.004049005 |
|    clip_fraction        | 0.00752     |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.41       |
|    explained_variance   | 0.784       |
|    learning_rate        | 3.51e-05    |
|    loss                 | 0.0235      |
|    n_updates            | 2060        |
|    policy_gradient_loss | -0.0021     |
|    std                  | 0.81        |
|    value_loss           | 0.0852      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=423000, episode_reward=1.68 +/- 3.00
Episode length: 265.20 +/- 49.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 265      |
|    mean_reward     | 1.68     |
| time/              |          |
|    total_timesteps | 423000   |
---------------------------------
box reached target
box reached target
-------------------------------
| time/              |        |
|    fps             | 610    |
|    iterations      | 207    |
|    time_elapsed    | 694    |
|    total_timesteps | 423936 |
-------------------------------
box reached target
Eval num_timesteps=424000, episode_reward=0.28 +/- 2.44
Episode length: 275.40 +/- 49.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 275          |
|    mean_reward          | 0.279        |
| time/                   |              |
|    total_timesteps      | 424000       |
| train/                  |              |
|    approx_kl            | 0.0027436847 |
|    clip_fraction        | 0.00889      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.41        |
|    explained_variance   | 0.837        |
|    learning_rate        | 3.51e-05     |
|    loss                 | 0.0414       |
|    n_updates            | 2070         |
|    policy_gradient_loss | -0.00349     |
|    std                  | 0.807        |
|    value_loss           | 0.0885       |
------------------------------------------
box reached target
Eval num_timesteps=425000, episode_reward=0.22 +/- 2.45
Episode length: 275.80 +/- 48.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 276      |
|    mean_reward     | 0.224    |
| time/              |          |
|    total_timesteps | 425000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 610    |
|    iterations      | 208    |
|    time_elapsed    | 698    |
|    total_timesteps | 425984 |
-------------------------------
Eval num_timesteps=426000, episode_reward=-0.41 +/- 0.74
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.405      |
| time/                   |             |
|    total_timesteps      | 426000      |
| train/                  |             |
|    approx_kl            | 0.004805008 |
|    clip_fraction        | 0.0232      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.41       |
|    explained_variance   | 0.9         |
|    learning_rate        | 3.51e-05    |
|    loss                 | -0.00204    |
|    n_updates            | 2080        |
|    policy_gradient_loss | -0.00508    |
|    std                  | 0.806       |
|    value_loss           | 0.0249      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=427000, episode_reward=0.84 +/- 2.25
Episode length: 273.00 +/- 54.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 273      |
|    mean_reward     | 0.842    |
| time/              |          |
|    total_timesteps | 427000   |
---------------------------------
box reached target
box reached target
Eval num_timesteps=428000, episode_reward=0.66 +/- 2.34
Episode length: 280.60 +/- 38.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 281      |
|    mean_reward     | 0.656    |
| time/              |          |
|    total_timesteps | 428000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 609    |
|    iterations      | 209    |
|    time_elapsed    | 702    |
|    total_timesteps | 428032 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=429000, episode_reward=0.87 +/- 2.33
Episode length: 282.80 +/- 34.40
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 283        |
|    mean_reward          | 0.872      |
| time/                   |            |
|    total_timesteps      | 429000     |
| train/                  |            |
|    approx_kl            | 0.00398516 |
|    clip_fraction        | 0.0195     |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.4       |
|    explained_variance   | 0.847      |
|    learning_rate        | 3.51e-05   |
|    loss                 | 0.0175     |
|    n_updates            | 2090       |
|    policy_gradient_loss | -0.00229   |
|    std                  | 0.805      |
|    value_loss           | 0.0761     |
----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=430000, episode_reward=2.91 +/- 3.10
Episode length: 240.20 +/- 51.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 240      |
|    mean_reward     | 2.91     |
| time/              |          |
|    total_timesteps | 430000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 609    |
|    iterations      | 210    |
|    time_elapsed    | 705    |
|    total_timesteps | 430080 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=431000, episode_reward=1.78 +/- 2.84
Episode length: 245.20 +/- 67.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 245          |
|    mean_reward          | 1.78         |
| time/                   |              |
|    total_timesteps      | 431000       |
| train/                  |              |
|    approx_kl            | 0.0029468606 |
|    clip_fraction        | 0.00981      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.4         |
|    explained_variance   | 0.845        |
|    learning_rate        | 3.52e-05     |
|    loss                 | 0.0189       |
|    n_updates            | 2100         |
|    policy_gradient_loss | -0.0028      |
|    std                  | 0.803        |
|    value_loss           | 0.118        |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=432000, episode_reward=-0.76 +/- 0.41
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.759   |
| time/              |          |
|    total_timesteps | 432000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 609    |
|    iterations      | 211    |
|    time_elapsed    | 708    |
|    total_timesteps | 432128 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=433000, episode_reward=3.05 +/- 2.86
Episode length: 248.20 +/- 52.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 248         |
|    mean_reward          | 3.05        |
| time/                   |             |
|    total_timesteps      | 433000      |
| train/                  |             |
|    approx_kl            | 0.001455105 |
|    clip_fraction        | 0.000928    |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.4        |
|    explained_variance   | 0.694       |
|    learning_rate        | 3.52e-05    |
|    loss                 | 0.018       |
|    n_updates            | 2110        |
|    policy_gradient_loss | -0.00168    |
|    std                  | 0.803       |
|    value_loss           | 0.0757      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=434000, episode_reward=-0.66 +/- 0.73
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.659   |
| time/              |          |
|    total_timesteps | 434000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 609    |
|    iterations      | 212    |
|    time_elapsed    | 711    |
|    total_timesteps | 434176 |
-------------------------------
box reached target
Eval num_timesteps=435000, episode_reward=0.30 +/- 2.41
Episode length: 276.40 +/- 47.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 276          |
|    mean_reward          | 0.302        |
| time/                   |              |
|    total_timesteps      | 435000       |
| train/                  |              |
|    approx_kl            | 0.0060654273 |
|    clip_fraction        | 0.028        |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.4         |
|    explained_variance   | 0.945        |
|    learning_rate        | 3.52e-05     |
|    loss                 | -0.0155      |
|    n_updates            | 2120         |
|    policy_gradient_loss | -0.00502     |
|    std                  | 0.803        |
|    value_loss           | 0.0378       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=436000, episode_reward=0.23 +/- 2.45
Episode length: 276.20 +/- 47.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 276      |
|    mean_reward     | 0.227    |
| time/              |          |
|    total_timesteps | 436000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 610    |
|    iterations      | 213    |
|    time_elapsed    | 715    |
|    total_timesteps | 436224 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=437000, episode_reward=1.92 +/- 2.91
Episode length: 273.00 +/- 35.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 273          |
|    mean_reward          | 1.92         |
| time/                   |              |
|    total_timesteps      | 437000       |
| train/                  |              |
|    approx_kl            | 0.0044349334 |
|    clip_fraction        | 0.0251       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.39        |
|    explained_variance   | 0.745        |
|    learning_rate        | 3.52e-05     |
|    loss                 | 0.00193      |
|    n_updates            | 2130         |
|    policy_gradient_loss | -0.00428     |
|    std                  | 0.8          |
|    value_loss           | 0.0756       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=438000, episode_reward=4.05 +/- 2.53
Episode length: 220.00 +/- 42.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 220      |
|    mean_reward     | 4.05     |
| time/              |          |
|    total_timesteps | 438000   |
---------------------------------
New best mean reward!
-------------------------------
| time/              |        |
|    fps             | 610    |
|    iterations      | 214    |
|    time_elapsed    | 718    |
|    total_timesteps | 438272 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=439000, episode_reward=1.94 +/- 2.77
Episode length: 256.00 +/- 54.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 256         |
|    mean_reward          | 1.94        |
| time/                   |             |
|    total_timesteps      | 439000      |
| train/                  |             |
|    approx_kl            | 0.002126576 |
|    clip_fraction        | 0.011       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.39       |
|    explained_variance   | 0.928       |
|    learning_rate        | 3.53e-05    |
|    loss                 | 0.0275      |
|    n_updates            | 2140        |
|    policy_gradient_loss | -0.00382    |
|    std                  | 0.8         |
|    value_loss           | 0.0466      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=440000, episode_reward=1.67 +/- 2.95
Episode length: 262.60 +/- 48.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 263      |
|    mean_reward     | 1.67     |
| time/              |          |
|    total_timesteps | 440000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 610    |
|    iterations      | 215    |
|    time_elapsed    | 721    |
|    total_timesteps | 440320 |
-------------------------------
box reached target
Eval num_timesteps=441000, episode_reward=0.38 +/- 2.48
Episode length: 285.00 +/- 30.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 285          |
|    mean_reward          | 0.376        |
| time/                   |              |
|    total_timesteps      | 441000       |
| train/                  |              |
|    approx_kl            | 0.0036863831 |
|    clip_fraction        | 0.00957      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.39        |
|    explained_variance   | 0.88         |
|    learning_rate        | 3.53e-05     |
|    loss                 | 0.0307       |
|    n_updates            | 2150         |
|    policy_gradient_loss | -0.00329     |
|    std                  | 0.801        |
|    value_loss           | 0.0179       |
------------------------------------------
Eval num_timesteps=442000, episode_reward=-0.66 +/- 0.68
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.661   |
| time/              |          |
|    total_timesteps | 442000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 610    |
|    iterations      | 216    |
|    time_elapsed    | 724    |
|    total_timesteps | 442368 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=443000, episode_reward=2.89 +/- 3.08
Episode length: 251.80 +/- 46.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 252         |
|    mean_reward          | 2.89        |
| time/                   |             |
|    total_timesteps      | 443000      |
| train/                  |             |
|    approx_kl            | 0.004401151 |
|    clip_fraction        | 0.0281      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.39       |
|    explained_variance   | 0.845       |
|    learning_rate        | 3.53e-05    |
|    loss                 | 0.00131     |
|    n_updates            | 2160        |
|    policy_gradient_loss | -0.00381    |
|    std                  | 0.801       |
|    value_loss           | 0.0102      |
-----------------------------------------
box reached target
Eval num_timesteps=444000, episode_reward=0.86 +/- 2.32
Episode length: 279.60 +/- 40.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 280      |
|    mean_reward     | 0.859    |
| time/              |          |
|    total_timesteps | 444000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 610    |
|    iterations      | 217    |
|    time_elapsed    | 727    |
|    total_timesteps | 444416 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=445000, episode_reward=1.31 +/- 1.97
Episode length: 272.40 +/- 55.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 272          |
|    mean_reward          | 1.31         |
| time/                   |              |
|    total_timesteps      | 445000       |
| train/                  |              |
|    approx_kl            | 0.0046455897 |
|    clip_fraction        | 0.0211       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.39        |
|    explained_variance   | 0.841        |
|    learning_rate        | 3.53e-05     |
|    loss                 | -0.0215      |
|    n_updates            | 2170         |
|    policy_gradient_loss | -0.00341     |
|    std                  | 0.801        |
|    value_loss           | 0.00295      |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=446000, episode_reward=0.58 +/- 2.38
Episode length: 281.40 +/- 37.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 281      |
|    mean_reward     | 0.579    |
| time/              |          |
|    total_timesteps | 446000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 610    |
|    iterations      | 218    |
|    time_elapsed    | 731    |
|    total_timesteps | 446464 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=447000, episode_reward=0.64 +/- 2.57
Episode length: 293.00 +/- 14.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 293          |
|    mean_reward          | 0.638        |
| time/                   |              |
|    total_timesteps      | 447000       |
| train/                  |              |
|    approx_kl            | 0.0021203905 |
|    clip_fraction        | 0.00864      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.39        |
|    explained_variance   | 0.772        |
|    learning_rate        | 3.54e-05     |
|    loss                 | 0.0893       |
|    n_updates            | 2180         |
|    policy_gradient_loss | -0.0023      |
|    std                  | 0.8          |
|    value_loss           | 0.145        |
------------------------------------------
box reached target
Eval num_timesteps=448000, episode_reward=0.29 +/- 2.48
Episode length: 273.20 +/- 53.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 273      |
|    mean_reward     | 0.29     |
| time/              |          |
|    total_timesteps | 448000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 610    |
|    iterations      | 219    |
|    time_elapsed    | 734    |
|    total_timesteps | 448512 |
-------------------------------
box reached target
Eval num_timesteps=449000, episode_reward=0.62 +/- 2.27
Episode length: 279.40 +/- 41.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 279          |
|    mean_reward          | 0.618        |
| time/                   |              |
|    total_timesteps      | 449000       |
| train/                  |              |
|    approx_kl            | 0.0025163111 |
|    clip_fraction        | 0.0128       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.39        |
|    explained_variance   | 0.758        |
|    learning_rate        | 3.54e-05     |
|    loss                 | 0.011        |
|    n_updates            | 2190         |
|    policy_gradient_loss | -0.00242     |
|    std                  | 0.799        |
|    value_loss           | 0.0599       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=450000, episode_reward=0.28 +/- 2.56
Episode length: 275.60 +/- 48.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 276      |
|    mean_reward     | 0.278    |
| time/              |          |
|    total_timesteps | 450000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 610    |
|    iterations      | 220    |
|    time_elapsed    | 737    |
|    total_timesteps | 450560 |
-------------------------------
Eval num_timesteps=451000, episode_reward=-0.68 +/- 0.56
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.679      |
| time/                   |             |
|    total_timesteps      | 451000      |
| train/                  |             |
|    approx_kl            | 0.004658282 |
|    clip_fraction        | 0.0235      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.39       |
|    explained_variance   | 0.6         |
|    learning_rate        | 3.54e-05    |
|    loss                 | 0.178       |
|    n_updates            | 2200        |
|    policy_gradient_loss | -0.00404    |
|    std                  | 0.797       |
|    value_loss           | 0.099       |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=452000, episode_reward=0.61 +/- 2.47
Episode length: 280.00 +/- 40.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 280      |
|    mean_reward     | 0.608    |
| time/              |          |
|    total_timesteps | 452000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 610    |
|    iterations      | 221    |
|    time_elapsed    | 741    |
|    total_timesteps | 452608 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=453000, episode_reward=2.02 +/- 2.69
Episode length: 256.00 +/- 53.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 256          |
|    mean_reward          | 2.02         |
| time/                   |              |
|    total_timesteps      | 453000       |
| train/                  |              |
|    approx_kl            | 0.0022845303 |
|    clip_fraction        | 0.00972      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.38        |
|    explained_variance   | 0.818        |
|    learning_rate        | 3.54e-05     |
|    loss                 | 0.0248       |
|    n_updates            | 2210         |
|    policy_gradient_loss | -0.00246     |
|    std                  | 0.797        |
|    value_loss           | 0.0505       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=454000, episode_reward=1.72 +/- 2.85
Episode length: 250.00 +/- 61.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 250      |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 454000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 610    |
|    iterations      | 222    |
|    time_elapsed    | 744    |
|    total_timesteps | 454656 |
-------------------------------
box reached target
Eval num_timesteps=455000, episode_reward=0.85 +/- 2.48
Episode length: 287.60 +/- 24.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 288          |
|    mean_reward          | 0.845        |
| time/                   |              |
|    total_timesteps      | 455000       |
| train/                  |              |
|    approx_kl            | 0.0026300093 |
|    clip_fraction        | 0.0117       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.38        |
|    explained_variance   | 0.912        |
|    learning_rate        | 3.55e-05     |
|    loss                 | 0.0254       |
|    n_updates            | 2220         |
|    policy_gradient_loss | -0.00288     |
|    std                  | 0.797        |
|    value_loss           | 0.0251       |
------------------------------------------
box reached target
Eval num_timesteps=456000, episode_reward=0.32 +/- 2.50
Episode length: 272.40 +/- 55.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 272      |
|    mean_reward     | 0.321    |
| time/              |          |
|    total_timesteps | 456000   |
---------------------------------
box reached target
box reached target
box reached target
-------------------------------
| time/              |        |
|    fps             | 610    |
|    iterations      | 223    |
|    time_elapsed    | 747    |
|    total_timesteps | 456704 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=457000, episode_reward=1.67 +/- 2.91
Episode length: 255.20 +/- 54.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 255          |
|    mean_reward          | 1.67         |
| time/                   |              |
|    total_timesteps      | 457000       |
| train/                  |              |
|    approx_kl            | 0.0020036094 |
|    clip_fraction        | 0.00215      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.38        |
|    explained_variance   | 0.955        |
|    learning_rate        | 3.55e-05     |
|    loss                 | 0.0131       |
|    n_updates            | 2230         |
|    policy_gradient_loss | -0.00113     |
|    std                  | 0.793        |
|    value_loss           | 0.0229       |
------------------------------------------
box reached target
Eval num_timesteps=458000, episode_reward=0.90 +/- 2.20
Episode length: 279.20 +/- 41.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 279      |
|    mean_reward     | 0.901    |
| time/              |          |
|    total_timesteps | 458000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 611    |
|    iterations      | 224    |
|    time_elapsed    | 750    |
|    total_timesteps | 458752 |
-------------------------------
Eval num_timesteps=459000, episode_reward=0.00 +/- 0.36
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | 0.00287      |
| time/                   |              |
|    total_timesteps      | 459000       |
| train/                  |              |
|    approx_kl            | 0.0032259186 |
|    clip_fraction        | 0.0224       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.37        |
|    explained_variance   | 0.84         |
|    learning_rate        | 3.55e-05     |
|    loss                 | 0.0532       |
|    n_updates            | 2240         |
|    policy_gradient_loss | -0.00279     |
|    std                  | 0.793        |
|    value_loss           | 0.0747       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=460000, episode_reward=3.06 +/- 2.89
Episode length: 240.40 +/- 51.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 240      |
|    mean_reward     | 3.06     |
| time/              |          |
|    total_timesteps | 460000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 611    |
|    iterations      | 225    |
|    time_elapsed    | 753    |
|    total_timesteps | 460800 |
-------------------------------
box reached target
Eval num_timesteps=461000, episode_reward=-0.97 +/- 0.10
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.967      |
| time/                   |             |
|    total_timesteps      | 461000      |
| train/                  |             |
|    approx_kl            | 0.003446491 |
|    clip_fraction        | 0.00664     |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.37       |
|    explained_variance   | 0.975       |
|    learning_rate        | 3.55e-05    |
|    loss                 | 0.00855     |
|    n_updates            | 2250        |
|    policy_gradient_loss | -0.00274    |
|    std                  | 0.79        |
|    value_loss           | 0.00639     |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=462000, episode_reward=1.53 +/- 3.10
Episode length: 249.40 +/- 62.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 249      |
|    mean_reward     | 1.53     |
| time/              |          |
|    total_timesteps | 462000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 611    |
|    iterations      | 226    |
|    time_elapsed    | 757    |
|    total_timesteps | 462848 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=463000, episode_reward=1.42 +/- 3.03
Episode length: 246.40 +/- 66.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 246          |
|    mean_reward          | 1.42         |
| time/                   |              |
|    total_timesteps      | 463000       |
| train/                  |              |
|    approx_kl            | 0.0037982943 |
|    clip_fraction        | 0.021        |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.36        |
|    explained_variance   | 0.854        |
|    learning_rate        | 3.56e-05     |
|    loss                 | 0.0304       |
|    n_updates            | 2260         |
|    policy_gradient_loss | -0.00356     |
|    std                  | 0.789        |
|    value_loss           | 0.077        |
------------------------------------------
box reached target
Eval num_timesteps=464000, episode_reward=-0.50 +/- 0.66
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.495   |
| time/              |          |
|    total_timesteps | 464000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 611    |
|    iterations      | 227    |
|    time_elapsed    | 760    |
|    total_timesteps | 464896 |
-------------------------------
Eval num_timesteps=465000, episode_reward=-0.31 +/- 0.59
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.308       |
| time/                   |              |
|    total_timesteps      | 465000       |
| train/                  |              |
|    approx_kl            | 0.0032931808 |
|    clip_fraction        | 0.0139       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.36        |
|    explained_variance   | 0.909        |
|    learning_rate        | 3.56e-05     |
|    loss                 | 0.0214       |
|    n_updates            | 2270         |
|    policy_gradient_loss | -0.00388     |
|    std                  | 0.79         |
|    value_loss           | 0.0224       |
------------------------------------------
box reached target
Eval num_timesteps=466000, episode_reward=0.48 +/- 2.46
Episode length: 275.40 +/- 49.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 275      |
|    mean_reward     | 0.476    |
| time/              |          |
|    total_timesteps | 466000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 611    |
|    iterations      | 228    |
|    time_elapsed    | 763    |
|    total_timesteps | 466944 |
-------------------------------
box reached target
Eval num_timesteps=467000, episode_reward=0.64 +/- 2.51
Episode length: 281.20 +/- 37.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 281          |
|    mean_reward          | 0.644        |
| time/                   |              |
|    total_timesteps      | 467000       |
| train/                  |              |
|    approx_kl            | 0.0047119968 |
|    clip_fraction        | 0.0321       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.37        |
|    explained_variance   | 0.512        |
|    learning_rate        | 3.56e-05     |
|    loss                 | 0.0211       |
|    n_updates            | 2280         |
|    policy_gradient_loss | -0.0036      |
|    std                  | 0.792        |
|    value_loss           | 0.0143       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=468000, episode_reward=0.91 +/- 2.37
Episode length: 284.00 +/- 32.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 284      |
|    mean_reward     | 0.912    |
| time/              |          |
|    total_timesteps | 468000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 611    |
|    iterations      | 229    |
|    time_elapsed    | 767    |
|    total_timesteps | 468992 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=469000, episode_reward=1.94 +/- 2.62
Episode length: 248.60 +/- 63.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 249          |
|    mean_reward          | 1.94         |
| time/                   |              |
|    total_timesteps      | 469000       |
| train/                  |              |
|    approx_kl            | 0.0039337426 |
|    clip_fraction        | 0.0145       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.37        |
|    explained_variance   | 0.921        |
|    learning_rate        | 3.56e-05     |
|    loss                 | 0.00862      |
|    n_updates            | 2290         |
|    policy_gradient_loss | -0.00253     |
|    std                  | 0.79         |
|    value_loss           | 0.0343       |
------------------------------------------
box reached target
Eval num_timesteps=470000, episode_reward=0.63 +/- 2.37
Episode length: 276.20 +/- 47.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 276      |
|    mean_reward     | 0.632    |
| time/              |          |
|    total_timesteps | 470000   |
---------------------------------
box reached target
box reached target
Eval num_timesteps=471000, episode_reward=0.43 +/- 2.48
Episode length: 286.20 +/- 27.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 286      |
|    mean_reward     | 0.433    |
| time/              |          |
|    total_timesteps | 471000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 611    |
|    iterations      | 230    |
|    time_elapsed    | 770    |
|    total_timesteps | 471040 |
-------------------------------
Eval num_timesteps=472000, episode_reward=-0.70 +/- 0.59
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.704       |
| time/                   |              |
|    total_timesteps      | 472000       |
| train/                  |              |
|    approx_kl            | 0.0033094352 |
|    clip_fraction        | 0.0132       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.37        |
|    explained_variance   | 0.787        |
|    learning_rate        | 3.57e-05     |
|    loss                 | 0.0362       |
|    n_updates            | 2300         |
|    policy_gradient_loss | -0.00277     |
|    std                  | 0.79         |
|    value_loss           | 0.0544       |
------------------------------------------
Eval num_timesteps=473000, episode_reward=-0.67 +/- 0.65
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.675   |
| time/              |          |
|    total_timesteps | 473000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 610    |
|    iterations      | 231    |
|    time_elapsed    | 774    |
|    total_timesteps | 473088 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=474000, episode_reward=0.60 +/- 2.44
Episode length: 283.00 +/- 34.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 283         |
|    mean_reward          | 0.603       |
| time/                   |             |
|    total_timesteps      | 474000      |
| train/                  |             |
|    approx_kl            | 0.007171651 |
|    clip_fraction        | 0.0706      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.37       |
|    explained_variance   | 0.932       |
|    learning_rate        | 3.57e-05    |
|    loss                 | -0.0461     |
|    n_updates            | 2310        |
|    policy_gradient_loss | -0.0094     |
|    std                  | 0.79        |
|    value_loss           | 0.0039      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=475000, episode_reward=2.21 +/- 2.46
Episode length: 258.40 +/- 51.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 258      |
|    mean_reward     | 2.21     |
| time/              |          |
|    total_timesteps | 475000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 610    |
|    iterations      | 232    |
|    time_elapsed    | 777    |
|    total_timesteps | 475136 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=476000, episode_reward=-0.69 +/- 0.62
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.689       |
| time/                   |              |
|    total_timesteps      | 476000       |
| train/                  |              |
|    approx_kl            | 0.0022516772 |
|    clip_fraction        | 0.00308      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.37        |
|    explained_variance   | 0.908        |
|    learning_rate        | 3.57e-05     |
|    loss                 | 0.0166       |
|    n_updates            | 2320         |
|    policy_gradient_loss | -0.00164     |
|    std                  | 0.791        |
|    value_loss           | 0.0294       |
------------------------------------------
box reached target
Eval num_timesteps=477000, episode_reward=0.76 +/- 2.29
Episode length: 272.00 +/- 56.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 272      |
|    mean_reward     | 0.759    |
| time/              |          |
|    total_timesteps | 477000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 610    |
|    iterations      | 233    |
|    time_elapsed    | 781    |
|    total_timesteps | 477184 |
-------------------------------
box reached target
Eval num_timesteps=478000, episode_reward=0.66 +/- 2.53
Episode length: 281.20 +/- 37.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 281          |
|    mean_reward          | 0.659        |
| time/                   |              |
|    total_timesteps      | 478000       |
| train/                  |              |
|    approx_kl            | 0.0013854045 |
|    clip_fraction        | 0.00234      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.37        |
|    explained_variance   | 0.967        |
|    learning_rate        | 3.57e-05     |
|    loss                 | -0.00232     |
|    n_updates            | 2330         |
|    policy_gradient_loss | -0.00241     |
|    std                  | 0.791        |
|    value_loss           | 0.016        |
------------------------------------------
Eval num_timesteps=479000, episode_reward=-0.65 +/- 0.70
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.65    |
| time/              |          |
|    total_timesteps | 479000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 610    |
|    iterations      | 234    |
|    time_elapsed    | 784    |
|    total_timesteps | 479232 |
-------------------------------
box reached target
Eval num_timesteps=480000, episode_reward=0.45 +/- 2.50
Episode length: 279.20 +/- 41.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 279          |
|    mean_reward          | 0.446        |
| time/                   |              |
|    total_timesteps      | 480000       |
| train/                  |              |
|    approx_kl            | 0.0033760406 |
|    clip_fraction        | 0.00645      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.37        |
|    explained_variance   | 0.841        |
|    learning_rate        | 3.58e-05     |
|    loss                 | 0.000574     |
|    n_updates            | 2340         |
|    policy_gradient_loss | -0.00127     |
|    std                  | 0.792        |
|    value_loss           | 0.0132       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=481000, episode_reward=-0.44 +/- 0.69
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.435   |
| time/              |          |
|    total_timesteps | 481000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 610    |
|    iterations      | 235    |
|    time_elapsed    | 787    |
|    total_timesteps | 481280 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=482000, episode_reward=0.22 +/- 2.44
Episode length: 281.00 +/- 38.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 281        |
|    mean_reward          | 0.219      |
| time/                   |            |
|    total_timesteps      | 482000     |
| train/                  |            |
|    approx_kl            | 0.00253561 |
|    clip_fraction        | 0.00298    |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.37      |
|    explained_variance   | 0.87       |
|    learning_rate        | 3.58e-05   |
|    loss                 | 0.0172     |
|    n_updates            | 2350       |
|    policy_gradient_loss | -0.0019    |
|    std                  | 0.791      |
|    value_loss           | 0.0501     |
----------------------------------------
box reached target
box reached target
Eval num_timesteps=483000, episode_reward=1.88 +/- 3.05
Episode length: 273.20 +/- 38.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 273      |
|    mean_reward     | 1.88     |
| time/              |          |
|    total_timesteps | 483000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 610    |
|    iterations      | 236    |
|    time_elapsed    | 791    |
|    total_timesteps | 483328 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=484000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -1           |
| time/                   |              |
|    total_timesteps      | 484000       |
| train/                  |              |
|    approx_kl            | 0.0039258655 |
|    clip_fraction        | 0.0331       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.37        |
|    explained_variance   | 0.978        |
|    learning_rate        | 3.58e-05     |
|    loss                 | 0.00493      |
|    n_updates            | 2360         |
|    policy_gradient_loss | -0.00693     |
|    std                  | 0.79         |
|    value_loss           | 0.00505      |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=485000, episode_reward=1.78 +/- 2.87
Episode length: 263.60 +/- 46.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 264      |
|    mean_reward     | 1.78     |
| time/              |          |
|    total_timesteps | 485000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 611    |
|    iterations      | 237    |
|    time_elapsed    | 794    |
|    total_timesteps | 485376 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=486000, episode_reward=0.79 +/- 2.35
Episode length: 286.40 +/- 27.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 286          |
|    mean_reward          | 0.789        |
| time/                   |              |
|    total_timesteps      | 486000       |
| train/                  |              |
|    approx_kl            | 0.0028109094 |
|    clip_fraction        | 0.00762      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.36        |
|    explained_variance   | 0.797        |
|    learning_rate        | 3.58e-05     |
|    loss                 | 0.0123       |
|    n_updates            | 2370         |
|    policy_gradient_loss | -0.00197     |
|    std                  | 0.789        |
|    value_loss           | 0.0716       |
------------------------------------------
box reached target
Eval num_timesteps=487000, episode_reward=-0.88 +/- 0.24
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.88    |
| time/              |          |
|    total_timesteps | 487000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 611    |
|    iterations      | 238    |
|    time_elapsed    | 797    |
|    total_timesteps | 487424 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=488000, episode_reward=1.52 +/- 2.99
Episode length: 263.40 +/- 45.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 263          |
|    mean_reward          | 1.52         |
| time/                   |              |
|    total_timesteps      | 488000       |
| train/                  |              |
|    approx_kl            | 0.0051981374 |
|    clip_fraction        | 0.0329       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.36        |
|    explained_variance   | 0.971        |
|    learning_rate        | 3.58e-05     |
|    loss                 | 0.00234      |
|    n_updates            | 2380         |
|    policy_gradient_loss | -0.00349     |
|    std                  | 0.788        |
|    value_loss           | 0.0153       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=489000, episode_reward=2.11 +/- 2.84
Episode length: 256.80 +/- 60.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 257      |
|    mean_reward     | 2.11     |
| time/              |          |
|    total_timesteps | 489000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 611    |
|    iterations      | 239    |
|    time_elapsed    | 800    |
|    total_timesteps | 489472 |
-------------------------------
box reached target
Eval num_timesteps=490000, episode_reward=0.59 +/- 2.40
Episode length: 276.60 +/- 46.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 277         |
|    mean_reward          | 0.591       |
| time/                   |             |
|    total_timesteps      | 490000      |
| train/                  |             |
|    approx_kl            | 0.004163447 |
|    clip_fraction        | 0.0083      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.36       |
|    explained_variance   | 0.982       |
|    learning_rate        | 3.59e-05    |
|    loss                 | -0.0115     |
|    n_updates            | 2390        |
|    policy_gradient_loss | -0.00229    |
|    std                  | 0.789       |
|    value_loss           | 0.0092      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=491000, episode_reward=0.94 +/- 2.37
Episode length: 275.00 +/- 50.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 275      |
|    mean_reward     | 0.937    |
| time/              |          |
|    total_timesteps | 491000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 611    |
|    iterations      | 240    |
|    time_elapsed    | 804    |
|    total_timesteps | 491520 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=492000, episode_reward=1.77 +/- 2.83
Episode length: 248.40 +/- 63.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 248          |
|    mean_reward          | 1.77         |
| time/                   |              |
|    total_timesteps      | 492000       |
| train/                  |              |
|    approx_kl            | 0.0026334545 |
|    clip_fraction        | 0.011        |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.36        |
|    explained_variance   | 0.729        |
|    learning_rate        | 3.59e-05     |
|    loss                 | 0.0113       |
|    n_updates            | 2400         |
|    policy_gradient_loss | -0.00302     |
|    std                  | 0.788        |
|    value_loss           | 0.069        |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=493000, episode_reward=0.59 +/- 2.47
Episode length: 270.20 +/- 59.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 270      |
|    mean_reward     | 0.592    |
| time/              |          |
|    total_timesteps | 493000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 611    |
|    iterations      | 241    |
|    time_elapsed    | 807    |
|    total_timesteps | 493568 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=494000, episode_reward=2.16 +/- 2.66
Episode length: 253.40 +/- 58.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 253          |
|    mean_reward          | 2.16         |
| time/                   |              |
|    total_timesteps      | 494000       |
| train/                  |              |
|    approx_kl            | 0.0033388818 |
|    clip_fraction        | 0.0151       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.36        |
|    explained_variance   | 0.862        |
|    learning_rate        | 3.59e-05     |
|    loss                 | 0.0711       |
|    n_updates            | 2410         |
|    policy_gradient_loss | -0.00263     |
|    std                  | 0.786        |
|    value_loss           | 0.105        |
------------------------------------------
box reached target
Eval num_timesteps=495000, episode_reward=-0.13 +/- 0.58
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.127   |
| time/              |          |
|    total_timesteps | 495000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 611    |
|    iterations      | 242    |
|    time_elapsed    | 810    |
|    total_timesteps | 495616 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=496000, episode_reward=2.04 +/- 2.75
Episode length: 245.80 +/- 67.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 246          |
|    mean_reward          | 2.04         |
| time/                   |              |
|    total_timesteps      | 496000       |
| train/                  |              |
|    approx_kl            | 0.0045491452 |
|    clip_fraction        | 0.0102       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.35        |
|    explained_variance   | 0.939        |
|    learning_rate        | 3.59e-05     |
|    loss                 | 0.0104       |
|    n_updates            | 2420         |
|    policy_gradient_loss | -0.00251     |
|    std                  | 0.786        |
|    value_loss           | 0.0404       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=497000, episode_reward=2.06 +/- 2.60
Episode length: 257.40 +/- 52.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 257      |
|    mean_reward     | 2.06     |
| time/              |          |
|    total_timesteps | 497000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 611    |
|    iterations      | 243    |
|    time_elapsed    | 813    |
|    total_timesteps | 497664 |
-------------------------------
box reached target
Eval num_timesteps=498000, episode_reward=0.26 +/- 2.51
Episode length: 273.80 +/- 52.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 274         |
|    mean_reward          | 0.257       |
| time/                   |             |
|    total_timesteps      | 498000      |
| train/                  |             |
|    approx_kl            | 0.004165246 |
|    clip_fraction        | 0.0354      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.35       |
|    explained_variance   | 0.905       |
|    learning_rate        | 3.6e-05     |
|    loss                 | -0.0137     |
|    n_updates            | 2430        |
|    policy_gradient_loss | -0.00508    |
|    std                  | 0.783       |
|    value_loss           | 0.0047      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=499000, episode_reward=-0.39 +/- 0.68
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.39    |
| time/              |          |
|    total_timesteps | 499000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 611    |
|    iterations      | 244    |
|    time_elapsed    | 817    |
|    total_timesteps | 499712 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=500000, episode_reward=0.29 +/- 2.58
Episode length: 272.80 +/- 54.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 273          |
|    mean_reward          | 0.288        |
| time/                   |              |
|    total_timesteps      | 500000       |
| train/                  |              |
|    approx_kl            | 0.0037476039 |
|    clip_fraction        | 0.0162       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.35        |
|    explained_variance   | 0.934        |
|    learning_rate        | 3.6e-05      |
|    loss                 | -0.00165     |
|    n_updates            | 2440         |
|    policy_gradient_loss | -0.00679     |
|    std                  | 0.782        |
|    value_loss           | 0.0395       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=501000, episode_reward=1.78 +/- 2.86
Episode length: 269.20 +/- 38.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 269      |
|    mean_reward     | 1.78     |
| time/              |          |
|    total_timesteps | 501000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 611    |
|    iterations      | 245    |
|    time_elapsed    | 820    |
|    total_timesteps | 501760 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=502000, episode_reward=2.06 +/- 2.67
Episode length: 254.20 +/- 57.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 254          |
|    mean_reward          | 2.06         |
| time/                   |              |
|    total_timesteps      | 502000       |
| train/                  |              |
|    approx_kl            | 0.0035052798 |
|    clip_fraction        | 0.0165       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.34        |
|    explained_variance   | 0.842        |
|    learning_rate        | 3.6e-05      |
|    loss                 | 0.0515       |
|    n_updates            | 2450         |
|    policy_gradient_loss | -0.00268     |
|    std                  | 0.781        |
|    value_loss           | 0.128        |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=503000, episode_reward=1.92 +/- 2.98
Episode length: 266.00 +/- 56.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 266      |
|    mean_reward     | 1.92     |
| time/              |          |
|    total_timesteps | 503000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 611    |
|    iterations      | 246    |
|    time_elapsed    | 823    |
|    total_timesteps | 503808 |
-------------------------------
box reached target
Eval num_timesteps=504000, episode_reward=0.29 +/- 2.48
Episode length: 277.20 +/- 45.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 277          |
|    mean_reward          | 0.292        |
| time/                   |              |
|    total_timesteps      | 504000       |
| train/                  |              |
|    approx_kl            | 0.0042927423 |
|    clip_fraction        | 0.0307       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.34        |
|    explained_variance   | 0.951        |
|    learning_rate        | 3.6e-05      |
|    loss                 | -0.000187    |
|    n_updates            | 2460         |
|    policy_gradient_loss | -0.0069      |
|    std                  | 0.783        |
|    value_loss           | 0.0152       |
------------------------------------------
box reached target
Eval num_timesteps=505000, episode_reward=0.48 +/- 2.42
Episode length: 274.80 +/- 50.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 275      |
|    mean_reward     | 0.477    |
| time/              |          |
|    total_timesteps | 505000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 611    |
|    iterations      | 247    |
|    time_elapsed    | 827    |
|    total_timesteps | 505856 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=506000, episode_reward=1.48 +/- 3.04
Episode length: 258.80 +/- 51.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 259          |
|    mean_reward          | 1.48         |
| time/                   |              |
|    total_timesteps      | 506000       |
| train/                  |              |
|    approx_kl            | 0.0046556653 |
|    clip_fraction        | 0.0229       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.35        |
|    explained_variance   | 0.875        |
|    learning_rate        | 3.61e-05     |
|    loss                 | -0.0224      |
|    n_updates            | 2470         |
|    policy_gradient_loss | -0.00532     |
|    std                  | 0.783        |
|    value_loss           | 0.00512      |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=507000, episode_reward=2.19 +/- 2.69
Episode length: 261.80 +/- 48.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 262      |
|    mean_reward     | 2.19     |
| time/              |          |
|    total_timesteps | 507000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 611    |
|    iterations      | 248    |
|    time_elapsed    | 830    |
|    total_timesteps | 507904 |
-------------------------------
Eval num_timesteps=508000, episode_reward=-0.46 +/- 0.67
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.461      |
| time/                   |             |
|    total_timesteps      | 508000      |
| train/                  |             |
|    approx_kl            | 0.005915894 |
|    clip_fraction        | 0.0442      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.35       |
|    explained_variance   | 0.899       |
|    learning_rate        | 3.61e-05    |
|    loss                 | -0.0151     |
|    n_updates            | 2480        |
|    policy_gradient_loss | -0.00748    |
|    std                  | 0.784       |
|    value_loss           | 0.0145      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=509000, episode_reward=0.90 +/- 2.39
Episode length: 280.40 +/- 39.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 280      |
|    mean_reward     | 0.902    |
| time/              |          |
|    total_timesteps | 509000   |
---------------------------------
box reached target
box reached target
-------------------------------
| time/              |        |
|    fps             | 611    |
|    iterations      | 249    |
|    time_elapsed    | 833    |
|    total_timesteps | 509952 |
-------------------------------
box reached target
Eval num_timesteps=510000, episode_reward=0.49 +/- 2.40
Episode length: 276.60 +/- 46.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 277          |
|    mean_reward          | 0.493        |
| time/                   |              |
|    total_timesteps      | 510000       |
| train/                  |              |
|    approx_kl            | 0.0051295627 |
|    clip_fraction        | 0.0381       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.35        |
|    explained_variance   | 0.758        |
|    learning_rate        | 3.61e-05     |
|    loss                 | 0.0197       |
|    n_updates            | 2490         |
|    policy_gradient_loss | -0.00547     |
|    std                  | 0.781        |
|    value_loss           | 0.112        |
------------------------------------------
box reached target
Eval num_timesteps=511000, episode_reward=-0.11 +/- 0.73
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.114   |
| time/              |          |
|    total_timesteps | 511000   |
---------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=512000, episode_reward=2.89 +/- 3.01
Episode length: 241.80 +/- 50.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 242      |
|    mean_reward     | 2.89     |
| time/              |          |
|    total_timesteps | 512000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 611    |
|    iterations      | 250    |
|    time_elapsed    | 837    |
|    total_timesteps | 512000 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=513000, episode_reward=0.21 +/- 2.43
Episode length: 279.40 +/- 41.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 279          |
|    mean_reward          | 0.214        |
| time/                   |              |
|    total_timesteps      | 513000       |
| train/                  |              |
|    approx_kl            | 0.0006818194 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.34        |
|    explained_variance   | 0.731        |
|    learning_rate        | 3.61e-05     |
|    loss                 | 0.0453       |
|    n_updates            | 2500         |
|    policy_gradient_loss | -8.04e-06    |
|    std                  | 0.779        |
|    value_loss           | 0.0893       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=514000, episode_reward=0.83 +/- 2.30
Episode length: 275.60 +/- 48.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 276      |
|    mean_reward     | 0.832    |
| time/              |          |
|    total_timesteps | 514000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 611    |
|    iterations      | 251    |
|    time_elapsed    | 840    |
|    total_timesteps | 514048 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=515000, episode_reward=0.69 +/- 2.44
Episode length: 281.40 +/- 37.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 281         |
|    mean_reward          | 0.686       |
| time/                   |             |
|    total_timesteps      | 515000      |
| train/                  |             |
|    approx_kl            | 0.004527758 |
|    clip_fraction        | 0.0364      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.34       |
|    explained_variance   | 0.89        |
|    learning_rate        | 3.62e-05    |
|    loss                 | 0.0194      |
|    n_updates            | 2510        |
|    policy_gradient_loss | -0.00505    |
|    std                  | 0.779       |
|    value_loss           | 0.0376      |
-----------------------------------------
box reached target
Eval num_timesteps=516000, episode_reward=0.77 +/- 2.41
Episode length: 285.20 +/- 29.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 285      |
|    mean_reward     | 0.773    |
| time/              |          |
|    total_timesteps | 516000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 611    |
|    iterations      | 252    |
|    time_elapsed    | 844    |
|    total_timesteps | 516096 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=517000, episode_reward=0.35 +/- 2.70
Episode length: 286.00 +/- 28.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 286          |
|    mean_reward          | 0.349        |
| time/                   |              |
|    total_timesteps      | 517000       |
| train/                  |              |
|    approx_kl            | 0.0048460094 |
|    clip_fraction        | 0.0315       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.34        |
|    explained_variance   | 0.92         |
|    learning_rate        | 3.62e-05     |
|    loss                 | 0.00474      |
|    n_updates            | 2520         |
|    policy_gradient_loss | -0.00381     |
|    std                  | 0.779        |
|    value_loss           | 0.0193       |
------------------------------------------
Eval num_timesteps=518000, episode_reward=-0.40 +/- 0.50
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.402   |
| time/              |          |
|    total_timesteps | 518000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 611    |
|    iterations      | 253    |
|    time_elapsed    | 847    |
|    total_timesteps | 518144 |
-------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=519000, episode_reward=4.07 +/- 2.66
Episode length: 218.80 +/- 57.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 219         |
|    mean_reward          | 4.07        |
| time/                   |             |
|    total_timesteps      | 519000      |
| train/                  |             |
|    approx_kl            | 0.006087752 |
|    clip_fraction        | 0.0494      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.34       |
|    explained_variance   | 0.75        |
|    learning_rate        | 3.62e-05    |
|    loss                 | 0.024       |
|    n_updates            | 2530        |
|    policy_gradient_loss | -0.00421    |
|    std                  | 0.779       |
|    value_loss           | 0.0704      |
-----------------------------------------
New best mean reward!
box reached target
box reached target
Eval num_timesteps=520000, episode_reward=1.52 +/- 3.09
Episode length: 254.00 +/- 56.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 254      |
|    mean_reward     | 1.52     |
| time/              |          |
|    total_timesteps | 520000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 611    |
|    iterations      | 254    |
|    time_elapsed    | 850    |
|    total_timesteps | 520192 |
-------------------------------
box reached target
Eval num_timesteps=521000, episode_reward=0.74 +/- 2.24
Episode length: 281.40 +/- 37.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 281          |
|    mean_reward          | 0.744        |
| time/                   |              |
|    total_timesteps      | 521000       |
| train/                  |              |
|    approx_kl            | 0.0034151813 |
|    clip_fraction        | 0.0152       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.34        |
|    explained_variance   | 0.739        |
|    learning_rate        | 3.62e-05     |
|    loss                 | -0.0164      |
|    n_updates            | 2540         |
|    policy_gradient_loss | -0.00491     |
|    std                  | 0.779        |
|    value_loss           | 0.054        |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=522000, episode_reward=1.45 +/- 3.00
Episode length: 251.00 +/- 60.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | 1.45     |
| time/              |          |
|    total_timesteps | 522000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 611    |
|    iterations      | 255    |
|    time_elapsed    | 853    |
|    total_timesteps | 522240 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=523000, episode_reward=1.98 +/- 3.02
Episode length: 277.20 +/- 27.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 277          |
|    mean_reward          | 1.98         |
| time/                   |              |
|    total_timesteps      | 523000       |
| train/                  |              |
|    approx_kl            | 0.0042850063 |
|    clip_fraction        | 0.0151       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.34        |
|    explained_variance   | 0.835        |
|    learning_rate        | 3.63e-05     |
|    loss                 | -0.0121      |
|    n_updates            | 2550         |
|    policy_gradient_loss | -0.00321     |
|    std                  | 0.778        |
|    value_loss           | 0.0129       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=524000, episode_reward=3.12 +/- 2.77
Episode length: 235.20 +/- 57.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 235      |
|    mean_reward     | 3.12     |
| time/              |          |
|    total_timesteps | 524000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 611    |
|    iterations      | 256    |
|    time_elapsed    | 857    |
|    total_timesteps | 524288 |
-------------------------------
box reached target
Eval num_timesteps=525000, episode_reward=0.76 +/- 2.25
Episode length: 273.20 +/- 53.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 273          |
|    mean_reward          | 0.756        |
| time/                   |              |
|    total_timesteps      | 525000       |
| train/                  |              |
|    approx_kl            | 0.0022697556 |
|    clip_fraction        | 0.0119       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.33        |
|    explained_variance   | 0.503        |
|    learning_rate        | 3.63e-05     |
|    loss                 | -0.00866     |
|    n_updates            | 2560         |
|    policy_gradient_loss | -0.00124     |
|    std                  | 0.777        |
|    value_loss           | 0.0487       |
------------------------------------------
box reached target
Eval num_timesteps=526000, episode_reward=-0.95 +/- 0.10
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.949   |
| time/              |          |
|    total_timesteps | 526000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 611    |
|    iterations      | 257    |
|    time_elapsed    | 860    |
|    total_timesteps | 526336 |
-------------------------------
box reached target
Eval num_timesteps=527000, episode_reward=0.97 +/- 2.30
Episode length: 289.60 +/- 20.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 290          |
|    mean_reward          | 0.973        |
| time/                   |              |
|    total_timesteps      | 527000       |
| train/                  |              |
|    approx_kl            | 0.0019578622 |
|    clip_fraction        | 0.00444      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.33        |
|    explained_variance   | 0.585        |
|    learning_rate        | 3.63e-05     |
|    loss                 | 0.07         |
|    n_updates            | 2570         |
|    policy_gradient_loss | -0.0015      |
|    std                  | 0.777        |
|    value_loss           | 0.0671       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=528000, episode_reward=1.04 +/- 2.34
Episode length: 293.20 +/- 13.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 293      |
|    mean_reward     | 1.04     |
| time/              |          |
|    total_timesteps | 528000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 611    |
|    iterations      | 258    |
|    time_elapsed    | 863    |
|    total_timesteps | 528384 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=529000, episode_reward=2.68 +/- 3.12
Episode length: 229.60 +/- 57.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 230          |
|    mean_reward          | 2.68         |
| time/                   |              |
|    total_timesteps      | 529000       |
| train/                  |              |
|    approx_kl            | 0.0035203279 |
|    clip_fraction        | 0.013        |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.33        |
|    explained_variance   | 0.854        |
|    learning_rate        | 3.63e-05     |
|    loss                 | -0.0006      |
|    n_updates            | 2580         |
|    policy_gradient_loss | -0.00358     |
|    std                  | 0.774        |
|    value_loss           | 0.0356       |
------------------------------------------
box reached target
Eval num_timesteps=530000, episode_reward=0.55 +/- 2.39
Episode length: 270.00 +/- 60.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 270      |
|    mean_reward     | 0.55     |
| time/              |          |
|    total_timesteps | 530000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 611    |
|    iterations      | 259    |
|    time_elapsed    | 867    |
|    total_timesteps | 530432 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=531000, episode_reward=2.96 +/- 2.75
Episode length: 229.40 +/- 58.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 229          |
|    mean_reward          | 2.96         |
| time/                   |              |
|    total_timesteps      | 531000       |
| train/                  |              |
|    approx_kl            | 0.0047369683 |
|    clip_fraction        | 0.0285       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.33        |
|    explained_variance   | 0.864        |
|    learning_rate        | 3.64e-05     |
|    loss                 | -0.00108     |
|    n_updates            | 2590         |
|    policy_gradient_loss | -0.0055      |
|    std                  | 0.776        |
|    value_loss           | 0.00958      |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=532000, episode_reward=0.23 +/- 2.47
Episode length: 276.20 +/- 47.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 276      |
|    mean_reward     | 0.235    |
| time/              |          |
|    total_timesteps | 532000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 611    |
|    iterations      | 260    |
|    time_elapsed    | 870    |
|    total_timesteps | 532480 |
-------------------------------
box reached target
Eval num_timesteps=533000, episode_reward=0.26 +/- 2.51
Episode length: 273.60 +/- 52.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 274          |
|    mean_reward          | 0.257        |
| time/                   |              |
|    total_timesteps      | 533000       |
| train/                  |              |
|    approx_kl            | 0.0030668913 |
|    clip_fraction        | 0.0202       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.33        |
|    explained_variance   | 0.837        |
|    learning_rate        | 3.64e-05     |
|    loss                 | 0.0874       |
|    n_updates            | 2600         |
|    policy_gradient_loss | -0.00448     |
|    std                  | 0.776        |
|    value_loss           | 0.0945       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=534000, episode_reward=2.67 +/- 2.99
Episode length: 230.40 +/- 57.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 230      |
|    mean_reward     | 2.67     |
| time/              |          |
|    total_timesteps | 534000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 611    |
|    iterations      | 261    |
|    time_elapsed    | 873    |
|    total_timesteps | 534528 |
-------------------------------
Eval num_timesteps=535000, episode_reward=-0.34 +/- 0.81
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.342       |
| time/                   |              |
|    total_timesteps      | 535000       |
| train/                  |              |
|    approx_kl            | 0.0041187694 |
|    clip_fraction        | 0.0173       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.33        |
|    explained_variance   | 0.759        |
|    learning_rate        | 3.64e-05     |
|    loss                 | 0.0655       |
|    n_updates            | 2610         |
|    policy_gradient_loss | -0.00574     |
|    std                  | 0.774        |
|    value_loss           | 0.0831       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=536000, episode_reward=2.74 +/- 3.06
Episode length: 237.60 +/- 54.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 238      |
|    mean_reward     | 2.74     |
| time/              |          |
|    total_timesteps | 536000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 611    |
|    iterations      | 262    |
|    time_elapsed    | 876    |
|    total_timesteps | 536576 |
-------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=537000, episode_reward=3.18 +/- 2.77
Episode length: 242.40 +/- 49.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 242         |
|    mean_reward          | 3.18        |
| time/                   |             |
|    total_timesteps      | 537000      |
| train/                  |             |
|    approx_kl            | 0.004035595 |
|    clip_fraction        | 0.0174      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.32       |
|    explained_variance   | 0.915       |
|    learning_rate        | 3.64e-05    |
|    loss                 | -0.00154    |
|    n_updates            | 2620        |
|    policy_gradient_loss | -0.00217    |
|    std                  | 0.775       |
|    value_loss           | 0.0035      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=538000, episode_reward=0.34 +/- 2.67
Episode length: 285.80 +/- 28.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 286      |
|    mean_reward     | 0.337    |
| time/              |          |
|    total_timesteps | 538000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 612    |
|    iterations      | 263    |
|    time_elapsed    | 880    |
|    total_timesteps | 538624 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=539000, episode_reward=3.10 +/- 2.78
Episode length: 224.40 +/- 62.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 224          |
|    mean_reward          | 3.1          |
| time/                   |              |
|    total_timesteps      | 539000       |
| train/                  |              |
|    approx_kl            | 0.0053418316 |
|    clip_fraction        | 0.0227       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.32        |
|    explained_variance   | 0.857        |
|    learning_rate        | 3.65e-05     |
|    loss                 | 0.0382       |
|    n_updates            | 2630         |
|    policy_gradient_loss | -0.00557     |
|    std                  | 0.772        |
|    value_loss           | 0.0486       |
------------------------------------------
box reached target
Eval num_timesteps=540000, episode_reward=0.89 +/- 2.49
Episode length: 284.60 +/- 30.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 285      |
|    mean_reward     | 0.887    |
| time/              |          |
|    total_timesteps | 540000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 612    |
|    iterations      | 264    |
|    time_elapsed    | 883    |
|    total_timesteps | 540672 |
-------------------------------
Eval num_timesteps=541000, episode_reward=-0.52 +/- 0.68
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.519       |
| time/                   |              |
|    total_timesteps      | 541000       |
| train/                  |              |
|    approx_kl            | 0.0041944953 |
|    clip_fraction        | 0.0146       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.32        |
|    explained_variance   | 0.654        |
|    learning_rate        | 3.65e-05     |
|    loss                 | 0.0261       |
|    n_updates            | 2640         |
|    policy_gradient_loss | -0.00216     |
|    std                  | 0.77         |
|    value_loss           | 0.0507       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=542000, episode_reward=2.90 +/- 2.73
Episode length: 229.00 +/- 58.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 229      |
|    mean_reward     | 2.9      |
| time/              |          |
|    total_timesteps | 542000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 612    |
|    iterations      | 265    |
|    time_elapsed    | 886    |
|    total_timesteps | 542720 |
-------------------------------
Eval num_timesteps=543000, episode_reward=-0.72 +/- 0.55
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.723       |
| time/                   |              |
|    total_timesteps      | 543000       |
| train/                  |              |
|    approx_kl            | 0.0038881302 |
|    clip_fraction        | 0.00693      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.31        |
|    explained_variance   | 0.788        |
|    learning_rate        | 3.65e-05     |
|    loss                 | 0.0349       |
|    n_updates            | 2650         |
|    policy_gradient_loss | -0.00244     |
|    std                  | 0.769        |
|    value_loss           | 0.0863       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=544000, episode_reward=0.31 +/- 2.62
Episode length: 284.80 +/- 30.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 285      |
|    mean_reward     | 0.312    |
| time/              |          |
|    total_timesteps | 544000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 612    |
|    iterations      | 266    |
|    time_elapsed    | 889    |
|    total_timesteps | 544768 |
-------------------------------
box reached target
Eval num_timesteps=545000, episode_reward=0.48 +/- 2.42
Episode length: 272.60 +/- 54.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 273          |
|    mean_reward          | 0.48         |
| time/                   |              |
|    total_timesteps      | 545000       |
| train/                  |              |
|    approx_kl            | 0.0039001652 |
|    clip_fraction        | 0.0293       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.31        |
|    explained_variance   | 0.657        |
|    learning_rate        | 3.65e-05     |
|    loss                 | -0.0093      |
|    n_updates            | 2660         |
|    policy_gradient_loss | -0.00398     |
|    std                  | 0.769        |
|    value_loss           | 0.0687       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=546000, episode_reward=1.69 +/- 3.01
Episode length: 247.60 +/- 67.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 248      |
|    mean_reward     | 1.69     |
| time/              |          |
|    total_timesteps | 546000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 612    |
|    iterations      | 267    |
|    time_elapsed    | 893    |
|    total_timesteps | 546816 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=547000, episode_reward=2.01 +/- 2.67
Episode length: 245.60 +/- 67.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 246          |
|    mean_reward          | 2.01         |
| time/                   |              |
|    total_timesteps      | 547000       |
| train/                  |              |
|    approx_kl            | 0.0031415555 |
|    clip_fraction        | 0.00962      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.31        |
|    explained_variance   | 0.879        |
|    learning_rate        | 3.66e-05     |
|    loss                 | 0.00983      |
|    n_updates            | 2670         |
|    policy_gradient_loss | -0.00469     |
|    std                  | 0.768        |
|    value_loss           | 0.0429       |
------------------------------------------
box reached target
Eval num_timesteps=548000, episode_reward=0.59 +/- 2.49
Episode length: 283.60 +/- 32.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 284      |
|    mean_reward     | 0.587    |
| time/              |          |
|    total_timesteps | 548000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 612    |
|    iterations      | 268    |
|    time_elapsed    | 896    |
|    total_timesteps | 548864 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=549000, episode_reward=0.71 +/- 2.30
Episode length: 272.00 +/- 56.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 272          |
|    mean_reward          | 0.708        |
| time/                   |              |
|    total_timesteps      | 549000       |
| train/                  |              |
|    approx_kl            | 0.0029332535 |
|    clip_fraction        | 0.00459      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.31        |
|    explained_variance   | 0.637        |
|    learning_rate        | 3.66e-05     |
|    loss                 | 0.0283       |
|    n_updates            | 2680         |
|    policy_gradient_loss | -0.000897    |
|    std                  | 0.766        |
|    value_loss           | 0.0871       |
------------------------------------------
Eval num_timesteps=550000, episode_reward=-0.08 +/- 0.47
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.0807  |
| time/              |          |
|    total_timesteps | 550000   |
---------------------------------
box reached target
box reached target
-------------------------------
| time/              |        |
|    fps             | 612    |
|    iterations      | 269    |
|    time_elapsed    | 899    |
|    total_timesteps | 550912 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=551000, episode_reward=2.91 +/- 3.20
Episode length: 273.20 +/- 32.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 273          |
|    mean_reward          | 2.91         |
| time/                   |              |
|    total_timesteps      | 551000       |
| train/                  |              |
|    approx_kl            | 0.0042880615 |
|    clip_fraction        | 0.0293       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.3         |
|    explained_variance   | 0.929        |
|    learning_rate        | 3.66e-05     |
|    loss                 | 0.038        |
|    n_updates            | 2690         |
|    policy_gradient_loss | -0.00588     |
|    std                  | 0.765        |
|    value_loss           | 0.0333       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=552000, episode_reward=-0.56 +/- 0.55
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.557   |
| time/              |          |
|    total_timesteps | 552000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 612    |
|    iterations      | 270    |
|    time_elapsed    | 903    |
|    total_timesteps | 552960 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=553000, episode_reward=2.11 +/- 2.76
Episode length: 259.80 +/- 50.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 260          |
|    mean_reward          | 2.11         |
| time/                   |              |
|    total_timesteps      | 553000       |
| train/                  |              |
|    approx_kl            | 0.0029938663 |
|    clip_fraction        | 0.0171       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.3         |
|    explained_variance   | 0.868        |
|    learning_rate        | 3.66e-05     |
|    loss                 | 0.00764      |
|    n_updates            | 2700         |
|    policy_gradient_loss | -0.00349     |
|    std                  | 0.764        |
|    value_loss           | 0.0548       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=554000, episode_reward=2.01 +/- 2.60
Episode length: 270.00 +/- 39.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 270      |
|    mean_reward     | 2.01     |
| time/              |          |
|    total_timesteps | 554000   |
---------------------------------
box reached target
box reached target
Eval num_timesteps=555000, episode_reward=2.35 +/- 2.57
Episode length: 266.80 +/- 43.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 267      |
|    mean_reward     | 2.35     |
| time/              |          |
|    total_timesteps | 555000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 612    |
|    iterations      | 271    |
|    time_elapsed    | 906    |
|    total_timesteps | 555008 |
-------------------------------
box reached target
Eval num_timesteps=556000, episode_reward=0.88 +/- 2.29
Episode length: 276.40 +/- 47.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 276         |
|    mean_reward          | 0.878       |
| time/                   |             |
|    total_timesteps      | 556000      |
| train/                  |             |
|    approx_kl            | 0.003959894 |
|    clip_fraction        | 0.0194      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.3        |
|    explained_variance   | 0.881       |
|    learning_rate        | 3.67e-05    |
|    loss                 | 0.0182      |
|    n_updates            | 2710        |
|    policy_gradient_loss | -0.00426    |
|    std                  | 0.764       |
|    value_loss           | 0.0328      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=557000, episode_reward=2.95 +/- 2.74
Episode length: 217.60 +/- 67.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 218      |
|    mean_reward     | 2.95     |
| time/              |          |
|    total_timesteps | 557000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 612    |
|    iterations      | 272    |
|    time_elapsed    | 910    |
|    total_timesteps | 557056 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=558000, episode_reward=0.42 +/- 2.45
Episode length: 271.00 +/- 58.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 271         |
|    mean_reward          | 0.416       |
| time/                   |             |
|    total_timesteps      | 558000      |
| train/                  |             |
|    approx_kl            | 0.004349985 |
|    clip_fraction        | 0.0231      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.3        |
|    explained_variance   | 0.846       |
|    learning_rate        | 3.67e-05    |
|    loss                 | 0.00606     |
|    n_updates            | 2720        |
|    policy_gradient_loss | -0.00315    |
|    std                  | 0.766       |
|    value_loss           | 0.00664     |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=559000, episode_reward=5.38 +/- 0.13
Episode length: 212.60 +/- 35.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 213      |
|    mean_reward     | 5.38     |
| time/              |          |
|    total_timesteps | 559000   |
---------------------------------
New best mean reward!
-------------------------------
| time/              |        |
|    fps             | 612    |
|    iterations      | 273    |
|    time_elapsed    | 913    |
|    total_timesteps | 559104 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=560000, episode_reward=1.48 +/- 3.20
Episode length: 259.40 +/- 50.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 259         |
|    mean_reward          | 1.48        |
| time/                   |             |
|    total_timesteps      | 560000      |
| train/                  |             |
|    approx_kl            | 0.003631085 |
|    clip_fraction        | 0.0129      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.3        |
|    explained_variance   | 0.808       |
|    learning_rate        | 3.67e-05    |
|    loss                 | -0.00197    |
|    n_updates            | 2730        |
|    policy_gradient_loss | -0.00288    |
|    std                  | 0.766       |
|    value_loss           | 0.105       |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=561000, episode_reward=2.21 +/- 2.56
Episode length: 244.40 +/- 68.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 244      |
|    mean_reward     | 2.21     |
| time/              |          |
|    total_timesteps | 561000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 612    |
|    iterations      | 274    |
|    time_elapsed    | 916    |
|    total_timesteps | 561152 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=562000, episode_reward=0.66 +/- 2.33
Episode length: 273.20 +/- 53.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 273          |
|    mean_reward          | 0.665        |
| time/                   |              |
|    total_timesteps      | 562000       |
| train/                  |              |
|    approx_kl            | 0.0035900176 |
|    clip_fraction        | 0.0221       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.3         |
|    explained_variance   | 0.863        |
|    learning_rate        | 3.67e-05     |
|    loss                 | -0.00495     |
|    n_updates            | 2740         |
|    policy_gradient_loss | -0.00431     |
|    std                  | 0.768        |
|    value_loss           | 0.016        |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=563000, episode_reward=2.91 +/- 2.81
Episode length: 237.80 +/- 54.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 238      |
|    mean_reward     | 2.91     |
| time/              |          |
|    total_timesteps | 563000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 612    |
|    iterations      | 275    |
|    time_elapsed    | 919    |
|    total_timesteps | 563200 |
-------------------------------
box reached target
Eval num_timesteps=564000, episode_reward=0.93 +/- 2.35
Episode length: 271.60 +/- 56.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 272          |
|    mean_reward          | 0.929        |
| time/                   |              |
|    total_timesteps      | 564000       |
| train/                  |              |
|    approx_kl            | 0.0034406667 |
|    clip_fraction        | 0.0121       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.31        |
|    explained_variance   | 0.826        |
|    learning_rate        | 3.68e-05     |
|    loss                 | 0.019        |
|    n_updates            | 2750         |
|    policy_gradient_loss | -0.00181     |
|    std                  | 0.768        |
|    value_loss           | 0.05         |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=565000, episode_reward=2.05 +/- 2.81
Episode length: 256.40 +/- 64.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 256      |
|    mean_reward     | 2.05     |
| time/              |          |
|    total_timesteps | 565000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 612    |
|    iterations      | 276    |
|    time_elapsed    | 922    |
|    total_timesteps | 565248 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=566000, episode_reward=1.77 +/- 2.92
Episode length: 257.60 +/- 52.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 258         |
|    mean_reward          | 1.77        |
| time/                   |             |
|    total_timesteps      | 566000      |
| train/                  |             |
|    approx_kl            | 0.004853225 |
|    clip_fraction        | 0.0259      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.3        |
|    explained_variance   | 0.49        |
|    learning_rate        | 3.68e-05    |
|    loss                 | -0.00112    |
|    n_updates            | 2760        |
|    policy_gradient_loss | -0.00305    |
|    std                  | 0.766       |
|    value_loss           | 0.106       |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=567000, episode_reward=1.77 +/- 2.81
Episode length: 248.60 +/- 63.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 249      |
|    mean_reward     | 1.77     |
| time/              |          |
|    total_timesteps | 567000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 612    |
|    iterations      | 277    |
|    time_elapsed    | 926    |
|    total_timesteps | 567296 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=568000, episode_reward=2.26 +/- 2.49
Episode length: 268.40 +/- 38.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 268         |
|    mean_reward          | 2.26        |
| time/                   |             |
|    total_timesteps      | 568000      |
| train/                  |             |
|    approx_kl            | 0.005783935 |
|    clip_fraction        | 0.0519      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.3        |
|    explained_variance   | 0.929       |
|    learning_rate        | 3.68e-05    |
|    loss                 | 0.0136      |
|    n_updates            | 2770        |
|    policy_gradient_loss | -0.00709    |
|    std                  | 0.766       |
|    value_loss           | 0.0159      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=569000, episode_reward=0.25 +/- 2.51
Episode length: 274.60 +/- 50.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 275      |
|    mean_reward     | 0.253    |
| time/              |          |
|    total_timesteps | 569000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 612    |
|    iterations      | 278    |
|    time_elapsed    | 929    |
|    total_timesteps | 569344 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=570000, episode_reward=1.59 +/- 3.18
Episode length: 262.00 +/- 55.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 262         |
|    mean_reward          | 1.59        |
| time/                   |             |
|    total_timesteps      | 570000      |
| train/                  |             |
|    approx_kl            | 0.004742073 |
|    clip_fraction        | 0.0285      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.3        |
|    explained_variance   | 0.836       |
|    learning_rate        | 3.68e-05    |
|    loss                 | 0.0289      |
|    n_updates            | 2780        |
|    policy_gradient_loss | -0.00593    |
|    std                  | 0.765       |
|    value_loss           | 0.128       |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=571000, episode_reward=3.27 +/- 2.26
Episode length: 255.20 +/- 45.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 255      |
|    mean_reward     | 3.27     |
| time/              |          |
|    total_timesteps | 571000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 612    |
|    iterations      | 279    |
|    time_elapsed    | 932    |
|    total_timesteps | 571392 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=572000, episode_reward=2.13 +/- 2.56
Episode length: 259.80 +/- 49.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 260          |
|    mean_reward          | 2.13         |
| time/                   |              |
|    total_timesteps      | 572000       |
| train/                  |              |
|    approx_kl            | 0.0027591516 |
|    clip_fraction        | 0.00962      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.3         |
|    explained_variance   | 0.512        |
|    learning_rate        | 3.69e-05     |
|    loss                 | 0.0204       |
|    n_updates            | 2790         |
|    policy_gradient_loss | -0.0024      |
|    std                  | 0.764        |
|    value_loss           | 0.0898       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=573000, episode_reward=2.95 +/- 3.03
Episode length: 238.60 +/- 52.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 239      |
|    mean_reward     | 2.95     |
| time/              |          |
|    total_timesteps | 573000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 612    |
|    iterations      | 280    |
|    time_elapsed    | 935    |
|    total_timesteps | 573440 |
-------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=574000, episode_reward=3.05 +/- 2.64
Episode length: 223.40 +/- 63.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 223         |
|    mean_reward          | 3.05        |
| time/                   |             |
|    total_timesteps      | 574000      |
| train/                  |             |
|    approx_kl            | 0.004740541 |
|    clip_fraction        | 0.0292      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.3        |
|    explained_variance   | 0.865       |
|    learning_rate        | 3.69e-05    |
|    loss                 | -0.00425    |
|    n_updates            | 2800        |
|    policy_gradient_loss | -0.00492    |
|    std                  | 0.763       |
|    value_loss           | 0.054       |
-----------------------------------------
box reached target
Eval num_timesteps=575000, episode_reward=0.72 +/- 2.34
Episode length: 282.60 +/- 34.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 283      |
|    mean_reward     | 0.724    |
| time/              |          |
|    total_timesteps | 575000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 281    |
|    time_elapsed    | 938    |
|    total_timesteps | 575488 |
-------------------------------
Eval num_timesteps=576000, episode_reward=-0.14 +/- 0.59
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.138       |
| time/                   |              |
|    total_timesteps      | 576000       |
| train/                  |              |
|    approx_kl            | 0.0039632693 |
|    clip_fraction        | 0.0134       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.29        |
|    explained_variance   | 0.903        |
|    learning_rate        | 3.69e-05     |
|    loss                 | 0.0433       |
|    n_updates            | 2810         |
|    policy_gradient_loss | -0.00312     |
|    std                  | 0.762        |
|    value_loss           | 0.0463       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=577000, episode_reward=0.74 +/- 2.30
Episode length: 274.80 +/- 50.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 275      |
|    mean_reward     | 0.738    |
| time/              |          |
|    total_timesteps | 577000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 282    |
|    time_elapsed    | 942    |
|    total_timesteps | 577536 |
-------------------------------
Eval num_timesteps=578000, episode_reward=-0.53 +/- 0.58
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.528       |
| time/                   |              |
|    total_timesteps      | 578000       |
| train/                  |              |
|    approx_kl            | 0.0040516993 |
|    clip_fraction        | 0.0194       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.29        |
|    explained_variance   | 0.841        |
|    learning_rate        | 3.69e-05     |
|    loss                 | 0.0432       |
|    n_updates            | 2820         |
|    policy_gradient_loss | -0.00276     |
|    std                  | 0.76         |
|    value_loss           | 0.0373       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=579000, episode_reward=1.82 +/- 2.72
Episode length: 258.80 +/- 51.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 259      |
|    mean_reward     | 1.82     |
| time/              |          |
|    total_timesteps | 579000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 283    |
|    time_elapsed    | 945    |
|    total_timesteps | 579584 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=580000, episode_reward=2.85 +/- 3.15
Episode length: 251.60 +/- 45.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 252          |
|    mean_reward          | 2.85         |
| time/                   |              |
|    total_timesteps      | 580000       |
| train/                  |              |
|    approx_kl            | 0.0029027155 |
|    clip_fraction        | 0.00908      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.29        |
|    explained_variance   | 0.937        |
|    learning_rate        | 3.7e-05      |
|    loss                 | 0.00958      |
|    n_updates            | 2830         |
|    policy_gradient_loss | -0.00204     |
|    std                  | 0.76         |
|    value_loss           | 0.0267       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=581000, episode_reward=1.23 +/- 2.14
Episode length: 293.40 +/- 13.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 293      |
|    mean_reward     | 1.23     |
| time/              |          |
|    total_timesteps | 581000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 284    |
|    time_elapsed    | 948    |
|    total_timesteps | 581632 |
-------------------------------
box reached target
Eval num_timesteps=582000, episode_reward=-0.17 +/- 0.66
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.173       |
| time/                   |              |
|    total_timesteps      | 582000       |
| train/                  |              |
|    approx_kl            | 0.0028591189 |
|    clip_fraction        | 0.0142       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.28        |
|    explained_variance   | 0.905        |
|    learning_rate        | 3.7e-05      |
|    loss                 | 0.0107       |
|    n_updates            | 2840         |
|    policy_gradient_loss | -0.0026      |
|    std                  | 0.757        |
|    value_loss           | 0.0617       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=583000, episode_reward=2.77 +/- 3.08
Episode length: 240.80 +/- 48.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 241      |
|    mean_reward     | 2.77     |
| time/              |          |
|    total_timesteps | 583000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 285    |
|    time_elapsed    | 951    |
|    total_timesteps | 583680 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=584000, episode_reward=2.00 +/- 2.56
Episode length: 246.20 +/- 66.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 246         |
|    mean_reward          | 2           |
| time/                   |             |
|    total_timesteps      | 584000      |
| train/                  |             |
|    approx_kl            | 0.005738452 |
|    clip_fraction        | 0.0395      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.28       |
|    explained_variance   | 0.852       |
|    learning_rate        | 3.7e-05     |
|    loss                 | 0.00186     |
|    n_updates            | 2850        |
|    policy_gradient_loss | -0.0055     |
|    std                  | 0.755       |
|    value_loss           | 0.0292      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=585000, episode_reward=0.33 +/- 2.42
Episode length: 270.20 +/- 59.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 270      |
|    mean_reward     | 0.325    |
| time/              |          |
|    total_timesteps | 585000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 286    |
|    time_elapsed    | 955    |
|    total_timesteps | 585728 |
-------------------------------
Eval num_timesteps=586000, episode_reward=-0.08 +/- 0.68
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.0821     |
| time/                   |             |
|    total_timesteps      | 586000      |
| train/                  |             |
|    approx_kl            | 0.005120679 |
|    clip_fraction        | 0.0247      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.27       |
|    explained_variance   | 0.795       |
|    learning_rate        | 3.7e-05     |
|    loss                 | 0.0834      |
|    n_updates            | 2860        |
|    policy_gradient_loss | -0.00221    |
|    std                  | 0.754       |
|    value_loss           | 0.11        |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=587000, episode_reward=2.76 +/- 3.08
Episode length: 229.60 +/- 59.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 230      |
|    mean_reward     | 2.76     |
| time/              |          |
|    total_timesteps | 587000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 287    |
|    time_elapsed    | 958    |
|    total_timesteps | 587776 |
-------------------------------
box reached target
Eval num_timesteps=588000, episode_reward=0.50 +/- 2.43
Episode length: 279.00 +/- 42.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 279          |
|    mean_reward          | 0.502        |
| time/                   |              |
|    total_timesteps      | 588000       |
| train/                  |              |
|    approx_kl            | 0.0031509832 |
|    clip_fraction        | 0.0125       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.27        |
|    explained_variance   | 0.861        |
|    learning_rate        | 3.71e-05     |
|    loss                 | -0.012       |
|    n_updates            | 2870         |
|    policy_gradient_loss | -0.0033      |
|    std                  | 0.753        |
|    value_loss           | 0.0322       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=589000, episode_reward=4.25 +/- 1.91
Episode length: 204.40 +/- 49.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 204      |
|    mean_reward     | 4.25     |
| time/              |          |
|    total_timesteps | 589000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 288    |
|    time_elapsed    | 961    |
|    total_timesteps | 589824 |
-------------------------------
box reached target
Eval num_timesteps=590000, episode_reward=0.59 +/- 2.46
Episode length: 285.20 +/- 29.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 285         |
|    mean_reward          | 0.594       |
| time/                   |             |
|    total_timesteps      | 590000      |
| train/                  |             |
|    approx_kl            | 0.005579164 |
|    clip_fraction        | 0.0341      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.27       |
|    explained_variance   | 0.725       |
|    learning_rate        | 3.71e-05    |
|    loss                 | 0.0062      |
|    n_updates            | 2880        |
|    policy_gradient_loss | -0.00547    |
|    std                  | 0.754       |
|    value_loss           | 0.0586      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=591000, episode_reward=1.97 +/- 2.80
Episode length: 255.40 +/- 55.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 255      |
|    mean_reward     | 1.97     |
| time/              |          |
|    total_timesteps | 591000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 289    |
|    time_elapsed    | 964    |
|    total_timesteps | 591872 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=592000, episode_reward=1.79 +/- 2.93
Episode length: 251.80 +/- 59.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 252          |
|    mean_reward          | 1.79         |
| time/                   |              |
|    total_timesteps      | 592000       |
| train/                  |              |
|    approx_kl            | 0.0029492618 |
|    clip_fraction        | 0.00796      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.27        |
|    explained_variance   | 0.854        |
|    learning_rate        | 3.71e-05     |
|    loss                 | 0.000495     |
|    n_updates            | 2890         |
|    policy_gradient_loss | -0.00152     |
|    std                  | 0.752        |
|    value_loss           | 0.101        |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=593000, episode_reward=2.84 +/- 3.14
Episode length: 234.80 +/- 58.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 235      |
|    mean_reward     | 2.84     |
| time/              |          |
|    total_timesteps | 593000   |
---------------------------------
box reached target
box reached target
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 290    |
|    time_elapsed    | 967    |
|    total_timesteps | 593920 |
-------------------------------
box reached target
Eval num_timesteps=594000, episode_reward=0.48 +/- 2.41
Episode length: 275.00 +/- 50.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 275          |
|    mean_reward          | 0.481        |
| time/                   |              |
|    total_timesteps      | 594000       |
| train/                  |              |
|    approx_kl            | 0.0051305327 |
|    clip_fraction        | 0.0361       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.27        |
|    explained_variance   | 0.951        |
|    learning_rate        | 3.71e-05     |
|    loss                 | 0.0323       |
|    n_updates            | 2900         |
|    policy_gradient_loss | -0.00533     |
|    std                  | 0.752        |
|    value_loss           | 0.0303       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=595000, episode_reward=1.75 +/- 2.86
Episode length: 248.40 +/- 63.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 248      |
|    mean_reward     | 1.75     |
| time/              |          |
|    total_timesteps | 595000   |
---------------------------------
box reached target
box reached target
box reached target
box reached target
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 291    |
|    time_elapsed    | 971    |
|    total_timesteps | 595968 |
-------------------------------
box reached target
Eval num_timesteps=596000, episode_reward=0.29 +/- 2.53
Episode length: 280.80 +/- 38.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 281         |
|    mean_reward          | 0.29        |
| time/                   |             |
|    total_timesteps      | 596000      |
| train/                  |             |
|    approx_kl            | 0.003562685 |
|    clip_fraction        | 0.022       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.26       |
|    explained_variance   | 0.773       |
|    learning_rate        | 3.72e-05    |
|    loss                 | 0.0867      |
|    n_updates            | 2910        |
|    policy_gradient_loss | -0.00453    |
|    std                  | 0.749       |
|    value_loss           | 0.209       |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=597000, episode_reward=0.51 +/- 2.61
Episode length: 276.40 +/- 47.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 276      |
|    mean_reward     | 0.51     |
| time/              |          |
|    total_timesteps | 597000   |
---------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=598000, episode_reward=1.82 +/- 2.87
Episode length: 254.20 +/- 57.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 254      |
|    mean_reward     | 1.82     |
| time/              |          |
|    total_timesteps | 598000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 292    |
|    time_elapsed    | 974    |
|    total_timesteps | 598016 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=599000, episode_reward=-0.30 +/- 0.59
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.295      |
| time/                   |             |
|    total_timesteps      | 599000      |
| train/                  |             |
|    approx_kl            | 0.004916745 |
|    clip_fraction        | 0.0327      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.25       |
|    explained_variance   | 0.895       |
|    learning_rate        | 3.72e-05    |
|    loss                 | -0.00261    |
|    n_updates            | 2920        |
|    policy_gradient_loss | -0.00461    |
|    std                  | 0.747       |
|    value_loss           | 0.0555      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=600000, episode_reward=0.64 +/- 2.47
Episode length: 280.20 +/- 39.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 280      |
|    mean_reward     | 0.642    |
| time/              |          |
|    total_timesteps | 600000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 293    |
|    time_elapsed    | 978    |
|    total_timesteps | 600064 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=601000, episode_reward=0.53 +/- 2.39
Episode length: 269.00 +/- 62.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 269          |
|    mean_reward          | 0.529        |
| time/                   |              |
|    total_timesteps      | 601000       |
| train/                  |              |
|    approx_kl            | 0.0044526625 |
|    clip_fraction        | 0.0183       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.25        |
|    explained_variance   | 0.925        |
|    learning_rate        | 3.72e-05     |
|    loss                 | 0.00451      |
|    n_updates            | 2930         |
|    policy_gradient_loss | -0.00342     |
|    std                  | 0.746        |
|    value_loss           | 0.0674       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=602000, episode_reward=3.04 +/- 2.62
Episode length: 228.60 +/- 58.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 229      |
|    mean_reward     | 3.04     |
| time/              |          |
|    total_timesteps | 602000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 294    |
|    time_elapsed    | 981    |
|    total_timesteps | 602112 |
-------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=603000, episode_reward=4.10 +/- 2.56
Episode length: 206.20 +/- 49.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | 4.1          |
| time/                   |              |
|    total_timesteps      | 603000       |
| train/                  |              |
|    approx_kl            | 0.0042186957 |
|    clip_fraction        | 0.0247       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.25        |
|    explained_variance   | 0.788        |
|    learning_rate        | 3.72e-05     |
|    loss                 | 0.00819      |
|    n_updates            | 2940         |
|    policy_gradient_loss | -0.00531     |
|    std                  | 0.744        |
|    value_loss           | 0.121        |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=604000, episode_reward=2.10 +/- 2.73
Episode length: 269.40 +/- 37.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 269      |
|    mean_reward     | 2.1      |
| time/              |          |
|    total_timesteps | 604000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 295    |
|    time_elapsed    | 984    |
|    total_timesteps | 604160 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=605000, episode_reward=1.98 +/- 2.74
Episode length: 247.40 +/- 66.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 247         |
|    mean_reward          | 1.98        |
| time/                   |             |
|    total_timesteps      | 605000      |
| train/                  |             |
|    approx_kl            | 0.004099873 |
|    clip_fraction        | 0.0248      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.24       |
|    explained_variance   | 0.822       |
|    learning_rate        | 3.72e-05    |
|    loss                 | -0.00206    |
|    n_updates            | 2950        |
|    policy_gradient_loss | -0.00437    |
|    std                  | 0.743       |
|    value_loss           | 0.147       |
-----------------------------------------
box reached target
Eval num_timesteps=606000, episode_reward=1.03 +/- 2.28
Episode length: 282.40 +/- 35.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 282      |
|    mean_reward     | 1.03     |
| time/              |          |
|    total_timesteps | 606000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 296    |
|    time_elapsed    | 987    |
|    total_timesteps | 606208 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=607000, episode_reward=0.92 +/- 2.49
Episode length: 287.20 +/- 25.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 287          |
|    mean_reward          | 0.917        |
| time/                   |              |
|    total_timesteps      | 607000       |
| train/                  |              |
|    approx_kl            | 0.0040919255 |
|    clip_fraction        | 0.0191       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.24        |
|    explained_variance   | 0.813        |
|    learning_rate        | 3.73e-05     |
|    loss                 | 0.00362      |
|    n_updates            | 2960         |
|    policy_gradient_loss | -0.00408     |
|    std                  | 0.743        |
|    value_loss           | 0.0373       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=608000, episode_reward=1.50 +/- 3.12
Episode length: 266.40 +/- 46.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 266      |
|    mean_reward     | 1.5      |
| time/              |          |
|    total_timesteps | 608000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 297    |
|    time_elapsed    | 991    |
|    total_timesteps | 608256 |
-------------------------------
box reached target
Eval num_timesteps=609000, episode_reward=0.97 +/- 2.34
Episode length: 281.60 +/- 36.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 282          |
|    mean_reward          | 0.968        |
| time/                   |              |
|    total_timesteps      | 609000       |
| train/                  |              |
|    approx_kl            | 0.0040901313 |
|    clip_fraction        | 0.0179       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.24        |
|    explained_variance   | 0.892        |
|    learning_rate        | 3.73e-05     |
|    loss                 | 0.00166      |
|    n_updates            | 2970         |
|    policy_gradient_loss | -0.00271     |
|    std                  | 0.742        |
|    value_loss           | 0.0528       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=610000, episode_reward=1.66 +/- 3.04
Episode length: 261.60 +/- 47.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 262      |
|    mean_reward     | 1.66     |
| time/              |          |
|    total_timesteps | 610000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 298    |
|    time_elapsed    | 994    |
|    total_timesteps | 610304 |
-------------------------------
box reached target
Eval num_timesteps=611000, episode_reward=-0.57 +/- 0.63
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.571      |
| time/                   |             |
|    total_timesteps      | 611000      |
| train/                  |             |
|    approx_kl            | 0.004694795 |
|    clip_fraction        | 0.0188      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.24       |
|    explained_variance   | 0.859       |
|    learning_rate        | 3.73e-05    |
|    loss                 | -0.00992    |
|    n_updates            | 2980        |
|    policy_gradient_loss | -0.00264    |
|    std                  | 0.742       |
|    value_loss           | 0.0302      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=612000, episode_reward=0.48 +/- 2.42
Episode length: 271.60 +/- 56.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 272      |
|    mean_reward     | 0.477    |
| time/              |          |
|    total_timesteps | 612000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 299    |
|    time_elapsed    | 997    |
|    total_timesteps | 612352 |
-------------------------------
box reached target
Eval num_timesteps=613000, episode_reward=1.38 +/- 2.12
Episode length: 294.20 +/- 11.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 294          |
|    mean_reward          | 1.38         |
| time/                   |              |
|    total_timesteps      | 613000       |
| train/                  |              |
|    approx_kl            | 0.0034566459 |
|    clip_fraction        | 0.0176       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.24        |
|    explained_variance   | 0.955        |
|    learning_rate        | 3.73e-05     |
|    loss                 | 0.0158       |
|    n_updates            | 2990         |
|    policy_gradient_loss | -0.00221     |
|    std                  | 0.743        |
|    value_loss           | 0.0202       |
------------------------------------------
box reached target
Eval num_timesteps=614000, episode_reward=0.69 +/- 2.37
Episode length: 282.20 +/- 35.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 282      |
|    mean_reward     | 0.691    |
| time/              |          |
|    total_timesteps | 614000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 300    |
|    time_elapsed    | 1001   |
|    total_timesteps | 614400 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=615000, episode_reward=2.03 +/- 2.88
Episode length: 276.40 +/- 36.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 276          |
|    mean_reward          | 2.03         |
| time/                   |              |
|    total_timesteps      | 615000       |
| train/                  |              |
|    approx_kl            | 0.0033792825 |
|    clip_fraction        | 0.0181       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.24        |
|    explained_variance   | 0.679        |
|    learning_rate        | 3.74e-05     |
|    loss                 | 0.00433      |
|    n_updates            | 3000         |
|    policy_gradient_loss | -0.00241     |
|    std                  | 0.742        |
|    value_loss           | 0.0683       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=616000, episode_reward=1.98 +/- 2.66
Episode length: 261.80 +/- 49.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 262      |
|    mean_reward     | 1.98     |
| time/              |          |
|    total_timesteps | 616000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 301    |
|    time_elapsed    | 1004   |
|    total_timesteps | 616448 |
-------------------------------
box reached target
Eval num_timesteps=617000, episode_reward=0.48 +/- 2.51
Episode length: 280.60 +/- 38.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 281          |
|    mean_reward          | 0.482        |
| time/                   |              |
|    total_timesteps      | 617000       |
| train/                  |              |
|    approx_kl            | 0.0024741942 |
|    clip_fraction        | 0.0125       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.24        |
|    explained_variance   | 0.893        |
|    learning_rate        | 3.74e-05     |
|    loss                 | 0.0567       |
|    n_updates            | 3010         |
|    policy_gradient_loss | -0.00141     |
|    std                  | 0.741        |
|    value_loss           | 0.0301       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=618000, episode_reward=1.91 +/- 2.83
Episode length: 266.80 +/- 41.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 267      |
|    mean_reward     | 1.91     |
| time/              |          |
|    total_timesteps | 618000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 302    |
|    time_elapsed    | 1007   |
|    total_timesteps | 618496 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=619000, episode_reward=1.06 +/- 2.31
Episode length: 291.20 +/- 17.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 291         |
|    mean_reward          | 1.06        |
| time/                   |             |
|    total_timesteps      | 619000      |
| train/                  |             |
|    approx_kl            | 0.004396781 |
|    clip_fraction        | 0.0307      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.24       |
|    explained_variance   | 0.775       |
|    learning_rate        | 3.74e-05    |
|    loss                 | 0.0168      |
|    n_updates            | 3020        |
|    policy_gradient_loss | -0.00414    |
|    std                  | 0.74        |
|    value_loss           | 0.0411      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=620000, episode_reward=1.81 +/- 2.80
Episode length: 245.20 +/- 67.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 245      |
|    mean_reward     | 1.81     |
| time/              |          |
|    total_timesteps | 620000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 303    |
|    time_elapsed    | 1011   |
|    total_timesteps | 620544 |
-------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=621000, episode_reward=3.93 +/- 2.47
Episode length: 196.00 +/- 53.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 196          |
|    mean_reward          | 3.93         |
| time/                   |              |
|    total_timesteps      | 621000       |
| train/                  |              |
|    approx_kl            | 0.0031775378 |
|    clip_fraction        | 0.0186       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.23        |
|    explained_variance   | 0.781        |
|    learning_rate        | 3.74e-05     |
|    loss                 | 0.0622       |
|    n_updates            | 3030         |
|    policy_gradient_loss | -0.00192     |
|    std                  | 0.739        |
|    value_loss           | 0.111        |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=622000, episode_reward=1.66 +/- 3.00
Episode length: 246.60 +/- 65.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 247      |
|    mean_reward     | 1.66     |
| time/              |          |
|    total_timesteps | 622000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 304    |
|    time_elapsed    | 1014   |
|    total_timesteps | 622592 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=623000, episode_reward=1.23 +/- 2.12
Episode length: 278.80 +/- 42.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 279          |
|    mean_reward          | 1.23         |
| time/                   |              |
|    total_timesteps      | 623000       |
| train/                  |              |
|    approx_kl            | 0.0042136135 |
|    clip_fraction        | 0.0185       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.23        |
|    explained_variance   | 0.878        |
|    learning_rate        | 3.75e-05     |
|    loss                 | 0.00308      |
|    n_updates            | 3040         |
|    policy_gradient_loss | -0.00354     |
|    std                  | 0.74         |
|    value_loss           | 0.0671       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=624000, episode_reward=1.55 +/- 3.14
Episode length: 262.20 +/- 48.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 262      |
|    mean_reward     | 1.55     |
| time/              |          |
|    total_timesteps | 624000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 305    |
|    time_elapsed    | 1017   |
|    total_timesteps | 624640 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=625000, episode_reward=2.04 +/- 2.65
Episode length: 251.00 +/- 61.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 251         |
|    mean_reward          | 2.04        |
| time/                   |             |
|    total_timesteps      | 625000      |
| train/                  |             |
|    approx_kl            | 0.005596895 |
|    clip_fraction        | 0.0364      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.23       |
|    explained_variance   | 0.592       |
|    learning_rate        | 3.75e-05    |
|    loss                 | 0.0322      |
|    n_updates            | 3050        |
|    policy_gradient_loss | -0.00622    |
|    std                  | 0.738       |
|    value_loss           | 0.0809      |
-----------------------------------------
box reached target
Eval num_timesteps=626000, episode_reward=0.86 +/- 2.31
Episode length: 283.20 +/- 33.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 283      |
|    mean_reward     | 0.864    |
| time/              |          |
|    total_timesteps | 626000   |
---------------------------------
box reached target
box reached target
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 306    |
|    time_elapsed    | 1020   |
|    total_timesteps | 626688 |
-------------------------------
Eval num_timesteps=627000, episode_reward=0.25 +/- 0.59
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | 0.251       |
| time/                   |             |
|    total_timesteps      | 627000      |
| train/                  |             |
|    approx_kl            | 0.004933872 |
|    clip_fraction        | 0.0321      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.23       |
|    explained_variance   | 0.927       |
|    learning_rate        | 3.75e-05    |
|    loss                 | 0.00579     |
|    n_updates            | 3060        |
|    policy_gradient_loss | -0.00521    |
|    std                  | 0.737       |
|    value_loss           | 0.0325      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=628000, episode_reward=2.01 +/- 2.61
Episode length: 254.60 +/- 55.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 255      |
|    mean_reward     | 2.01     |
| time/              |          |
|    total_timesteps | 628000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 307    |
|    time_elapsed    | 1024   |
|    total_timesteps | 628736 |
-------------------------------
box reached target
Eval num_timesteps=629000, episode_reward=0.40 +/- 2.57
Episode length: 286.40 +/- 27.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 286          |
|    mean_reward          | 0.395        |
| time/                   |              |
|    total_timesteps      | 629000       |
| train/                  |              |
|    approx_kl            | 0.0038211637 |
|    clip_fraction        | 0.0224       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.23        |
|    explained_variance   | 0.764        |
|    learning_rate        | 3.75e-05     |
|    loss                 | 0.00429      |
|    n_updates            | 3070         |
|    policy_gradient_loss | -0.00399     |
|    std                  | 0.739        |
|    value_loss           | 0.0624       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=630000, episode_reward=1.81 +/- 2.91
Episode length: 254.80 +/- 56.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 255      |
|    mean_reward     | 1.81     |
| time/              |          |
|    total_timesteps | 630000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 308    |
|    time_elapsed    | 1027   |
|    total_timesteps | 630784 |
-------------------------------
Eval num_timesteps=631000, episode_reward=-0.17 +/- 0.76
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.168       |
| time/                   |              |
|    total_timesteps      | 631000       |
| train/                  |              |
|    approx_kl            | 0.0036084992 |
|    clip_fraction        | 0.0182       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.23        |
|    explained_variance   | 0.664        |
|    learning_rate        | 3.76e-05     |
|    loss                 | 0.00523      |
|    n_updates            | 3080         |
|    policy_gradient_loss | -0.0039      |
|    std                  | 0.737        |
|    value_loss           | 0.071        |
------------------------------------------
box reached target
Eval num_timesteps=632000, episode_reward=-0.50 +/- 0.61
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.499   |
| time/              |          |
|    total_timesteps | 632000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 309    |
|    time_elapsed    | 1030   |
|    total_timesteps | 632832 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=633000, episode_reward=1.99 +/- 2.65
Episode length: 260.40 +/- 48.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 260         |
|    mean_reward          | 1.99        |
| time/                   |             |
|    total_timesteps      | 633000      |
| train/                  |             |
|    approx_kl            | 0.004195524 |
|    clip_fraction        | 0.0318      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.22       |
|    explained_variance   | 0.909       |
|    learning_rate        | 3.76e-05    |
|    loss                 | 0.00132     |
|    n_updates            | 3090        |
|    policy_gradient_loss | -0.00455    |
|    std                  | 0.733       |
|    value_loss           | 0.0365      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=634000, episode_reward=0.71 +/- 2.35
Episode length: 279.60 +/- 40.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 280      |
|    mean_reward     | 0.709    |
| time/              |          |
|    total_timesteps | 634000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 310    |
|    time_elapsed    | 1034   |
|    total_timesteps | 634880 |
-------------------------------
box reached target
Eval num_timesteps=635000, episode_reward=0.87 +/- 2.45
Episode length: 289.00 +/- 22.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 289          |
|    mean_reward          | 0.865        |
| time/                   |              |
|    total_timesteps      | 635000       |
| train/                  |              |
|    approx_kl            | 0.0030531422 |
|    clip_fraction        | 0.0128       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.22        |
|    explained_variance   | 0.65         |
|    learning_rate        | 3.76e-05     |
|    loss                 | -0.0202      |
|    n_updates            | 3100         |
|    policy_gradient_loss | -0.00266     |
|    std                  | 0.734        |
|    value_loss           | 0.102        |
------------------------------------------
Eval num_timesteps=636000, episode_reward=-0.34 +/- 0.81
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.337   |
| time/              |          |
|    total_timesteps | 636000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 311    |
|    time_elapsed    | 1037   |
|    total_timesteps | 636928 |
-------------------------------
box reached target
Eval num_timesteps=637000, episode_reward=0.71 +/- 2.28
Episode length: 274.20 +/- 51.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 274          |
|    mean_reward          | 0.706        |
| time/                   |              |
|    total_timesteps      | 637000       |
| train/                  |              |
|    approx_kl            | 0.0029997653 |
|    clip_fraction        | 0.0137       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.22        |
|    explained_variance   | 0.83         |
|    learning_rate        | 3.76e-05     |
|    loss                 | 0.00277      |
|    n_updates            | 3110         |
|    policy_gradient_loss | -0.00342     |
|    std                  | 0.734        |
|    value_loss           | 0.0391       |
------------------------------------------
box reached target
Eval num_timesteps=638000, episode_reward=0.88 +/- 2.48
Episode length: 281.40 +/- 37.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 281      |
|    mean_reward     | 0.885    |
| time/              |          |
|    total_timesteps | 638000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 312    |
|    time_elapsed    | 1040   |
|    total_timesteps | 638976 |
-------------------------------
Eval num_timesteps=639000, episode_reward=0.05 +/- 0.44
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | 0.0516       |
| time/                   |              |
|    total_timesteps      | 639000       |
| train/                  |              |
|    approx_kl            | 0.0037850146 |
|    clip_fraction        | 0.0251       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.21        |
|    explained_variance   | 0.863        |
|    learning_rate        | 3.77e-05     |
|    loss                 | 0.0137       |
|    n_updates            | 3120         |
|    policy_gradient_loss | -0.00383     |
|    std                  | 0.732        |
|    value_loss           | 0.00748      |
------------------------------------------
box reached target
Eval num_timesteps=640000, episode_reward=-0.86 +/- 0.23
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.86    |
| time/              |          |
|    total_timesteps | 640000   |
---------------------------------
box reached target
Eval num_timesteps=641000, episode_reward=0.66 +/- 2.35
Episode length: 273.40 +/- 53.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 273      |
|    mean_reward     | 0.664    |
| time/              |          |
|    total_timesteps | 641000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 313    |
|    time_elapsed    | 1044   |
|    total_timesteps | 641024 |
-------------------------------
Eval num_timesteps=642000, episode_reward=-0.65 +/- 0.61
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.65       |
| time/                   |             |
|    total_timesteps      | 642000      |
| train/                  |             |
|    approx_kl            | 0.005026267 |
|    clip_fraction        | 0.0347      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.21       |
|    explained_variance   | 0.805       |
|    learning_rate        | 3.77e-05    |
|    loss                 | -0.0312     |
|    n_updates            | 3130        |
|    policy_gradient_loss | -0.00464    |
|    std                  | 0.73        |
|    value_loss           | 0.0672      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=643000, episode_reward=3.03 +/- 2.82
Episode length: 246.40 +/- 50.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 246      |
|    mean_reward     | 3.03     |
| time/              |          |
|    total_timesteps | 643000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 314    |
|    time_elapsed    | 1048   |
|    total_timesteps | 643072 |
-------------------------------
Eval num_timesteps=644000, episode_reward=-0.52 +/- 0.60
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.515      |
| time/                   |             |
|    total_timesteps      | 644000      |
| train/                  |             |
|    approx_kl            | 0.004313054 |
|    clip_fraction        | 0.0233      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.2        |
|    explained_variance   | 0.878       |
|    learning_rate        | 3.77e-05    |
|    loss                 | -0.00996    |
|    n_updates            | 3140        |
|    policy_gradient_loss | -0.00455    |
|    std                  | 0.728       |
|    value_loss           | 0.00773     |
-----------------------------------------
box reached target
Eval num_timesteps=645000, episode_reward=-0.20 +/- 0.66
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.202   |
| time/              |          |
|    total_timesteps | 645000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 315    |
|    time_elapsed    | 1051   |
|    total_timesteps | 645120 |
-------------------------------
box reached target
Eval num_timesteps=646000, episode_reward=0.75 +/- 2.30
Episode length: 272.20 +/- 55.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 272          |
|    mean_reward          | 0.748        |
| time/                   |              |
|    total_timesteps      | 646000       |
| train/                  |              |
|    approx_kl            | 0.0020914818 |
|    clip_fraction        | 0.00508      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.2         |
|    explained_variance   | 0.975        |
|    learning_rate        | 3.77e-05     |
|    loss                 | -0.00905     |
|    n_updates            | 3150         |
|    policy_gradient_loss | -0.00209     |
|    std                  | 0.726        |
|    value_loss           | 0.00538      |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=647000, episode_reward=1.55 +/- 3.13
Episode length: 263.20 +/- 49.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 263      |
|    mean_reward     | 1.55     |
| time/              |          |
|    total_timesteps | 647000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 316    |
|    time_elapsed    | 1054   |
|    total_timesteps | 647168 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=648000, episode_reward=1.64 +/- 3.23
Episode length: 275.60 +/- 31.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 276          |
|    mean_reward          | 1.64         |
| time/                   |              |
|    total_timesteps      | 648000       |
| train/                  |              |
|    approx_kl            | 0.0028798468 |
|    clip_fraction        | 0.0116       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.19        |
|    explained_variance   | 0.853        |
|    learning_rate        | 3.78e-05     |
|    loss                 | 0.0157       |
|    n_updates            | 3160         |
|    policy_gradient_loss | -0.00294     |
|    std                  | 0.725        |
|    value_loss           | 0.023        |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=649000, episode_reward=3.06 +/- 2.82
Episode length: 227.00 +/- 60.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 227      |
|    mean_reward     | 3.06     |
| time/              |          |
|    total_timesteps | 649000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 317    |
|    time_elapsed    | 1058   |
|    total_timesteps | 649216 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=650000, episode_reward=1.22 +/- 2.00
Episode length: 273.00 +/- 54.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 273        |
|    mean_reward          | 1.22       |
| time/                   |            |
|    total_timesteps      | 650000     |
| train/                  |            |
|    approx_kl            | 0.00372876 |
|    clip_fraction        | 0.0235     |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.19      |
|    explained_variance   | 0.857      |
|    learning_rate        | 3.78e-05   |
|    loss                 | -0.0112    |
|    n_updates            | 3170       |
|    policy_gradient_loss | -0.00362   |
|    std                  | 0.723      |
|    value_loss           | 0.00897    |
----------------------------------------
box reached target
Eval num_timesteps=651000, episode_reward=0.45 +/- 2.40
Episode length: 273.60 +/- 52.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 274      |
|    mean_reward     | 0.453    |
| time/              |          |
|    total_timesteps | 651000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 318    |
|    time_elapsed    | 1061   |
|    total_timesteps | 651264 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=652000, episode_reward=-0.36 +/- 0.79
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.363      |
| time/                   |             |
|    total_timesteps      | 652000      |
| train/                  |             |
|    approx_kl            | 0.002926444 |
|    clip_fraction        | 0.0193      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.19       |
|    explained_variance   | 0.914       |
|    learning_rate        | 3.78e-05    |
|    loss                 | 0.0266      |
|    n_updates            | 3180        |
|    policy_gradient_loss | -0.0037     |
|    std                  | 0.722       |
|    value_loss           | 0.0401      |
-----------------------------------------
box reached target
Eval num_timesteps=653000, episode_reward=0.84 +/- 2.36
Episode length: 275.60 +/- 48.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 276      |
|    mean_reward     | 0.836    |
| time/              |          |
|    total_timesteps | 653000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 319    |
|    time_elapsed    | 1064   |
|    total_timesteps | 653312 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=654000, episode_reward=1.60 +/- 3.10
Episode length: 258.20 +/- 53.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 258          |
|    mean_reward          | 1.6          |
| time/                   |              |
|    total_timesteps      | 654000       |
| train/                  |              |
|    approx_kl            | 0.0063842284 |
|    clip_fraction        | 0.0649       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.18        |
|    explained_variance   | 0.922        |
|    learning_rate        | 3.78e-05     |
|    loss                 | -0.000947    |
|    n_updates            | 3190         |
|    policy_gradient_loss | -0.00799     |
|    std                  | 0.723        |
|    value_loss           | 0.0375       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=655000, episode_reward=1.49 +/- 3.04
Episode length: 250.40 +/- 60.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 250      |
|    mean_reward     | 1.49     |
| time/              |          |
|    total_timesteps | 655000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 320    |
|    time_elapsed    | 1067   |
|    total_timesteps | 655360 |
-------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=656000, episode_reward=4.13 +/- 2.57
Episode length: 222.60 +/- 40.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 223         |
|    mean_reward          | 4.13        |
| time/                   |             |
|    total_timesteps      | 656000      |
| train/                  |             |
|    approx_kl            | 0.004925083 |
|    clip_fraction        | 0.0195      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.19       |
|    explained_variance   | 0.979       |
|    learning_rate        | 3.79e-05    |
|    loss                 | 0.02        |
|    n_updates            | 3200        |
|    policy_gradient_loss | -0.00303    |
|    std                  | 0.722       |
|    value_loss           | 0.00732     |
-----------------------------------------
box reached target
Eval num_timesteps=657000, episode_reward=0.54 +/- 2.55
Episode length: 289.80 +/- 20.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 290      |
|    mean_reward     | 0.536    |
| time/              |          |
|    total_timesteps | 657000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 321    |
|    time_elapsed    | 1071   |
|    total_timesteps | 657408 |
-------------------------------
box reached target
Eval num_timesteps=658000, episode_reward=0.46 +/- 2.40
Episode length: 279.40 +/- 41.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 279         |
|    mean_reward          | 0.46        |
| time/                   |             |
|    total_timesteps      | 658000      |
| train/                  |             |
|    approx_kl            | 0.003088986 |
|    clip_fraction        | 0.0238      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.18       |
|    explained_variance   | 0.727       |
|    learning_rate        | 3.79e-05    |
|    loss                 | 0.0025      |
|    n_updates            | 3210        |
|    policy_gradient_loss | -0.00415    |
|    std                  | 0.722       |
|    value_loss           | 0.00528     |
-----------------------------------------
box reached target
Eval num_timesteps=659000, episode_reward=0.53 +/- 2.43
Episode length: 274.80 +/- 50.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 275      |
|    mean_reward     | 0.53     |
| time/              |          |
|    total_timesteps | 659000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 322    |
|    time_elapsed    | 1074   |
|    total_timesteps | 659456 |
-------------------------------
box reached target
Eval num_timesteps=660000, episode_reward=0.63 +/- 2.35
Episode length: 273.20 +/- 53.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 273         |
|    mean_reward          | 0.631       |
| time/                   |             |
|    total_timesteps      | 660000      |
| train/                  |             |
|    approx_kl            | 0.003951287 |
|    clip_fraction        | 0.0187      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.18       |
|    explained_variance   | 0.813       |
|    learning_rate        | 3.79e-05    |
|    loss                 | -0.00466    |
|    n_updates            | 3220        |
|    policy_gradient_loss | -0.0032     |
|    std                  | 0.722       |
|    value_loss           | 0.00591     |
-----------------------------------------
box reached target
Eval num_timesteps=661000, episode_reward=0.44 +/- 2.57
Episode length: 277.80 +/- 44.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | 0.436    |
| time/              |          |
|    total_timesteps | 661000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 323    |
|    time_elapsed    | 1077   |
|    total_timesteps | 661504 |
-------------------------------
box reached target
Eval num_timesteps=662000, episode_reward=1.17 +/- 2.24
Episode length: 279.00 +/- 42.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 279         |
|    mean_reward          | 1.17        |
| time/                   |             |
|    total_timesteps      | 662000      |
| train/                  |             |
|    approx_kl            | 0.006468098 |
|    clip_fraction        | 0.0521      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.18       |
|    explained_variance   | 0.831       |
|    learning_rate        | 3.79e-05    |
|    loss                 | -0.0193     |
|    n_updates            | 3230        |
|    policy_gradient_loss | -0.00787    |
|    std                  | 0.721       |
|    value_loss           | 0.00267     |
-----------------------------------------
Eval num_timesteps=663000, episode_reward=-0.56 +/- 0.57
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.561   |
| time/              |          |
|    total_timesteps | 663000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 324    |
|    time_elapsed    | 1081   |
|    total_timesteps | 663552 |
-------------------------------
box reached target
Eval num_timesteps=664000, episode_reward=-0.97 +/- 0.06
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.969      |
| time/                   |             |
|    total_timesteps      | 664000      |
| train/                  |             |
|    approx_kl            | 0.003951692 |
|    clip_fraction        | 0.0117      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.18       |
|    explained_variance   | 0.872       |
|    learning_rate        | 3.8e-05     |
|    loss                 | 0.0125      |
|    n_updates            | 3240        |
|    policy_gradient_loss | -0.00281    |
|    std                  | 0.722       |
|    value_loss           | 0.00584     |
-----------------------------------------
Eval num_timesteps=665000, episode_reward=-0.57 +/- 0.58
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.572   |
| time/              |          |
|    total_timesteps | 665000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 325    |
|    time_elapsed    | 1084   |
|    total_timesteps | 665600 |
-------------------------------
box reached target
Eval num_timesteps=666000, episode_reward=0.67 +/- 2.41
Episode length: 280.40 +/- 39.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 280          |
|    mean_reward          | 0.672        |
| time/                   |              |
|    total_timesteps      | 666000       |
| train/                  |              |
|    approx_kl            | 0.0044231545 |
|    clip_fraction        | 0.016        |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.18        |
|    explained_variance   | 0.755        |
|    learning_rate        | 3.8e-05      |
|    loss                 | 0.00123      |
|    n_updates            | 3250         |
|    policy_gradient_loss | -0.00444     |
|    std                  | 0.72         |
|    value_loss           | 0.0369       |
------------------------------------------
box reached target
Eval num_timesteps=667000, episode_reward=0.02 +/- 0.70
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | 0.0193   |
| time/              |          |
|    total_timesteps | 667000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 326    |
|    time_elapsed    | 1088   |
|    total_timesteps | 667648 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=668000, episode_reward=2.46 +/- 2.41
Episode length: 275.20 +/- 30.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 275         |
|    mean_reward          | 2.46        |
| time/                   |             |
|    total_timesteps      | 668000      |
| train/                  |             |
|    approx_kl            | 0.002945112 |
|    clip_fraction        | 0.00708     |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.18       |
|    explained_variance   | 0.472       |
|    learning_rate        | 3.8e-05     |
|    loss                 | -0.0112     |
|    n_updates            | 3260        |
|    policy_gradient_loss | -0.00362    |
|    std                  | 0.719       |
|    value_loss           | 0.0604      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=669000, episode_reward=2.09 +/- 2.74
Episode length: 270.00 +/- 38.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 270      |
|    mean_reward     | 2.09     |
| time/              |          |
|    total_timesteps | 669000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 327    |
|    time_elapsed    | 1091   |
|    total_timesteps | 669696 |
-------------------------------
box reached target
Eval num_timesteps=670000, episode_reward=1.16 +/- 2.19
Episode length: 271.20 +/- 57.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 271         |
|    mean_reward          | 1.16        |
| time/                   |             |
|    total_timesteps      | 670000      |
| train/                  |             |
|    approx_kl            | 0.003855599 |
|    clip_fraction        | 0.0195      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.18       |
|    explained_variance   | 0.799       |
|    learning_rate        | 3.8e-05     |
|    loss                 | 0.00411     |
|    n_updates            | 3270        |
|    policy_gradient_loss | -0.00345    |
|    std                  | 0.721       |
|    value_loss           | 0.01        |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=671000, episode_reward=1.54 +/- 3.11
Episode length: 263.00 +/- 45.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 263      |
|    mean_reward     | 1.54     |
| time/              |          |
|    total_timesteps | 671000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 328    |
|    time_elapsed    | 1094   |
|    total_timesteps | 671744 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=672000, episode_reward=0.83 +/- 2.33
Episode length: 283.40 +/- 33.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 283          |
|    mean_reward          | 0.832        |
| time/                   |              |
|    total_timesteps      | 672000       |
| train/                  |              |
|    approx_kl            | 0.0053690015 |
|    clip_fraction        | 0.0397       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.18        |
|    explained_variance   | 0.894        |
|    learning_rate        | 3.81e-05     |
|    loss                 | -0.00265     |
|    n_updates            | 3280         |
|    policy_gradient_loss | -0.00614     |
|    std                  | 0.722        |
|    value_loss           | 0.00634      |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=673000, episode_reward=2.00 +/- 2.88
Episode length: 267.40 +/- 42.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 267      |
|    mean_reward     | 2        |
| time/              |          |
|    total_timesteps | 673000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 329    |
|    time_elapsed    | 1097   |
|    total_timesteps | 673792 |
-------------------------------
Eval num_timesteps=674000, episode_reward=-0.73 +/- 0.55
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.73       |
| time/                   |             |
|    total_timesteps      | 674000      |
| train/                  |             |
|    approx_kl            | 0.003688963 |
|    clip_fraction        | 0.0176      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.18       |
|    explained_variance   | 0.871       |
|    learning_rate        | 3.81e-05    |
|    loss                 | 0.00331     |
|    n_updates            | 3290        |
|    policy_gradient_loss | -0.0032     |
|    std                  | 0.72        |
|    value_loss           | 0.0464      |
-----------------------------------------
Eval num_timesteps=675000, episode_reward=-0.47 +/- 0.65
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.466   |
| time/              |          |
|    total_timesteps | 675000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 330    |
|    time_elapsed    | 1101   |
|    total_timesteps | 675840 |
-------------------------------
box reached target
Eval num_timesteps=676000, episode_reward=0.86 +/- 2.30
Episode length: 272.60 +/- 54.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 273          |
|    mean_reward          | 0.862        |
| time/                   |              |
|    total_timesteps      | 676000       |
| train/                  |              |
|    approx_kl            | 0.0026713216 |
|    clip_fraction        | 0.0084       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.18        |
|    explained_variance   | 0.749        |
|    learning_rate        | 3.81e-05     |
|    loss                 | -0.0155      |
|    n_updates            | 3300         |
|    policy_gradient_loss | -0.0039      |
|    std                  | 0.721        |
|    value_loss           | 0.00935      |
------------------------------------------
box reached target
Eval num_timesteps=677000, episode_reward=-0.47 +/- 0.73
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.466   |
| time/              |          |
|    total_timesteps | 677000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 331    |
|    time_elapsed    | 1104   |
|    total_timesteps | 677888 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=678000, episode_reward=1.47 +/- 3.03
Episode length: 246.20 +/- 66.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 246          |
|    mean_reward          | 1.47         |
| time/                   |              |
|    total_timesteps      | 678000       |
| train/                  |              |
|    approx_kl            | 0.0041024853 |
|    clip_fraction        | 0.0249       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.18        |
|    explained_variance   | 0.924        |
|    learning_rate        | 3.81e-05     |
|    loss                 | 0.00132      |
|    n_updates            | 3310         |
|    policy_gradient_loss | -0.00459     |
|    std                  | 0.72         |
|    value_loss           | 0.0168       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=679000, episode_reward=0.33 +/- 2.39
Episode length: 275.80 +/- 48.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 276      |
|    mean_reward     | 0.332    |
| time/              |          |
|    total_timesteps | 679000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 332    |
|    time_elapsed    | 1107   |
|    total_timesteps | 679936 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=680000, episode_reward=1.58 +/- 3.17
Episode length: 282.60 +/- 21.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 283          |
|    mean_reward          | 1.58         |
| time/                   |              |
|    total_timesteps      | 680000       |
| train/                  |              |
|    approx_kl            | 0.0025600512 |
|    clip_fraction        | 0.00532      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.17        |
|    explained_variance   | 0.786        |
|    learning_rate        | 3.82e-05     |
|    loss                 | 0.0184       |
|    n_updates            | 3320         |
|    policy_gradient_loss | -0.00338     |
|    std                  | 0.718        |
|    value_loss           | 0.102        |
------------------------------------------
Eval num_timesteps=681000, episode_reward=-0.36 +/- 0.60
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.36    |
| time/              |          |
|    total_timesteps | 681000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 333    |
|    time_elapsed    | 1111   |
|    total_timesteps | 681984 |
-------------------------------
Eval num_timesteps=682000, episode_reward=-0.93 +/- 0.14
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.928       |
| time/                   |              |
|    total_timesteps      | 682000       |
| train/                  |              |
|    approx_kl            | 0.0031765676 |
|    clip_fraction        | 0.0134       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.17        |
|    explained_variance   | 0.921        |
|    learning_rate        | 3.82e-05     |
|    loss                 | 0.000148     |
|    n_updates            | 3330         |
|    policy_gradient_loss | -0.00244     |
|    std                  | 0.717        |
|    value_loss           | 0.00584      |
------------------------------------------
box reached target
Eval num_timesteps=683000, episode_reward=0.28 +/- 2.56
Episode length: 277.40 +/- 45.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 277      |
|    mean_reward     | 0.278    |
| time/              |          |
|    total_timesteps | 683000   |
---------------------------------
box reached target
Eval num_timesteps=684000, episode_reward=-0.61 +/- 0.36
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.613   |
| time/              |          |
|    total_timesteps | 684000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 334    |
|    time_elapsed    | 1115   |
|    total_timesteps | 684032 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=685000, episode_reward=0.71 +/- 2.24
Episode length: 277.00 +/- 46.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 277          |
|    mean_reward          | 0.714        |
| time/                   |              |
|    total_timesteps      | 685000       |
| train/                  |              |
|    approx_kl            | 0.0066536097 |
|    clip_fraction        | 0.0737       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.17        |
|    explained_variance   | 0.961        |
|    learning_rate        | 3.82e-05     |
|    loss                 | -0.0178      |
|    n_updates            | 3340         |
|    policy_gradient_loss | -0.00846     |
|    std                  | 0.718        |
|    value_loss           | 0.0087       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=686000, episode_reward=2.15 +/- 2.83
Episode length: 290.20 +/- 12.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 290      |
|    mean_reward     | 2.15     |
| time/              |          |
|    total_timesteps | 686000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 335    |
|    time_elapsed    | 1118   |
|    total_timesteps | 686080 |
-------------------------------
box reached target
Eval num_timesteps=687000, episode_reward=0.41 +/- 2.39
Episode length: 271.20 +/- 57.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 271          |
|    mean_reward          | 0.408        |
| time/                   |              |
|    total_timesteps      | 687000       |
| train/                  |              |
|    approx_kl            | 0.0031816666 |
|    clip_fraction        | 0.013        |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.17        |
|    explained_variance   | 0.76         |
|    learning_rate        | 3.82e-05     |
|    loss                 | 0.0574       |
|    n_updates            | 3350         |
|    policy_gradient_loss | -0.00283     |
|    std                  | 0.717        |
|    value_loss           | 0.148        |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=688000, episode_reward=0.49 +/- 2.40
Episode length: 284.60 +/- 30.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 285      |
|    mean_reward     | 0.488    |
| time/              |          |
|    total_timesteps | 688000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 336    |
|    time_elapsed    | 1122   |
|    total_timesteps | 688128 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=689000, episode_reward=1.60 +/- 3.05
Episode length: 255.00 +/- 55.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 255          |
|    mean_reward          | 1.6          |
| time/                   |              |
|    total_timesteps      | 689000       |
| train/                  |              |
|    approx_kl            | 0.0026281388 |
|    clip_fraction        | 0.00625      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.17        |
|    explained_variance   | 0.746        |
|    learning_rate        | 3.83e-05     |
|    loss                 | -0.00306     |
|    n_updates            | 3360         |
|    policy_gradient_loss | -0.00268     |
|    std                  | 0.716        |
|    value_loss           | 0.0815       |
------------------------------------------
box reached target
Eval num_timesteps=690000, episode_reward=0.45 +/- 2.46
Episode length: 273.00 +/- 54.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 273      |
|    mean_reward     | 0.447    |
| time/              |          |
|    total_timesteps | 690000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 337    |
|    time_elapsed    | 1125   |
|    total_timesteps | 690176 |
-------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=691000, episode_reward=2.78 +/- 3.09
Episode length: 247.80 +/- 50.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 248          |
|    mean_reward          | 2.78         |
| time/                   |              |
|    total_timesteps      | 691000       |
| train/                  |              |
|    approx_kl            | 0.0062425504 |
|    clip_fraction        | 0.0514       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.17        |
|    explained_variance   | 0.893        |
|    learning_rate        | 3.83e-05     |
|    loss                 | 0.0142       |
|    n_updates            | 3370         |
|    policy_gradient_loss | -0.0063      |
|    std                  | 0.716        |
|    value_loss           | 0.016        |
------------------------------------------
Eval num_timesteps=692000, episode_reward=-0.60 +/- 0.66
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.596   |
| time/              |          |
|    total_timesteps | 692000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 338    |
|    time_elapsed    | 1128   |
|    total_timesteps | 692224 |
-------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=693000, episode_reward=2.70 +/- 3.02
Episode length: 228.80 +/- 59.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 229         |
|    mean_reward          | 2.7         |
| time/                   |             |
|    total_timesteps      | 693000      |
| train/                  |             |
|    approx_kl            | 0.004926055 |
|    clip_fraction        | 0.0404      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.16       |
|    explained_variance   | 0.772       |
|    learning_rate        | 3.83e-05    |
|    loss                 | 0.000907    |
|    n_updates            | 3380        |
|    policy_gradient_loss | -0.00658    |
|    std                  | 0.715       |
|    value_loss           | 0.047       |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=694000, episode_reward=1.91 +/- 3.05
Episode length: 268.80 +/- 43.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 269      |
|    mean_reward     | 1.91     |
| time/              |          |
|    total_timesteps | 694000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 339    |
|    time_elapsed    | 1131   |
|    total_timesteps | 694272 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=695000, episode_reward=1.56 +/- 3.13
Episode length: 266.40 +/- 43.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 266         |
|    mean_reward          | 1.56        |
| time/                   |             |
|    total_timesteps      | 695000      |
| train/                  |             |
|    approx_kl            | 0.002655987 |
|    clip_fraction        | 0.00771     |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.17       |
|    explained_variance   | 0.759       |
|    learning_rate        | 3.83e-05    |
|    loss                 | -0.000534   |
|    n_updates            | 3390        |
|    policy_gradient_loss | -0.00179    |
|    std                  | 0.716       |
|    value_loss           | 0.037       |
-----------------------------------------
Eval num_timesteps=696000, episode_reward=-0.52 +/- 0.59
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.519   |
| time/              |          |
|    total_timesteps | 696000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 340    |
|    time_elapsed    | 1135   |
|    total_timesteps | 696320 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=697000, episode_reward=2.17 +/- 2.54
Episode length: 264.40 +/- 45.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 264          |
|    mean_reward          | 2.17         |
| time/                   |              |
|    total_timesteps      | 697000       |
| train/                  |              |
|    approx_kl            | 0.0056921327 |
|    clip_fraction        | 0.043        |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.17        |
|    explained_variance   | 0.943        |
|    learning_rate        | 3.84e-05     |
|    loss                 | 0.0117       |
|    n_updates            | 3400         |
|    policy_gradient_loss | -0.0061      |
|    std                  | 0.716        |
|    value_loss           | 0.00949      |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=698000, episode_reward=1.84 +/- 2.75
Episode length: 249.80 +/- 62.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 250      |
|    mean_reward     | 1.84     |
| time/              |          |
|    total_timesteps | 698000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 341    |
|    time_elapsed    | 1138   |
|    total_timesteps | 698368 |
-------------------------------
box reached target
Eval num_timesteps=699000, episode_reward=0.53 +/- 2.39
Episode length: 276.20 +/- 47.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 276          |
|    mean_reward          | 0.533        |
| time/                   |              |
|    total_timesteps      | 699000       |
| train/                  |              |
|    approx_kl            | 0.0048878356 |
|    clip_fraction        | 0.0354       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.17        |
|    explained_variance   | 0.886        |
|    learning_rate        | 3.84e-05     |
|    loss                 | -0.0036      |
|    n_updates            | 3410         |
|    policy_gradient_loss | -0.0048      |
|    std                  | 0.714        |
|    value_loss           | 0.0608       |
------------------------------------------
box reached target
Eval num_timesteps=700000, episode_reward=0.79 +/- 2.27
Episode length: 279.60 +/- 40.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 280      |
|    mean_reward     | 0.793    |
| time/              |          |
|    total_timesteps | 700000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 342    |
|    time_elapsed    | 1141   |
|    total_timesteps | 700416 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=701000, episode_reward=0.74 +/- 2.45
Episode length: 288.20 +/- 23.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 288          |
|    mean_reward          | 0.743        |
| time/                   |              |
|    total_timesteps      | 701000       |
| train/                  |              |
|    approx_kl            | 0.0036872793 |
|    clip_fraction        | 0.0257       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.16        |
|    explained_variance   | 0.912        |
|    learning_rate        | 3.84e-05     |
|    loss                 | 0.00771      |
|    n_updates            | 3420         |
|    policy_gradient_loss | -0.00267     |
|    std                  | 0.715        |
|    value_loss           | 0.022        |
------------------------------------------
box reached target
Eval num_timesteps=702000, episode_reward=0.54 +/- 2.49
Episode length: 272.60 +/- 54.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 273      |
|    mean_reward     | 0.541    |
| time/              |          |
|    total_timesteps | 702000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 343    |
|    time_elapsed    | 1145   |
|    total_timesteps | 702464 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=703000, episode_reward=0.61 +/- 2.60
Episode length: 280.60 +/- 38.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 281          |
|    mean_reward          | 0.611        |
| time/                   |              |
|    total_timesteps      | 703000       |
| train/                  |              |
|    approx_kl            | 0.0038473143 |
|    clip_fraction        | 0.0155       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.16        |
|    explained_variance   | 0.912        |
|    learning_rate        | 3.84e-05     |
|    loss                 | 0.00296      |
|    n_updates            | 3430         |
|    policy_gradient_loss | -0.00297     |
|    std                  | 0.715        |
|    value_loss           | 0.0364       |
------------------------------------------
Eval num_timesteps=704000, episode_reward=-0.18 +/- 0.71
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.182   |
| time/              |          |
|    total_timesteps | 704000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 344    |
|    time_elapsed    | 1148   |
|    total_timesteps | 704512 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=705000, episode_reward=1.56 +/- 3.14
Episode length: 267.20 +/- 50.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 267          |
|    mean_reward          | 1.56         |
| time/                   |              |
|    total_timesteps      | 705000       |
| train/                  |              |
|    approx_kl            | 0.0035055561 |
|    clip_fraction        | 0.0257       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.17        |
|    explained_variance   | 0.951        |
|    learning_rate        | 3.85e-05     |
|    loss                 | -0.00892     |
|    n_updates            | 3440         |
|    policy_gradient_loss | -0.00401     |
|    std                  | 0.716        |
|    value_loss           | 0.006        |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=706000, episode_reward=1.16 +/- 2.23
Episode length: 295.00 +/- 10.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 295      |
|    mean_reward     | 1.16     |
| time/              |          |
|    total_timesteps | 706000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 345    |
|    time_elapsed    | 1151   |
|    total_timesteps | 706560 |
-------------------------------
Eval num_timesteps=707000, episode_reward=-0.47 +/- 0.66
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.465       |
| time/                   |              |
|    total_timesteps      | 707000       |
| train/                  |              |
|    approx_kl            | 0.0038261143 |
|    clip_fraction        | 0.00957      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.17        |
|    explained_variance   | 0.902        |
|    learning_rate        | 3.85e-05     |
|    loss                 | -0.00353     |
|    n_updates            | 3450         |
|    policy_gradient_loss | -0.00245     |
|    std                  | 0.715        |
|    value_loss           | 0.0564       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=708000, episode_reward=2.79 +/- 3.09
Episode length: 248.60 +/- 47.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 249      |
|    mean_reward     | 2.79     |
| time/              |          |
|    total_timesteps | 708000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 346    |
|    time_elapsed    | 1155   |
|    total_timesteps | 708608 |
-------------------------------
Eval num_timesteps=709000, episode_reward=-0.67 +/- 0.66
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.668      |
| time/                   |             |
|    total_timesteps      | 709000      |
| train/                  |             |
|    approx_kl            | 0.004103558 |
|    clip_fraction        | 0.012       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.16       |
|    explained_variance   | 0.959       |
|    learning_rate        | 3.85e-05    |
|    loss                 | -0.00073    |
|    n_updates            | 3460        |
|    policy_gradient_loss | -0.00313    |
|    std                  | 0.715       |
|    value_loss           | 0.0093      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=710000, episode_reward=2.82 +/- 3.12
Episode length: 244.60 +/- 45.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 245      |
|    mean_reward     | 2.82     |
| time/              |          |
|    total_timesteps | 710000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 347    |
|    time_elapsed    | 1158   |
|    total_timesteps | 710656 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=711000, episode_reward=0.63 +/- 2.40
Episode length: 274.80 +/- 50.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 275          |
|    mean_reward          | 0.63         |
| time/                   |              |
|    total_timesteps      | 711000       |
| train/                  |              |
|    approx_kl            | 0.0030436977 |
|    clip_fraction        | 0.0158       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.17        |
|    explained_variance   | 0.925        |
|    learning_rate        | 3.85e-05     |
|    loss                 | -0.0184      |
|    n_updates            | 3470         |
|    policy_gradient_loss | -0.00468     |
|    std                  | 0.717        |
|    value_loss           | 0.00385      |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=712000, episode_reward=1.51 +/- 3.08
Episode length: 250.60 +/- 60.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | 1.51     |
| time/              |          |
|    total_timesteps | 712000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 348    |
|    time_elapsed    | 1161   |
|    total_timesteps | 712704 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=713000, episode_reward=1.53 +/- 3.10
Episode length: 267.20 +/- 40.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 267          |
|    mean_reward          | 1.53         |
| time/                   |              |
|    total_timesteps      | 713000       |
| train/                  |              |
|    approx_kl            | 0.0027585677 |
|    clip_fraction        | 0.00908      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.17        |
|    explained_variance   | 0.905        |
|    learning_rate        | 3.86e-05     |
|    loss                 | 0.00395      |
|    n_updates            | 3480         |
|    policy_gradient_loss | -0.00331     |
|    std                  | 0.716        |
|    value_loss           | 0.0231       |
------------------------------------------
Eval num_timesteps=714000, episode_reward=-0.42 +/- 0.72
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.417   |
| time/              |          |
|    total_timesteps | 714000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 349    |
|    time_elapsed    | 1164   |
|    total_timesteps | 714752 |
-------------------------------
box reached target
Eval num_timesteps=715000, episode_reward=0.65 +/- 2.57
Episode length: 290.40 +/- 19.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 290         |
|    mean_reward          | 0.654       |
| time/                   |             |
|    total_timesteps      | 715000      |
| train/                  |             |
|    approx_kl            | 0.004727387 |
|    clip_fraction        | 0.0337      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.16       |
|    explained_variance   | 0.848       |
|    learning_rate        | 3.86e-05    |
|    loss                 | 0.00782     |
|    n_updates            | 3490        |
|    policy_gradient_loss | -0.00413    |
|    std                  | 0.714       |
|    value_loss           | 0.0362      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=716000, episode_reward=-0.10 +/- 0.75
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.0993  |
| time/              |          |
|    total_timesteps | 716000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 350    |
|    time_elapsed    | 1168   |
|    total_timesteps | 716800 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=717000, episode_reward=1.85 +/- 2.87
Episode length: 253.20 +/- 57.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 253          |
|    mean_reward          | 1.85         |
| time/                   |              |
|    total_timesteps      | 717000       |
| train/                  |              |
|    approx_kl            | 0.0031486615 |
|    clip_fraction        | 0.0154       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.16        |
|    explained_variance   | 0.873        |
|    learning_rate        | 3.86e-05     |
|    loss                 | -7.86e-06    |
|    n_updates            | 3500         |
|    policy_gradient_loss | -0.00168     |
|    std                  | 0.714        |
|    value_loss           | 0.0394       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=718000, episode_reward=5.25 +/- 0.08
Episode length: 179.80 +/- 22.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 180      |
|    mean_reward     | 5.25     |
| time/              |          |
|    total_timesteps | 718000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 351    |
|    time_elapsed    | 1171   |
|    total_timesteps | 718848 |
-------------------------------
box reached target
Eval num_timesteps=719000, episode_reward=0.88 +/- 2.37
Episode length: 280.60 +/- 38.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 281          |
|    mean_reward          | 0.879        |
| time/                   |              |
|    total_timesteps      | 719000       |
| train/                  |              |
|    approx_kl            | 0.0031923533 |
|    clip_fraction        | 0.00869      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.16        |
|    explained_variance   | 0.906        |
|    learning_rate        | 3.86e-05     |
|    loss                 | 0.00539      |
|    n_updates            | 3510         |
|    policy_gradient_loss | -0.00241     |
|    std                  | 0.713        |
|    value_loss           | 0.0202       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=720000, episode_reward=2.14 +/- 2.64
Episode length: 248.60 +/- 62.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 249      |
|    mean_reward     | 2.14     |
| time/              |          |
|    total_timesteps | 720000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 352    |
|    time_elapsed    | 1174   |
|    total_timesteps | 720896 |
-------------------------------
box reached target
Eval num_timesteps=721000, episode_reward=0.50 +/- 2.41
Episode length: 270.80 +/- 58.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 271          |
|    mean_reward          | 0.499        |
| time/                   |              |
|    total_timesteps      | 721000       |
| train/                  |              |
|    approx_kl            | 0.0031585055 |
|    clip_fraction        | 0.0271       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.16        |
|    explained_variance   | 0.885        |
|    learning_rate        | 3.87e-05     |
|    loss                 | 0.00602      |
|    n_updates            | 3520         |
|    policy_gradient_loss | -0.0034      |
|    std                  | 0.713        |
|    value_loss           | 0.0429       |
------------------------------------------
Eval num_timesteps=722000, episode_reward=-0.87 +/- 0.18
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.87    |
| time/              |          |
|    total_timesteps | 722000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 353    |
|    time_elapsed    | 1178   |
|    total_timesteps | 722944 |
-------------------------------
box reached target
Eval num_timesteps=723000, episode_reward=0.75 +/- 2.32
Episode length: 273.80 +/- 52.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 274          |
|    mean_reward          | 0.751        |
| time/                   |              |
|    total_timesteps      | 723000       |
| train/                  |              |
|    approx_kl            | 0.0030524856 |
|    clip_fraction        | 0.0104       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.16        |
|    explained_variance   | 0.866        |
|    learning_rate        | 3.87e-05     |
|    loss                 | 0.0512       |
|    n_updates            | 3530         |
|    policy_gradient_loss | -0.00259     |
|    std                  | 0.713        |
|    value_loss           | 0.0368       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=724000, episode_reward=3.17 +/- 2.85
Episode length: 247.20 +/- 54.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 247      |
|    mean_reward     | 3.17     |
| time/              |          |
|    total_timesteps | 724000   |
---------------------------------
box reached target
box reached target
box reached target
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 354    |
|    time_elapsed    | 1181   |
|    total_timesteps | 724992 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=725000, episode_reward=3.13 +/- 2.91
Episode length: 256.00 +/- 41.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 256         |
|    mean_reward          | 3.13        |
| time/                   |             |
|    total_timesteps      | 725000      |
| train/                  |             |
|    approx_kl            | 0.001879733 |
|    clip_fraction        | 0.0064      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.16       |
|    explained_variance   | 0.907       |
|    learning_rate        | 3.87e-05    |
|    loss                 | 0.0585      |
|    n_updates            | 3540        |
|    policy_gradient_loss | -0.00189    |
|    std                  | 0.713       |
|    value_loss           | 0.0793      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=726000, episode_reward=4.25 +/- 2.04
Episode length: 204.60 +/- 51.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 205      |
|    mean_reward     | 4.25     |
| time/              |          |
|    total_timesteps | 726000   |
---------------------------------
box reached target
box reached target
Eval num_timesteps=727000, episode_reward=1.14 +/- 2.05
Episode length: 279.00 +/- 42.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 279      |
|    mean_reward     | 1.14     |
| time/              |          |
|    total_timesteps | 727000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 355    |
|    time_elapsed    | 1185   |
|    total_timesteps | 727040 |
-------------------------------
box reached target
Eval num_timesteps=728000, episode_reward=0.76 +/- 2.31
Episode length: 285.20 +/- 29.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 285          |
|    mean_reward          | 0.761        |
| time/                   |              |
|    total_timesteps      | 728000       |
| train/                  |              |
|    approx_kl            | 0.0025672715 |
|    clip_fraction        | 0.0122       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.16        |
|    explained_variance   | 0.792        |
|    learning_rate        | 3.87e-05     |
|    loss                 | -0.0221      |
|    n_updates            | 3550         |
|    policy_gradient_loss | -0.00302     |
|    std                  | 0.714        |
|    value_loss           | 0.0468       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=729000, episode_reward=1.88 +/- 2.91
Episode length: 250.00 +/- 61.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 250      |
|    mean_reward     | 1.88     |
| time/              |          |
|    total_timesteps | 729000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 356    |
|    time_elapsed    | 1188   |
|    total_timesteps | 729088 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=730000, episode_reward=1.90 +/- 2.69
Episode length: 268.40 +/- 45.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 268         |
|    mean_reward          | 1.9         |
| time/                   |             |
|    total_timesteps      | 730000      |
| train/                  |             |
|    approx_kl            | 0.006173926 |
|    clip_fraction        | 0.0586      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.16       |
|    explained_variance   | 0.725       |
|    learning_rate        | 3.87e-05    |
|    loss                 | -0.0143     |
|    n_updates            | 3560        |
|    policy_gradient_loss | -0.00602    |
|    std                  | 0.714       |
|    value_loss           | 0.00713     |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=731000, episode_reward=-0.52 +/- 0.61
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.521   |
| time/              |          |
|    total_timesteps | 731000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 357    |
|    time_elapsed    | 1191   |
|    total_timesteps | 731136 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=732000, episode_reward=2.20 +/- 2.38
Episode length: 248.40 +/- 63.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 248          |
|    mean_reward          | 2.2          |
| time/                   |              |
|    total_timesteps      | 732000       |
| train/                  |              |
|    approx_kl            | 0.0042325417 |
|    clip_fraction        | 0.0258       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.16        |
|    explained_variance   | 0.968        |
|    learning_rate        | 3.88e-05     |
|    loss                 | -0.00934     |
|    n_updates            | 3570         |
|    policy_gradient_loss | -0.00522     |
|    std                  | 0.712        |
|    value_loss           | 0.0132       |
------------------------------------------
Eval num_timesteps=733000, episode_reward=-0.79 +/- 0.43
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.786   |
| time/              |          |
|    total_timesteps | 733000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 358    |
|    time_elapsed    | 1194   |
|    total_timesteps | 733184 |
-------------------------------
box reached target
Eval num_timesteps=734000, episode_reward=0.26 +/- 2.52
Episode length: 275.00 +/- 50.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 275          |
|    mean_reward          | 0.258        |
| time/                   |              |
|    total_timesteps      | 734000       |
| train/                  |              |
|    approx_kl            | 0.0039048821 |
|    clip_fraction        | 0.0249       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.16        |
|    explained_variance   | 0.952        |
|    learning_rate        | 3.88e-05     |
|    loss                 | -0.0103      |
|    n_updates            | 3580         |
|    policy_gradient_loss | -0.00258     |
|    std                  | 0.714        |
|    value_loss           | 0.0101       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=735000, episode_reward=0.73 +/- 2.29
Episode length: 274.80 +/- 50.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 275      |
|    mean_reward     | 0.726    |
| time/              |          |
|    total_timesteps | 735000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 359    |
|    time_elapsed    | 1198   |
|    total_timesteps | 735232 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=736000, episode_reward=1.93 +/- 2.86
Episode length: 265.60 +/- 56.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 266         |
|    mean_reward          | 1.93        |
| time/                   |             |
|    total_timesteps      | 736000      |
| train/                  |             |
|    approx_kl            | 0.004271652 |
|    clip_fraction        | 0.0133      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.16       |
|    explained_variance   | 0.837       |
|    learning_rate        | 3.88e-05    |
|    loss                 | 0.011       |
|    n_updates            | 3590        |
|    policy_gradient_loss | -0.00308    |
|    std                  | 0.711       |
|    value_loss           | 0.0417      |
-----------------------------------------
box reached target
Eval num_timesteps=737000, episode_reward=-0.87 +/- 0.26
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.869   |
| time/              |          |
|    total_timesteps | 737000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 360    |
|    time_elapsed    | 1201   |
|    total_timesteps | 737280 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=738000, episode_reward=2.14 +/- 2.67
Episode length: 250.40 +/- 60.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 250          |
|    mean_reward          | 2.14         |
| time/                   |              |
|    total_timesteps      | 738000       |
| train/                  |              |
|    approx_kl            | 0.0037479084 |
|    clip_fraction        | 0.0203       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.15        |
|    explained_variance   | 0.938        |
|    learning_rate        | 3.88e-05     |
|    loss                 | 0.00023      |
|    n_updates            | 3600         |
|    policy_gradient_loss | -0.00475     |
|    std                  | 0.711        |
|    value_loss           | 0.00863      |
------------------------------------------
box reached target
Eval num_timesteps=739000, episode_reward=1.16 +/- 2.16
Episode length: 280.40 +/- 39.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 280      |
|    mean_reward     | 1.16     |
| time/              |          |
|    total_timesteps | 739000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 361    |
|    time_elapsed    | 1204   |
|    total_timesteps | 739328 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=740000, episode_reward=1.75 +/- 2.82
Episode length: 245.40 +/- 68.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 245          |
|    mean_reward          | 1.75         |
| time/                   |              |
|    total_timesteps      | 740000       |
| train/                  |              |
|    approx_kl            | 0.0029215072 |
|    clip_fraction        | 0.0111       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.15        |
|    explained_variance   | 0.908        |
|    learning_rate        | 3.89e-05     |
|    loss                 | 6.17e-05     |
|    n_updates            | 3610         |
|    policy_gradient_loss | -0.00267     |
|    std                  | 0.709        |
|    value_loss           | 0.00681      |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=741000, episode_reward=0.62 +/- 2.32
Episode length: 269.60 +/- 60.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 270      |
|    mean_reward     | 0.618    |
| time/              |          |
|    total_timesteps | 741000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 362    |
|    time_elapsed    | 1208   |
|    total_timesteps | 741376 |
-------------------------------
Eval num_timesteps=742000, episode_reward=-0.19 +/- 0.70
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.189       |
| time/                   |              |
|    total_timesteps      | 742000       |
| train/                  |              |
|    approx_kl            | 0.0028795684 |
|    clip_fraction        | 0.0101       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.15        |
|    explained_variance   | 0.935        |
|    learning_rate        | 3.89e-05     |
|    loss                 | -0.00929     |
|    n_updates            | 3620         |
|    policy_gradient_loss | -0.00227     |
|    std                  | 0.708        |
|    value_loss           | 0.0322       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=743000, episode_reward=0.36 +/- 2.43
Episode length: 270.20 +/- 59.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 270      |
|    mean_reward     | 0.361    |
| time/              |          |
|    total_timesteps | 743000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 363    |
|    time_elapsed    | 1211   |
|    total_timesteps | 743424 |
-------------------------------
box reached target
Eval num_timesteps=744000, episode_reward=1.04 +/- 2.22
Episode length: 283.40 +/- 33.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 283         |
|    mean_reward          | 1.04        |
| time/                   |             |
|    total_timesteps      | 744000      |
| train/                  |             |
|    approx_kl            | 0.003472372 |
|    clip_fraction        | 0.00698     |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.14       |
|    explained_variance   | 0.982       |
|    learning_rate        | 3.89e-05    |
|    loss                 | 0.00857     |
|    n_updates            | 3630        |
|    policy_gradient_loss | -0.00186    |
|    std                  | 0.707       |
|    value_loss           | 0.0114      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=745000, episode_reward=0.23 +/- 2.45
Episode length: 271.40 +/- 57.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 271      |
|    mean_reward     | 0.225    |
| time/              |          |
|    total_timesteps | 745000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 364    |
|    time_elapsed    | 1214   |
|    total_timesteps | 745472 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=746000, episode_reward=2.14 +/- 2.64
Episode length: 244.20 +/- 68.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 244         |
|    mean_reward          | 2.14        |
| time/                   |             |
|    total_timesteps      | 746000      |
| train/                  |             |
|    approx_kl            | 0.002998014 |
|    clip_fraction        | 0.0199      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.14       |
|    explained_variance   | 0.904       |
|    learning_rate        | 3.89e-05    |
|    loss                 | -0.0103     |
|    n_updates            | 3640        |
|    policy_gradient_loss | -0.00405    |
|    std                  | 0.708       |
|    value_loss           | 0.0242      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=747000, episode_reward=0.92 +/- 2.27
Episode length: 274.20 +/- 51.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 274      |
|    mean_reward     | 0.924    |
| time/              |          |
|    total_timesteps | 747000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 365    |
|    time_elapsed    | 1218   |
|    total_timesteps | 747520 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=748000, episode_reward=3.05 +/- 2.79
Episode length: 231.60 +/- 61.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 232         |
|    mean_reward          | 3.05        |
| time/                   |             |
|    total_timesteps      | 748000      |
| train/                  |             |
|    approx_kl            | 0.004482713 |
|    clip_fraction        | 0.0189      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.14       |
|    explained_variance   | 0.651       |
|    learning_rate        | 3.9e-05     |
|    loss                 | 0.0294      |
|    n_updates            | 3650        |
|    policy_gradient_loss | -0.00321    |
|    std                  | 0.707       |
|    value_loss           | 0.078       |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=749000, episode_reward=2.85 +/- 3.14
Episode length: 229.60 +/- 59.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 230      |
|    mean_reward     | 2.85     |
| time/              |          |
|    total_timesteps | 749000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 366    |
|    time_elapsed    | 1221   |
|    total_timesteps | 749568 |
-------------------------------
box reached target
Eval num_timesteps=750000, episode_reward=0.81 +/- 2.32
Episode length: 276.40 +/- 47.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 276          |
|    mean_reward          | 0.808        |
| time/                   |              |
|    total_timesteps      | 750000       |
| train/                  |              |
|    approx_kl            | 0.0033653404 |
|    clip_fraction        | 0.0167       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.14        |
|    explained_variance   | 0.758        |
|    learning_rate        | 3.9e-05      |
|    loss                 | 0.0114       |
|    n_updates            | 3660         |
|    policy_gradient_loss | -0.00404     |
|    std                  | 0.705        |
|    value_loss           | 0.0785       |
------------------------------------------
box reached target
Eval num_timesteps=751000, episode_reward=0.22 +/- 2.44
Episode length: 279.60 +/- 40.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 280      |
|    mean_reward     | 0.218    |
| time/              |          |
|    total_timesteps | 751000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 367    |
|    time_elapsed    | 1224   |
|    total_timesteps | 751616 |
-------------------------------
box reached target
Eval num_timesteps=752000, episode_reward=0.55 +/- 2.37
Episode length: 277.60 +/- 44.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 278          |
|    mean_reward          | 0.554        |
| time/                   |              |
|    total_timesteps      | 752000       |
| train/                  |              |
|    approx_kl            | 0.0032182178 |
|    clip_fraction        | 0.00767      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.14        |
|    explained_variance   | 0.94         |
|    learning_rate        | 3.9e-05      |
|    loss                 | -0.00963     |
|    n_updates            | 3670         |
|    policy_gradient_loss | -0.00193     |
|    std                  | 0.706        |
|    value_loss           | 0.0128       |
------------------------------------------
box reached target
Eval num_timesteps=753000, episode_reward=0.96 +/- 2.43
Episode length: 296.80 +/- 6.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | 0.96     |
| time/              |          |
|    total_timesteps | 753000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 368    |
|    time_elapsed    | 1227   |
|    total_timesteps | 753664 |
-------------------------------
box reached target
Eval num_timesteps=754000, episode_reward=-0.45 +/- 0.51
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.448      |
| time/                   |             |
|    total_timesteps      | 754000      |
| train/                  |             |
|    approx_kl            | 0.004207475 |
|    clip_fraction        | 0.0209      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.14       |
|    explained_variance   | 0.954       |
|    learning_rate        | 3.9e-05     |
|    loss                 | 0.018       |
|    n_updates            | 3680        |
|    policy_gradient_loss | -0.00458    |
|    std                  | 0.705       |
|    value_loss           | 0.00944     |
-----------------------------------------
box reached target
Eval num_timesteps=755000, episode_reward=0.55 +/- 2.40
Episode length: 275.40 +/- 49.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 275      |
|    mean_reward     | 0.552    |
| time/              |          |
|    total_timesteps | 755000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 369    |
|    time_elapsed    | 1231   |
|    total_timesteps | 755712 |
-------------------------------
Eval num_timesteps=756000, episode_reward=0.17 +/- 0.60
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | 0.165        |
| time/                   |              |
|    total_timesteps      | 756000       |
| train/                  |              |
|    approx_kl            | 0.0030657302 |
|    clip_fraction        | 0.0156       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.14        |
|    explained_variance   | 0.982        |
|    learning_rate        | 3.91e-05     |
|    loss                 | -0.0134      |
|    n_updates            | 3690         |
|    policy_gradient_loss | -0.00249     |
|    std                  | 0.704        |
|    value_loss           | 0.00694      |
------------------------------------------
box reached target
Eval num_timesteps=757000, episode_reward=-0.21 +/- 0.67
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.215   |
| time/              |          |
|    total_timesteps | 757000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 370    |
|    time_elapsed    | 1234   |
|    total_timesteps | 757760 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=758000, episode_reward=3.16 +/- 2.83
Episode length: 246.80 +/- 45.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 247         |
|    mean_reward          | 3.16        |
| time/                   |             |
|    total_timesteps      | 758000      |
| train/                  |             |
|    approx_kl            | 0.004007336 |
|    clip_fraction        | 0.021       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.13       |
|    explained_variance   | 0.811       |
|    learning_rate        | 3.91e-05    |
|    loss                 | 0.0163      |
|    n_updates            | 3700        |
|    policy_gradient_loss | -0.00226    |
|    std                  | 0.704       |
|    value_loss           | 0.0452      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=759000, episode_reward=1.97 +/- 3.00
Episode length: 259.40 +/- 54.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 259      |
|    mean_reward     | 1.97     |
| time/              |          |
|    total_timesteps | 759000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 371    |
|    time_elapsed    | 1237   |
|    total_timesteps | 759808 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=760000, episode_reward=1.75 +/- 2.97
Episode length: 265.80 +/- 41.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 266          |
|    mean_reward          | 1.75         |
| time/                   |              |
|    total_timesteps      | 760000       |
| train/                  |              |
|    approx_kl            | 0.0052525005 |
|    clip_fraction        | 0.0269       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.13        |
|    explained_variance   | 0.927        |
|    learning_rate        | 3.91e-05     |
|    loss                 | -0.0193      |
|    n_updates            | 3710         |
|    policy_gradient_loss | -0.00503     |
|    std                  | 0.703        |
|    value_loss           | 0.00368      |
------------------------------------------
box reached target
Eval num_timesteps=761000, episode_reward=0.43 +/- 2.47
Episode length: 282.80 +/- 34.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 283      |
|    mean_reward     | 0.433    |
| time/              |          |
|    total_timesteps | 761000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 372    |
|    time_elapsed    | 1241   |
|    total_timesteps | 761856 |
-------------------------------
box reached target
Eval num_timesteps=762000, episode_reward=0.76 +/- 2.48
Episode length: 282.20 +/- 35.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 282         |
|    mean_reward          | 0.763       |
| time/                   |             |
|    total_timesteps      | 762000      |
| train/                  |             |
|    approx_kl            | 0.006777767 |
|    clip_fraction        | 0.0659      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.13       |
|    explained_variance   | 0.932       |
|    learning_rate        | 3.91e-05    |
|    loss                 | -0.00331    |
|    n_updates            | 3720        |
|    policy_gradient_loss | -0.0079     |
|    std                  | 0.703       |
|    value_loss           | 0.0043      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=763000, episode_reward=1.84 +/- 2.95
Episode length: 261.20 +/- 48.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 261      |
|    mean_reward     | 1.84     |
| time/              |          |
|    total_timesteps | 763000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 373    |
|    time_elapsed    | 1244   |
|    total_timesteps | 763904 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=764000, episode_reward=1.54 +/- 3.11
Episode length: 256.40 +/- 53.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 256         |
|    mean_reward          | 1.54        |
| time/                   |             |
|    total_timesteps      | 764000      |
| train/                  |             |
|    approx_kl            | 0.006553119 |
|    clip_fraction        | 0.0452      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.13       |
|    explained_variance   | 0.866       |
|    learning_rate        | 3.92e-05    |
|    loss                 | -0.0295     |
|    n_updates            | 3730        |
|    policy_gradient_loss | -0.00476    |
|    std                  | 0.704       |
|    value_loss           | 0.00439     |
-----------------------------------------
box reached target
Eval num_timesteps=765000, episode_reward=-0.69 +/- 0.44
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.69    |
| time/              |          |
|    total_timesteps | 765000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 374    |
|    time_elapsed    | 1247   |
|    total_timesteps | 765952 |
-------------------------------
box reached target
Eval num_timesteps=766000, episode_reward=0.55 +/- 2.38
Episode length: 272.20 +/- 55.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 272          |
|    mean_reward          | 0.552        |
| time/                   |              |
|    total_timesteps      | 766000       |
| train/                  |              |
|    approx_kl            | 0.0044153007 |
|    clip_fraction        | 0.0143       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.13        |
|    explained_variance   | 0.975        |
|    learning_rate        | 3.92e-05     |
|    loss                 | -0.00747     |
|    n_updates            | 3740         |
|    policy_gradient_loss | -0.00303     |
|    std                  | 0.703        |
|    value_loss           | 0.0127       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=767000, episode_reward=1.81 +/- 2.85
Episode length: 252.00 +/- 58.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 252      |
|    mean_reward     | 1.81     |
| time/              |          |
|    total_timesteps | 767000   |
---------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=768000, episode_reward=1.62 +/- 3.22
Episode length: 263.40 +/- 51.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 263      |
|    mean_reward     | 1.62     |
| time/              |          |
|    total_timesteps | 768000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 375    |
|    time_elapsed    | 1251   |
|    total_timesteps | 768000 |
-------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=769000, episode_reward=0.35 +/- 2.52
Episode length: 281.20 +/- 37.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 281          |
|    mean_reward          | 0.354        |
| time/                   |              |
|    total_timesteps      | 769000       |
| train/                  |              |
|    approx_kl            | 0.0038176444 |
|    clip_fraction        | 0.0179       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.13        |
|    explained_variance   | 0.637        |
|    learning_rate        | 3.92e-05     |
|    loss                 | -0.0113      |
|    n_updates            | 3750         |
|    policy_gradient_loss | -0.003       |
|    std                  | 0.701        |
|    value_loss           | 0.0916       |
------------------------------------------
box reached target
Eval num_timesteps=770000, episode_reward=0.61 +/- 2.54
Episode length: 291.80 +/- 16.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 292      |
|    mean_reward     | 0.606    |
| time/              |          |
|    total_timesteps | 770000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 376    |
|    time_elapsed    | 1255   |
|    total_timesteps | 770048 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=771000, episode_reward=-0.74 +/- 0.52
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.738       |
| time/                   |              |
|    total_timesteps      | 771000       |
| train/                  |              |
|    approx_kl            | 0.0038460973 |
|    clip_fraction        | 0.0171       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.12        |
|    explained_variance   | 0.916        |
|    learning_rate        | 3.92e-05     |
|    loss                 | 0.0342       |
|    n_updates            | 3760         |
|    policy_gradient_loss | -0.00726     |
|    std                  | 0.699        |
|    value_loss           | 0.0449       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=772000, episode_reward=1.84 +/- 3.01
Episode length: 279.80 +/- 26.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 280      |
|    mean_reward     | 1.84     |
| time/              |          |
|    total_timesteps | 772000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 377    |
|    time_elapsed    | 1258   |
|    total_timesteps | 772096 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=773000, episode_reward=2.15 +/- 2.72
Episode length: 266.80 +/- 44.10
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 267        |
|    mean_reward          | 2.15       |
| time/                   |            |
|    total_timesteps      | 773000     |
| train/                  |            |
|    approx_kl            | 0.00540879 |
|    clip_fraction        | 0.0354     |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.12      |
|    explained_variance   | 0.909      |
|    learning_rate        | 3.93e-05   |
|    loss                 | -0.00854   |
|    n_updates            | 3770       |
|    policy_gradient_loss | -0.00432   |
|    std                  | 0.698      |
|    value_loss           | 0.0654     |
----------------------------------------
box reached target
Eval num_timesteps=774000, episode_reward=0.57 +/- 2.46
Episode length: 280.20 +/- 39.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 280      |
|    mean_reward     | 0.567    |
| time/              |          |
|    total_timesteps | 774000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 378    |
|    time_elapsed    | 1261   |
|    total_timesteps | 774144 |
-------------------------------
Eval num_timesteps=775000, episode_reward=-0.78 +/- 0.44
Episode length: 300.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 300        |
|    mean_reward          | -0.781     |
| time/                   |            |
|    total_timesteps      | 775000     |
| train/                  |            |
|    approx_kl            | 0.00495942 |
|    clip_fraction        | 0.0379     |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.12      |
|    explained_variance   | 0.907      |
|    learning_rate        | 3.93e-05   |
|    loss                 | 0.00663    |
|    n_updates            | 3780       |
|    policy_gradient_loss | -0.00387   |
|    std                  | 0.698      |
|    value_loss           | 0.00587    |
----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=776000, episode_reward=0.97 +/- 2.24
Episode length: 280.80 +/- 38.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 281      |
|    mean_reward     | 0.971    |
| time/              |          |
|    total_timesteps | 776000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 379    |
|    time_elapsed    | 1265   |
|    total_timesteps | 776192 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=777000, episode_reward=0.74 +/- 2.25
Episode length: 270.60 +/- 58.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 271          |
|    mean_reward          | 0.742        |
| time/                   |              |
|    total_timesteps      | 777000       |
| train/                  |              |
|    approx_kl            | 0.0037321714 |
|    clip_fraction        | 0.0165       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.12        |
|    explained_variance   | 0.945        |
|    learning_rate        | 3.93e-05     |
|    loss                 | -0.00308     |
|    n_updates            | 3790         |
|    policy_gradient_loss | -0.0042      |
|    std                  | 0.698        |
|    value_loss           | 0.0467       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=778000, episode_reward=1.66 +/- 3.04
Episode length: 240.40 +/- 73.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 240      |
|    mean_reward     | 1.66     |
| time/              |          |
|    total_timesteps | 778000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 380    |
|    time_elapsed    | 1268   |
|    total_timesteps | 778240 |
-------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=779000, episode_reward=4.11 +/- 2.56
Episode length: 207.40 +/- 49.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 207          |
|    mean_reward          | 4.11         |
| time/                   |              |
|    total_timesteps      | 779000       |
| train/                  |              |
|    approx_kl            | 0.0032941476 |
|    clip_fraction        | 0.0143       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.11        |
|    explained_variance   | 0.936        |
|    learning_rate        | 3.93e-05     |
|    loss                 | 0.00781      |
|    n_updates            | 3800         |
|    policy_gradient_loss | -0.00297     |
|    std                  | 0.697        |
|    value_loss           | 0.053        |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=780000, episode_reward=0.50 +/- 2.42
Episode length: 279.80 +/- 40.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 280      |
|    mean_reward     | 0.495    |
| time/              |          |
|    total_timesteps | 780000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 381    |
|    time_elapsed    | 1271   |
|    total_timesteps | 780288 |
-------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=781000, episode_reward=2.01 +/- 2.73
Episode length: 266.40 +/- 41.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 266          |
|    mean_reward          | 2.01         |
| time/                   |              |
|    total_timesteps      | 781000       |
| train/                  |              |
|    approx_kl            | 0.0037437468 |
|    clip_fraction        | 0.0159       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.11        |
|    explained_variance   | 0.923        |
|    learning_rate        | 3.94e-05     |
|    loss                 | -0.00589     |
|    n_updates            | 3810         |
|    policy_gradient_loss | -0.00354     |
|    std                  | 0.693        |
|    value_loss           | 0.0591       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=782000, episode_reward=0.74 +/- 2.35
Episode length: 285.00 +/- 30.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 285      |
|    mean_reward     | 0.738    |
| time/              |          |
|    total_timesteps | 782000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 382    |
|    time_elapsed    | 1274   |
|    total_timesteps | 782336 |
-------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=783000, episode_reward=3.15 +/- 2.74
Episode length: 232.60 +/- 55.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 233          |
|    mean_reward          | 3.15         |
| time/                   |              |
|    total_timesteps      | 783000       |
| train/                  |              |
|    approx_kl            | 0.0044698245 |
|    clip_fraction        | 0.0254       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.1         |
|    explained_variance   | 0.829        |
|    learning_rate        | 3.94e-05     |
|    loss                 | 0.00869      |
|    n_updates            | 3820         |
|    policy_gradient_loss | -0.00537     |
|    std                  | 0.694        |
|    value_loss           | 0.0593       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=784000, episode_reward=0.71 +/- 2.32
Episode length: 277.40 +/- 45.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 277      |
|    mean_reward     | 0.707    |
| time/              |          |
|    total_timesteps | 784000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 383    |
|    time_elapsed    | 1278   |
|    total_timesteps | 784384 |
-------------------------------
box reached target
Eval num_timesteps=785000, episode_reward=0.54 +/- 2.55
Episode length: 285.20 +/- 29.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 285          |
|    mean_reward          | 0.544        |
| time/                   |              |
|    total_timesteps      | 785000       |
| train/                  |              |
|    approx_kl            | 0.0033377612 |
|    clip_fraction        | 0.0142       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.1         |
|    explained_variance   | 0.876        |
|    learning_rate        | 3.94e-05     |
|    loss                 | 0.0197       |
|    n_updates            | 3830         |
|    policy_gradient_loss | -0.00255     |
|    std                  | 0.693        |
|    value_loss           | 0.0835       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=786000, episode_reward=2.77 +/- 2.96
Episode length: 227.60 +/- 59.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 228      |
|    mean_reward     | 2.77     |
| time/              |          |
|    total_timesteps | 786000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 384    |
|    time_elapsed    | 1281   |
|    total_timesteps | 786432 |
-------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=787000, episode_reward=1.83 +/- 2.92
Episode length: 248.80 +/- 63.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 249          |
|    mean_reward          | 1.83         |
| time/                   |              |
|    total_timesteps      | 787000       |
| train/                  |              |
|    approx_kl            | 0.0047363513 |
|    clip_fraction        | 0.0249       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.1         |
|    explained_variance   | 0.921        |
|    learning_rate        | 3.94e-05     |
|    loss                 | 0.0126       |
|    n_updates            | 3840         |
|    policy_gradient_loss | -0.00422     |
|    std                  | 0.695        |
|    value_loss           | 0.0295       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=788000, episode_reward=3.98 +/- 2.49
Episode length: 203.80 +/- 52.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 204      |
|    mean_reward     | 3.98     |
| time/              |          |
|    total_timesteps | 788000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 385    |
|    time_elapsed    | 1284   |
|    total_timesteps | 788480 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=789000, episode_reward=0.56 +/- 2.48
Episode length: 274.60 +/- 50.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 275         |
|    mean_reward          | 0.556       |
| time/                   |             |
|    total_timesteps      | 789000      |
| train/                  |             |
|    approx_kl            | 0.004248294 |
|    clip_fraction        | 0.025       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.11       |
|    explained_variance   | 0.974       |
|    learning_rate        | 3.95e-05    |
|    loss                 | -0.0156     |
|    n_updates            | 3850        |
|    policy_gradient_loss | -0.00663    |
|    std                  | 0.694       |
|    value_loss           | 0.0107      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=790000, episode_reward=2.94 +/- 2.70
Episode length: 224.00 +/- 63.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 224      |
|    mean_reward     | 2.94     |
| time/              |          |
|    total_timesteps | 790000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 386    |
|    time_elapsed    | 1287   |
|    total_timesteps | 790528 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=791000, episode_reward=0.32 +/- 2.44
Episode length: 275.80 +/- 48.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 276          |
|    mean_reward          | 0.32         |
| time/                   |              |
|    total_timesteps      | 791000       |
| train/                  |              |
|    approx_kl            | 0.0016746842 |
|    clip_fraction        | 0.0102       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.11        |
|    explained_variance   | 0.874        |
|    learning_rate        | 3.95e-05     |
|    loss                 | 0.00218      |
|    n_updates            | 3860         |
|    policy_gradient_loss | -0.00145     |
|    std                  | 0.694        |
|    value_loss           | 0.094        |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=792000, episode_reward=1.62 +/- 2.94
Episode length: 246.20 +/- 65.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 246      |
|    mean_reward     | 1.62     |
| time/              |          |
|    total_timesteps | 792000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 387    |
|    time_elapsed    | 1290   |
|    total_timesteps | 792576 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=793000, episode_reward=1.62 +/- 3.21
Episode length: 265.40 +/- 43.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 265         |
|    mean_reward          | 1.62        |
| time/                   |             |
|    total_timesteps      | 793000      |
| train/                  |             |
|    approx_kl            | 0.006791659 |
|    clip_fraction        | 0.0512      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.11       |
|    explained_variance   | 0.914       |
|    learning_rate        | 3.95e-05    |
|    loss                 | -0.0144     |
|    n_updates            | 3870        |
|    policy_gradient_loss | -0.00705    |
|    std                  | 0.696       |
|    value_loss           | 0.037       |
-----------------------------------------
Eval num_timesteps=794000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 794000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 388    |
|    time_elapsed    | 1294   |
|    total_timesteps | 794624 |
-------------------------------
box reached target
Eval num_timesteps=795000, episode_reward=0.49 +/- 2.34
Episode length: 279.80 +/- 40.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 280          |
|    mean_reward          | 0.49         |
| time/                   |              |
|    total_timesteps      | 795000       |
| train/                  |              |
|    approx_kl            | 0.0038576734 |
|    clip_fraction        | 0.0104       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.11        |
|    explained_variance   | 0.886        |
|    learning_rate        | 3.95e-05     |
|    loss                 | 0.0189       |
|    n_updates            | 3880         |
|    policy_gradient_loss | -0.00171     |
|    std                  | 0.696        |
|    value_loss           | 0.00983      |
------------------------------------------
box reached target
Eval num_timesteps=796000, episode_reward=-0.45 +/- 0.67
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.454   |
| time/              |          |
|    total_timesteps | 796000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 389    |
|    time_elapsed    | 1297   |
|    total_timesteps | 796672 |
-------------------------------
Eval num_timesteps=797000, episode_reward=-0.44 +/- 0.54
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.439      |
| time/                   |             |
|    total_timesteps      | 797000      |
| train/                  |             |
|    approx_kl            | 0.004618971 |
|    clip_fraction        | 0.0392      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.11       |
|    explained_variance   | 0.929       |
|    learning_rate        | 3.96e-05    |
|    loss                 | 0.0128      |
|    n_updates            | 3890        |
|    policy_gradient_loss | -0.00711    |
|    std                  | 0.696       |
|    value_loss           | 0.0345      |
-----------------------------------------
box reached target
Eval num_timesteps=798000, episode_reward=0.22 +/- 2.43
Episode length: 275.60 +/- 48.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 276      |
|    mean_reward     | 0.217    |
| time/              |          |
|    total_timesteps | 798000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 390    |
|    time_elapsed    | 1300   |
|    total_timesteps | 798720 |
-------------------------------
box reached target
Eval num_timesteps=799000, episode_reward=0.49 +/- 2.40
Episode length: 271.40 +/- 57.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 271         |
|    mean_reward          | 0.49        |
| time/                   |             |
|    total_timesteps      | 799000      |
| train/                  |             |
|    approx_kl            | 0.003230746 |
|    clip_fraction        | 0.0126      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.11       |
|    explained_variance   | 0.961       |
|    learning_rate        | 3.96e-05    |
|    loss                 | -0.00383    |
|    n_updates            | 3900        |
|    policy_gradient_loss | -0.00343    |
|    std                  | 0.696       |
|    value_loss           | 0.0169      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=800000, episode_reward=0.36 +/- 2.72
Episode length: 298.80 +/- 2.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 299      |
|    mean_reward     | 0.359    |
| time/              |          |
|    total_timesteps | 800000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 391    |
|    time_elapsed    | 1304   |
|    total_timesteps | 800768 |
-------------------------------
box reached target
Eval num_timesteps=801000, episode_reward=-0.73 +/- 0.47
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.73        |
| time/                   |              |
|    total_timesteps      | 801000       |
| train/                  |              |
|    approx_kl            | 0.0048777256 |
|    clip_fraction        | 0.0289       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.11        |
|    explained_variance   | 0.949        |
|    learning_rate        | 3.96e-05     |
|    loss                 | -0.00603     |
|    n_updates            | 3910         |
|    policy_gradient_loss | -0.00319     |
|    std                  | 0.695        |
|    value_loss           | 0.0117       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=802000, episode_reward=1.79 +/- 2.88
Episode length: 251.80 +/- 59.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 252      |
|    mean_reward     | 1.79     |
| time/              |          |
|    total_timesteps | 802000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 392    |
|    time_elapsed    | 1307   |
|    total_timesteps | 802816 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=803000, episode_reward=2.36 +/- 2.27
Episode length: 241.20 +/- 72.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 241         |
|    mean_reward          | 2.36        |
| time/                   |             |
|    total_timesteps      | 803000      |
| train/                  |             |
|    approx_kl            | 0.004499179 |
|    clip_fraction        | 0.0284      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.11       |
|    explained_variance   | 0.8         |
|    learning_rate        | 3.96e-05    |
|    loss                 | 0.0172      |
|    n_updates            | 3920        |
|    policy_gradient_loss | -0.00242    |
|    std                  | 0.696       |
|    value_loss           | 0.0263      |
-----------------------------------------
box reached target
Eval num_timesteps=804000, episode_reward=1.02 +/- 2.23
Episode length: 276.80 +/- 46.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 277      |
|    mean_reward     | 1.02     |
| time/              |          |
|    total_timesteps | 804000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 393    |
|    time_elapsed    | 1310   |
|    total_timesteps | 804864 |
-------------------------------
Eval num_timesteps=805000, episode_reward=-0.48 +/- 0.82
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.484       |
| time/                   |              |
|    total_timesteps      | 805000       |
| train/                  |              |
|    approx_kl            | 0.0040662084 |
|    clip_fraction        | 0.0208       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.11        |
|    explained_variance   | 0.863        |
|    learning_rate        | 3.97e-05     |
|    loss                 | -0.00272     |
|    n_updates            | 3930         |
|    policy_gradient_loss | -0.00408     |
|    std                  | 0.695        |
|    value_loss           | 0.00373      |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=806000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 806000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 394    |
|    time_elapsed    | 1314   |
|    total_timesteps | 806912 |
-------------------------------
Eval num_timesteps=807000, episode_reward=-0.79 +/- 0.44
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.788      |
| time/                   |             |
|    total_timesteps      | 807000      |
| train/                  |             |
|    approx_kl            | 0.008355039 |
|    clip_fraction        | 0.0689      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.11       |
|    explained_variance   | 0.887       |
|    learning_rate        | 3.97e-05    |
|    loss                 | 0.0507      |
|    n_updates            | 3940        |
|    policy_gradient_loss | -0.00527    |
|    std                  | 0.694       |
|    value_loss           | 0.0505      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=808000, episode_reward=2.08 +/- 2.57
Episode length: 250.20 +/- 61.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 250      |
|    mean_reward     | 2.08     |
| time/              |          |
|    total_timesteps | 808000   |
---------------------------------
box reached target
box reached target
box reached target
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 395    |
|    time_elapsed    | 1317   |
|    total_timesteps | 808960 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=809000, episode_reward=2.03 +/- 2.66
Episode length: 260.80 +/- 49.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 261          |
|    mean_reward          | 2.03         |
| time/                   |              |
|    total_timesteps      | 809000       |
| train/                  |              |
|    approx_kl            | 0.0040757395 |
|    clip_fraction        | 0.0347       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.1         |
|    explained_variance   | 0.964        |
|    learning_rate        | 3.97e-05     |
|    loss                 | -0.00111     |
|    n_updates            | 3950         |
|    policy_gradient_loss | -0.00634     |
|    std                  | 0.692        |
|    value_loss           | 0.0235       |
------------------------------------------
box reached target
Eval num_timesteps=810000, episode_reward=0.25 +/- 2.51
Episode length: 269.00 +/- 62.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 269      |
|    mean_reward     | 0.254    |
| time/              |          |
|    total_timesteps | 810000   |
---------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=811000, episode_reward=-0.38 +/- 0.78
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.378   |
| time/              |          |
|    total_timesteps | 811000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 396    |
|    time_elapsed    | 1321   |
|    total_timesteps | 811008 |
-------------------------------
box reached target
Eval num_timesteps=812000, episode_reward=0.60 +/- 2.50
Episode length: 289.60 +/- 20.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 290          |
|    mean_reward          | 0.596        |
| time/                   |              |
|    total_timesteps      | 812000       |
| train/                  |              |
|    approx_kl            | 0.0053013237 |
|    clip_fraction        | 0.0341       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.1         |
|    explained_variance   | 0.937        |
|    learning_rate        | 3.97e-05     |
|    loss                 | 0.0266       |
|    n_updates            | 3960         |
|    policy_gradient_loss | -0.00396     |
|    std                  | 0.691        |
|    value_loss           | 0.0387       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=813000, episode_reward=2.02 +/- 2.69
Episode length: 253.00 +/- 57.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 253      |
|    mean_reward     | 2.02     |
| time/              |          |
|    total_timesteps | 813000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 397    |
|    time_elapsed    | 1324   |
|    total_timesteps | 813056 |
-------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=814000, episode_reward=2.07 +/- 2.68
Episode length: 250.20 +/- 61.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 250          |
|    mean_reward          | 2.07         |
| time/                   |              |
|    total_timesteps      | 814000       |
| train/                  |              |
|    approx_kl            | 0.0024154894 |
|    clip_fraction        | 0.00708      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.09        |
|    explained_variance   | 0.894        |
|    learning_rate        | 3.98e-05     |
|    loss                 | 0.0371       |
|    n_updates            | 3970         |
|    policy_gradient_loss | -0.00234     |
|    std                  | 0.69         |
|    value_loss           | 0.0354       |
------------------------------------------
box reached target
Eval num_timesteps=815000, episode_reward=0.31 +/- 2.44
Episode length: 274.40 +/- 51.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 274      |
|    mean_reward     | 0.308    |
| time/              |          |
|    total_timesteps | 815000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 398    |
|    time_elapsed    | 1328   |
|    total_timesteps | 815104 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=816000, episode_reward=0.77 +/- 2.31
Episode length: 271.20 +/- 57.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 271          |
|    mean_reward          | 0.772        |
| time/                   |              |
|    total_timesteps      | 816000       |
| train/                  |              |
|    approx_kl            | 0.0028402572 |
|    clip_fraction        | 0.0105       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.09        |
|    explained_variance   | 0.936        |
|    learning_rate        | 3.98e-05     |
|    loss                 | 0.000611     |
|    n_updates            | 3980         |
|    policy_gradient_loss | -0.00509     |
|    std                  | 0.691        |
|    value_loss           | 0.0235       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=817000, episode_reward=3.08 +/- 2.75
Episode length: 232.20 +/- 57.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 232      |
|    mean_reward     | 3.08     |
| time/              |          |
|    total_timesteps | 817000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 399    |
|    time_elapsed    | 1331   |
|    total_timesteps | 817152 |
-------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=818000, episode_reward=2.03 +/- 2.67
Episode length: 245.60 +/- 66.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 246          |
|    mean_reward          | 2.03         |
| time/                   |              |
|    total_timesteps      | 818000       |
| train/                  |              |
|    approx_kl            | 0.0052882875 |
|    clip_fraction        | 0.0286       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.09        |
|    explained_variance   | 0.975        |
|    learning_rate        | 3.98e-05     |
|    loss                 | 0.00428      |
|    n_updates            | 3990         |
|    policy_gradient_loss | -0.00415     |
|    std                  | 0.689        |
|    value_loss           | 0.0189       |
------------------------------------------
box reached target
Eval num_timesteps=819000, episode_reward=0.82 +/- 2.31
Episode length: 275.80 +/- 48.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 276      |
|    mean_reward     | 0.821    |
| time/              |          |
|    total_timesteps | 819000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 400    |
|    time_elapsed    | 1334   |
|    total_timesteps | 819200 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=820000, episode_reward=0.66 +/- 2.50
Episode length: 289.20 +/- 21.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 289          |
|    mean_reward          | 0.657        |
| time/                   |              |
|    total_timesteps      | 820000       |
| train/                  |              |
|    approx_kl            | 0.0043540904 |
|    clip_fraction        | 0.035        |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.09        |
|    explained_variance   | 0.958        |
|    learning_rate        | 3.98e-05     |
|    loss                 | 0.00294      |
|    n_updates            | 4000         |
|    policy_gradient_loss | -0.00452     |
|    std                  | 0.69         |
|    value_loss           | 0.0141       |
------------------------------------------
box reached target
Eval num_timesteps=821000, episode_reward=-0.37 +/- 0.78
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.366   |
| time/              |          |
|    total_timesteps | 821000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 401    |
|    time_elapsed    | 1337   |
|    total_timesteps | 821248 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=822000, episode_reward=0.32 +/- 2.60
Episode length: 287.80 +/- 24.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 288          |
|    mean_reward          | 0.323        |
| time/                   |              |
|    total_timesteps      | 822000       |
| train/                  |              |
|    approx_kl            | 0.0040558837 |
|    clip_fraction        | 0.0257       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.09        |
|    explained_variance   | 0.948        |
|    learning_rate        | 3.99e-05     |
|    loss                 | -0.0154      |
|    n_updates            | 4010         |
|    policy_gradient_loss | -0.00474     |
|    std                  | 0.689        |
|    value_loss           | 0.0166       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=823000, episode_reward=0.56 +/- 2.48
Episode length: 280.80 +/- 38.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 281      |
|    mean_reward     | 0.564    |
| time/              |          |
|    total_timesteps | 823000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 402    |
|    time_elapsed    | 1341   |
|    total_timesteps | 823296 |
-------------------------------
box reached target
Eval num_timesteps=824000, episode_reward=0.75 +/- 2.40
Episode length: 278.00 +/- 44.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 278        |
|    mean_reward          | 0.753      |
| time/                   |            |
|    total_timesteps      | 824000     |
| train/                  |            |
|    approx_kl            | 0.00422158 |
|    clip_fraction        | 0.021      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.09      |
|    explained_variance   | 0.928      |
|    learning_rate        | 3.99e-05   |
|    loss                 | 0.0207     |
|    n_updates            | 4020       |
|    policy_gradient_loss | -0.00391   |
|    std                  | 0.688      |
|    value_loss           | 0.0497     |
----------------------------------------
box reached target
box reached target
Eval num_timesteps=825000, episode_reward=0.48 +/- 2.38
Episode length: 270.40 +/- 59.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 270      |
|    mean_reward     | 0.482    |
| time/              |          |
|    total_timesteps | 825000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 403    |
|    time_elapsed    | 1344   |
|    total_timesteps | 825344 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=826000, episode_reward=2.10 +/- 2.67
Episode length: 238.20 +/- 75.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 238          |
|    mean_reward          | 2.1          |
| time/                   |              |
|    total_timesteps      | 826000       |
| train/                  |              |
|    approx_kl            | 0.0043559074 |
|    clip_fraction        | 0.0264       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.09        |
|    explained_variance   | 0.901        |
|    learning_rate        | 3.99e-05     |
|    loss                 | -0.00664     |
|    n_updates            | 4030         |
|    policy_gradient_loss | -0.00294     |
|    std                  | 0.687        |
|    value_loss           | 0.0187       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=827000, episode_reward=0.96 +/- 2.38
Episode length: 289.60 +/- 20.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 290      |
|    mean_reward     | 0.958    |
| time/              |          |
|    total_timesteps | 827000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 404    |
|    time_elapsed    | 1347   |
|    total_timesteps | 827392 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=828000, episode_reward=1.20 +/- 2.25
Episode length: 280.40 +/- 39.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 280          |
|    mean_reward          | 1.2          |
| time/                   |              |
|    total_timesteps      | 828000       |
| train/                  |              |
|    approx_kl            | 0.0041852044 |
|    clip_fraction        | 0.0146       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.08        |
|    explained_variance   | 0.952        |
|    learning_rate        | 3.99e-05     |
|    loss                 | 0.000539     |
|    n_updates            | 4040         |
|    policy_gradient_loss | -0.00282     |
|    std                  | 0.685        |
|    value_loss           | 0.0245       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=829000, episode_reward=0.61 +/- 2.30
Episode length: 278.00 +/- 44.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | 0.612    |
| time/              |          |
|    total_timesteps | 829000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 405    |
|    time_elapsed    | 1351   |
|    total_timesteps | 829440 |
-------------------------------
box reached target
Eval num_timesteps=830000, episode_reward=0.80 +/- 2.18
Episode length: 275.40 +/- 49.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 275         |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 830000      |
| train/                  |             |
|    approx_kl            | 0.006078555 |
|    clip_fraction        | 0.05        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.08       |
|    explained_variance   | 0.971       |
|    learning_rate        | 4e-05       |
|    loss                 | -0.0176     |
|    n_updates            | 4050        |
|    policy_gradient_loss | -0.00529    |
|    std                  | 0.685       |
|    value_loss           | 0.0203      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=831000, episode_reward=1.51 +/- 3.08
Episode length: 251.80 +/- 59.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 252      |
|    mean_reward     | 1.51     |
| time/              |          |
|    total_timesteps | 831000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 406    |
|    time_elapsed    | 1354   |
|    total_timesteps | 831488 |
-------------------------------
box reached target
Eval num_timesteps=832000, episode_reward=0.49 +/- 2.38
Episode length: 275.00 +/- 50.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 275          |
|    mean_reward          | 0.486        |
| time/                   |              |
|    total_timesteps      | 832000       |
| train/                  |              |
|    approx_kl            | 0.0054368293 |
|    clip_fraction        | 0.0463       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.08        |
|    explained_variance   | 0.855        |
|    learning_rate        | 4e-05        |
|    loss                 | -0.0165      |
|    n_updates            | 4060         |
|    policy_gradient_loss | -0.00637     |
|    std                  | 0.687        |
|    value_loss           | 0.013        |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=833000, episode_reward=0.63 +/- 2.35
Episode length: 276.40 +/- 47.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 276      |
|    mean_reward     | 0.635    |
| time/              |          |
|    total_timesteps | 833000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 407    |
|    time_elapsed    | 1357   |
|    total_timesteps | 833536 |
-------------------------------
box reached target
Eval num_timesteps=834000, episode_reward=1.04 +/- 2.13
Episode length: 269.60 +/- 60.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 270          |
|    mean_reward          | 1.04         |
| time/                   |              |
|    total_timesteps      | 834000       |
| train/                  |              |
|    approx_kl            | 0.0033555848 |
|    clip_fraction        | 0.0138       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.08        |
|    explained_variance   | 0.964        |
|    learning_rate        | 4e-05        |
|    loss                 | 0.00243      |
|    n_updates            | 4070         |
|    policy_gradient_loss | -0.00471     |
|    std                  | 0.685        |
|    value_loss           | 0.0356       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=835000, episode_reward=1.63 +/- 2.94
Episode length: 255.60 +/- 59.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 256      |
|    mean_reward     | 1.63     |
| time/              |          |
|    total_timesteps | 835000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 408    |
|    time_elapsed    | 1360   |
|    total_timesteps | 835584 |
-------------------------------
box reached target
Eval num_timesteps=836000, episode_reward=0.74 +/- 2.34
Episode length: 272.40 +/- 55.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 272         |
|    mean_reward          | 0.74        |
| time/                   |             |
|    total_timesteps      | 836000      |
| train/                  |             |
|    approx_kl            | 0.003701348 |
|    clip_fraction        | 0.0171      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.08       |
|    explained_variance   | 0.972       |
|    learning_rate        | 4e-05       |
|    loss                 | 0.0283      |
|    n_updates            | 4080        |
|    policy_gradient_loss | -0.00461    |
|    std                  | 0.684       |
|    value_loss           | 0.00768     |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=837000, episode_reward=1.84 +/- 2.99
Episode length: 244.80 +/- 67.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 245      |
|    mean_reward     | 1.84     |
| time/              |          |
|    total_timesteps | 837000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 409    |
|    time_elapsed    | 1364   |
|    total_timesteps | 837632 |
-------------------------------
box reached target
Eval num_timesteps=838000, episode_reward=0.55 +/- 2.38
Episode length: 271.80 +/- 56.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 272         |
|    mean_reward          | 0.55        |
| time/                   |             |
|    total_timesteps      | 838000      |
| train/                  |             |
|    approx_kl            | 0.003657635 |
|    clip_fraction        | 0.0145      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.07       |
|    explained_variance   | 0.934       |
|    learning_rate        | 4.01e-05    |
|    loss                 | -0.00769    |
|    n_updates            | 4090        |
|    policy_gradient_loss | -0.00384    |
|    std                  | 0.681       |
|    value_loss           | 0.0207      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=839000, episode_reward=1.79 +/- 2.84
Episode length: 252.20 +/- 58.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 252      |
|    mean_reward     | 1.79     |
| time/              |          |
|    total_timesteps | 839000   |
---------------------------------
box reached target
box reached target
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 410    |
|    time_elapsed    | 1367   |
|    total_timesteps | 839680 |
-------------------------------
box reached target
Eval num_timesteps=840000, episode_reward=0.26 +/- 2.51
Episode length: 272.40 +/- 55.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 272          |
|    mean_reward          | 0.255        |
| time/                   |              |
|    total_timesteps      | 840000       |
| train/                  |              |
|    approx_kl            | 0.0047483183 |
|    clip_fraction        | 0.0398       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.07        |
|    explained_variance   | 0.849        |
|    learning_rate        | 4.01e-05     |
|    loss                 | 0.0581       |
|    n_updates            | 4100         |
|    policy_gradient_loss | -0.00491     |
|    std                  | 0.683        |
|    value_loss           | 0.0861       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=841000, episode_reward=2.65 +/- 3.10
Episode length: 223.40 +/- 64.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 223      |
|    mean_reward     | 2.65     |
| time/              |          |
|    total_timesteps | 841000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 411    |
|    time_elapsed    | 1370   |
|    total_timesteps | 841728 |
-------------------------------
box reached target
Eval num_timesteps=842000, episode_reward=0.37 +/- 2.38
Episode length: 278.80 +/- 42.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 279          |
|    mean_reward          | 0.374        |
| time/                   |              |
|    total_timesteps      | 842000       |
| train/                  |              |
|    approx_kl            | 0.0035884338 |
|    clip_fraction        | 0.0221       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.07        |
|    explained_variance   | 0.906        |
|    learning_rate        | 4.01e-05     |
|    loss                 | -0.00137     |
|    n_updates            | 4110         |
|    policy_gradient_loss | -0.00202     |
|    std                  | 0.681        |
|    value_loss           | 0.0148       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=843000, episode_reward=3.14 +/- 2.43
Episode length: 230.80 +/- 56.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 231      |
|    mean_reward     | 3.14     |
| time/              |          |
|    total_timesteps | 843000   |
---------------------------------
box reached target
box reached target
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 412    |
|    time_elapsed    | 1373   |
|    total_timesteps | 843776 |
-------------------------------
Eval num_timesteps=844000, episode_reward=-0.26 +/- 0.66
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.263       |
| time/                   |              |
|    total_timesteps      | 844000       |
| train/                  |              |
|    approx_kl            | 0.0025393951 |
|    clip_fraction        | 0.00801      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.07        |
|    explained_variance   | 0.935        |
|    learning_rate        | 4.01e-05     |
|    loss                 | 0.0246       |
|    n_updates            | 4120         |
|    policy_gradient_loss | -0.00185     |
|    std                  | 0.681        |
|    value_loss           | 0.0643       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=845000, episode_reward=0.67 +/- 2.56
Episode length: 282.80 +/- 34.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 283      |
|    mean_reward     | 0.669    |
| time/              |          |
|    total_timesteps | 845000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 413    |
|    time_elapsed    | 1377   |
|    total_timesteps | 845824 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=846000, episode_reward=1.53 +/- 3.10
Episode length: 253.20 +/- 58.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 253          |
|    mean_reward          | 1.53         |
| time/                   |              |
|    total_timesteps      | 846000       |
| train/                  |              |
|    approx_kl            | 0.0050288695 |
|    clip_fraction        | 0.042        |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.07        |
|    explained_variance   | 0.978        |
|    learning_rate        | 4.01e-05     |
|    loss                 | 0.0176       |
|    n_updates            | 4130         |
|    policy_gradient_loss | -0.00558     |
|    std                  | 0.68         |
|    value_loss           | 0.00737      |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=847000, episode_reward=0.58 +/- 2.33
Episode length: 275.00 +/- 50.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 275      |
|    mean_reward     | 0.583    |
| time/              |          |
|    total_timesteps | 847000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 414    |
|    time_elapsed    | 1380   |
|    total_timesteps | 847872 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=848000, episode_reward=1.49 +/- 3.16
Episode length: 264.20 +/- 60.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 264          |
|    mean_reward          | 1.49         |
| time/                   |              |
|    total_timesteps      | 848000       |
| train/                  |              |
|    approx_kl            | 0.0048874035 |
|    clip_fraction        | 0.0226       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.06        |
|    explained_variance   | 0.918        |
|    learning_rate        | 4.02e-05     |
|    loss                 | -0.00153     |
|    n_updates            | 4140         |
|    policy_gradient_loss | -0.00241     |
|    std                  | 0.68         |
|    value_loss           | 0.0313       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=849000, episode_reward=2.77 +/- 3.08
Episode length: 237.00 +/- 55.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 237      |
|    mean_reward     | 2.77     |
| time/              |          |
|    total_timesteps | 849000   |
---------------------------------
box reached target
box reached target
box reached target
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 415    |
|    time_elapsed    | 1383   |
|    total_timesteps | 849920 |
-------------------------------
box reached target
Eval num_timesteps=850000, episode_reward=0.60 +/- 2.39
Episode length: 272.40 +/- 55.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 272          |
|    mean_reward          | 0.598        |
| time/                   |              |
|    total_timesteps      | 850000       |
| train/                  |              |
|    approx_kl            | 0.0049113873 |
|    clip_fraction        | 0.018        |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.06        |
|    explained_variance   | 0.936        |
|    learning_rate        | 4.02e-05     |
|    loss                 | 0.0143       |
|    n_updates            | 4150         |
|    policy_gradient_loss | -0.00316     |
|    std                  | 0.679        |
|    value_loss           | 0.046        |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=851000, episode_reward=1.48 +/- 3.04
Episode length: 250.60 +/- 61.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | 1.48     |
| time/              |          |
|    total_timesteps | 851000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 416    |
|    time_elapsed    | 1386   |
|    total_timesteps | 851968 |
-------------------------------
box reached target
Eval num_timesteps=852000, episode_reward=0.48 +/- 2.34
Episode length: 273.20 +/- 53.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 273          |
|    mean_reward          | 0.476        |
| time/                   |              |
|    total_timesteps      | 852000       |
| train/                  |              |
|    approx_kl            | 0.0039913678 |
|    clip_fraction        | 0.0245       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.06        |
|    explained_variance   | 0.842        |
|    learning_rate        | 4.02e-05     |
|    loss                 | 0.0188       |
|    n_updates            | 4160         |
|    policy_gradient_loss | -0.00256     |
|    std                  | 0.677        |
|    value_loss           | 0.0887       |
------------------------------------------
box reached target
Eval num_timesteps=853000, episode_reward=-0.37 +/- 0.51
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.374   |
| time/              |          |
|    total_timesteps | 853000   |
---------------------------------
box reached target
box reached target
Eval num_timesteps=854000, episode_reward=1.98 +/- 2.65
Episode length: 242.00 +/- 72.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 242      |
|    mean_reward     | 1.98     |
| time/              |          |
|    total_timesteps | 854000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 417    |
|    time_elapsed    | 1390   |
|    total_timesteps | 854016 |
-------------------------------
Eval num_timesteps=855000, episode_reward=-0.19 +/- 0.67
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.194       |
| time/                   |              |
|    total_timesteps      | 855000       |
| train/                  |              |
|    approx_kl            | 0.0050487584 |
|    clip_fraction        | 0.0309       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.06        |
|    explained_variance   | 0.872        |
|    learning_rate        | 4.02e-05     |
|    loss                 | -0.011       |
|    n_updates            | 4170         |
|    policy_gradient_loss | -0.00523     |
|    std                  | 0.677        |
|    value_loss           | 0.0391       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=856000, episode_reward=0.54 +/- 2.31
Episode length: 271.40 +/- 57.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 271      |
|    mean_reward     | 0.537    |
| time/              |          |
|    total_timesteps | 856000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 613    |
|    iterations      | 418    |
|    time_elapsed    | 1394   |
|    total_timesteps | 856064 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=857000, episode_reward=1.83 +/- 2.91
Episode length: 256.40 +/- 55.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 256         |
|    mean_reward          | 1.83        |
| time/                   |             |
|    total_timesteps      | 857000      |
| train/                  |             |
|    approx_kl            | 0.002045909 |
|    clip_fraction        | 0.0064      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.05       |
|    explained_variance   | 0.828       |
|    learning_rate        | 4.03e-05    |
|    loss                 | 0.00178     |
|    n_updates            | 4180        |
|    policy_gradient_loss | -0.00189    |
|    std                  | 0.676       |
|    value_loss           | 0.0838      |
-----------------------------------------
box reached target
Eval num_timesteps=858000, episode_reward=0.69 +/- 2.47
Episode length: 284.20 +/- 31.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 284      |
|    mean_reward     | 0.692    |
| time/              |          |
|    total_timesteps | 858000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 419    |
|    time_elapsed    | 1397   |
|    total_timesteps | 858112 |
-------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=859000, episode_reward=2.71 +/- 3.03
Episode length: 231.00 +/- 60.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 231         |
|    mean_reward          | 2.71        |
| time/                   |             |
|    total_timesteps      | 859000      |
| train/                  |             |
|    approx_kl            | 0.004794049 |
|    clip_fraction        | 0.0194      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.05       |
|    explained_variance   | 0.686       |
|    learning_rate        | 4.03e-05    |
|    loss                 | 0.0239      |
|    n_updates            | 4190        |
|    policy_gradient_loss | -0.00343    |
|    std                  | 0.677       |
|    value_loss           | 0.0258      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=860000, episode_reward=1.99 +/- 2.79
Episode length: 257.40 +/- 61.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 257      |
|    mean_reward     | 1.99     |
| time/              |          |
|    total_timesteps | 860000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 420    |
|    time_elapsed    | 1400   |
|    total_timesteps | 860160 |
-------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=861000, episode_reward=1.50 +/- 3.06
Episode length: 253.60 +/- 56.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 254          |
|    mean_reward          | 1.5          |
| time/                   |              |
|    total_timesteps      | 861000       |
| train/                  |              |
|    approx_kl            | 0.0057819597 |
|    clip_fraction        | 0.0341       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.05        |
|    explained_variance   | 0.736        |
|    learning_rate        | 4.03e-05     |
|    loss                 | 0.0102       |
|    n_updates            | 4200         |
|    policy_gradient_loss | -0.00666     |
|    std                  | 0.676        |
|    value_loss           | 0.0789       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=862000, episode_reward=0.82 +/- 2.51
Episode length: 284.60 +/- 30.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 285      |
|    mean_reward     | 0.819    |
| time/              |          |
|    total_timesteps | 862000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 421    |
|    time_elapsed    | 1404   |
|    total_timesteps | 862208 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=863000, episode_reward=0.59 +/- 2.40
Episode length: 278.00 +/- 44.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 278          |
|    mean_reward          | 0.591        |
| time/                   |              |
|    total_timesteps      | 863000       |
| train/                  |              |
|    approx_kl            | 0.0049340753 |
|    clip_fraction        | 0.0318       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.05        |
|    explained_variance   | 0.888        |
|    learning_rate        | 4.03e-05     |
|    loss                 | 0.0243       |
|    n_updates            | 4210         |
|    policy_gradient_loss | -0.0061      |
|    std                  | 0.675        |
|    value_loss           | 0.0618       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=864000, episode_reward=4.23 +/- 1.89
Episode length: 196.40 +/- 54.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 196      |
|    mean_reward     | 4.23     |
| time/              |          |
|    total_timesteps | 864000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 422    |
|    time_elapsed    | 1407   |
|    total_timesteps | 864256 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=865000, episode_reward=1.87 +/- 2.82
Episode length: 251.00 +/- 61.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 251          |
|    mean_reward          | 1.87         |
| time/                   |              |
|    total_timesteps      | 865000       |
| train/                  |              |
|    approx_kl            | 0.0027438896 |
|    clip_fraction        | 0.00962      |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.05        |
|    explained_variance   | 0.853        |
|    learning_rate        | 4.04e-05     |
|    loss                 | 0.0534       |
|    n_updates            | 4220         |
|    policy_gradient_loss | -0.00236     |
|    std                  | 0.674        |
|    value_loss           | 0.068        |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=866000, episode_reward=1.52 +/- 3.09
Episode length: 254.00 +/- 59.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 254      |
|    mean_reward     | 1.52     |
| time/              |          |
|    total_timesteps | 866000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 423    |
|    time_elapsed    | 1410   |
|    total_timesteps | 866304 |
-------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=867000, episode_reward=1.74 +/- 2.95
Episode length: 248.80 +/- 63.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 249          |
|    mean_reward          | 1.74         |
| time/                   |              |
|    total_timesteps      | 867000       |
| train/                  |              |
|    approx_kl            | 0.0053636953 |
|    clip_fraction        | 0.0487       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.05        |
|    explained_variance   | 0.907        |
|    learning_rate        | 4.04e-05     |
|    loss                 | 0.00104      |
|    n_updates            | 4230         |
|    policy_gradient_loss | -0.00538     |
|    std                  | 0.674        |
|    value_loss           | 0.0478       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=868000, episode_reward=-0.84 +/- 0.20
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.841   |
| time/              |          |
|    total_timesteps | 868000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 424    |
|    time_elapsed    | 1413   |
|    total_timesteps | 868352 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=869000, episode_reward=0.32 +/- 2.45
Episode length: 276.60 +/- 46.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 277         |
|    mean_reward          | 0.321       |
| time/                   |             |
|    total_timesteps      | 869000      |
| train/                  |             |
|    approx_kl            | 0.003962241 |
|    clip_fraction        | 0.0231      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.04       |
|    explained_variance   | 0.848       |
|    learning_rate        | 4.04e-05    |
|    loss                 | 0.0268      |
|    n_updates            | 4240        |
|    policy_gradient_loss | -0.00353    |
|    std                  | 0.671       |
|    value_loss           | 0.104       |
-----------------------------------------
box reached target
Eval num_timesteps=870000, episode_reward=-0.08 +/- 0.64
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.0819  |
| time/              |          |
|    total_timesteps | 870000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 425    |
|    time_elapsed    | 1417   |
|    total_timesteps | 870400 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=871000, episode_reward=2.05 +/- 2.74
Episode length: 259.00 +/- 51.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 259          |
|    mean_reward          | 2.05         |
| time/                   |              |
|    total_timesteps      | 871000       |
| train/                  |              |
|    approx_kl            | 0.0056969514 |
|    clip_fraction        | 0.041        |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.04        |
|    explained_variance   | 0.934        |
|    learning_rate        | 4.04e-05     |
|    loss                 | -0.00795     |
|    n_updates            | 4250         |
|    policy_gradient_loss | -0.00653     |
|    std                  | 0.671        |
|    value_loss           | 0.0538       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=872000, episode_reward=1.71 +/- 2.97
Episode length: 247.00 +/- 65.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 247      |
|    mean_reward     | 1.71     |
| time/              |          |
|    total_timesteps | 872000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 426    |
|    time_elapsed    | 1420   |
|    total_timesteps | 872448 |
-------------------------------
Eval num_timesteps=873000, episode_reward=-0.17 +/- 0.55
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.172       |
| time/                   |              |
|    total_timesteps      | 873000       |
| train/                  |              |
|    approx_kl            | 0.0050785774 |
|    clip_fraction        | 0.0236       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.04        |
|    explained_variance   | 0.82         |
|    learning_rate        | 4.05e-05     |
|    loss                 | -0.00823     |
|    n_updates            | 4260         |
|    policy_gradient_loss | -0.00351     |
|    std                  | 0.672        |
|    value_loss           | 0.0282       |
------------------------------------------
box reached target
Eval num_timesteps=874000, episode_reward=0.44 +/- 2.44
Episode length: 269.40 +/- 61.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 269      |
|    mean_reward     | 0.443    |
| time/              |          |
|    total_timesteps | 874000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 427    |
|    time_elapsed    | 1423   |
|    total_timesteps | 874496 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=875000, episode_reward=0.74 +/- 2.39
Episode length: 275.80 +/- 48.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 276          |
|    mean_reward          | 0.739        |
| time/                   |              |
|    total_timesteps      | 875000       |
| train/                  |              |
|    approx_kl            | 0.0051861135 |
|    clip_fraction        | 0.0279       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.04        |
|    explained_variance   | 0.789        |
|    learning_rate        | 4.05e-05     |
|    loss                 | 0.00433      |
|    n_updates            | 4270         |
|    policy_gradient_loss | -0.00521     |
|    std                  | 0.672        |
|    value_loss           | 0.0174       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=876000, episode_reward=1.62 +/- 3.21
Episode length: 255.80 +/- 54.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 256      |
|    mean_reward     | 1.62     |
| time/              |          |
|    total_timesteps | 876000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 428    |
|    time_elapsed    | 1426   |
|    total_timesteps | 876544 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=877000, episode_reward=0.89 +/- 2.30
Episode length: 275.80 +/- 48.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 276          |
|    mean_reward          | 0.894        |
| time/                   |              |
|    total_timesteps      | 877000       |
| train/                  |              |
|    approx_kl            | 0.0034876247 |
|    clip_fraction        | 0.0228       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.04        |
|    explained_variance   | 0.93         |
|    learning_rate        | 4.05e-05     |
|    loss                 | -0.0212      |
|    n_updates            | 4280         |
|    policy_gradient_loss | -0.00508     |
|    std                  | 0.673        |
|    value_loss           | 0.0284       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=878000, episode_reward=1.90 +/- 2.83
Episode length: 257.60 +/- 53.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 258      |
|    mean_reward     | 1.9      |
| time/              |          |
|    total_timesteps | 878000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 429    |
|    time_elapsed    | 1430   |
|    total_timesteps | 878592 |
-------------------------------
box reached target
Eval num_timesteps=879000, episode_reward=0.44 +/- 2.48
Episode length: 280.20 +/- 39.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 280          |
|    mean_reward          | 0.441        |
| time/                   |              |
|    total_timesteps      | 879000       |
| train/                  |              |
|    approx_kl            | 0.0044850917 |
|    clip_fraction        | 0.0329       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.04        |
|    explained_variance   | 0.928        |
|    learning_rate        | 4.05e-05     |
|    loss                 | 0.00557      |
|    n_updates            | 4290         |
|    policy_gradient_loss | -0.00463     |
|    std                  | 0.672        |
|    value_loss           | 0.0393       |
------------------------------------------
box reached target
Eval num_timesteps=880000, episode_reward=0.67 +/- 2.37
Episode length: 272.60 +/- 54.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 273      |
|    mean_reward     | 0.673    |
| time/              |          |
|    total_timesteps | 880000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 430    |
|    time_elapsed    | 1433   |
|    total_timesteps | 880640 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=881000, episode_reward=1.48 +/- 3.04
Episode length: 254.20 +/- 56.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 254          |
|    mean_reward          | 1.48         |
| time/                   |              |
|    total_timesteps      | 881000       |
| train/                  |              |
|    approx_kl            | 0.0049635624 |
|    clip_fraction        | 0.0401       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.04        |
|    explained_variance   | 0.967        |
|    learning_rate        | 4.06e-05     |
|    loss                 | -0.013       |
|    n_updates            | 4300         |
|    policy_gradient_loss | -0.00477     |
|    std                  | 0.674        |
|    value_loss           | 0.00756      |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=882000, episode_reward=1.51 +/- 3.07
Episode length: 245.40 +/- 67.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 245      |
|    mean_reward     | 1.51     |
| time/              |          |
|    total_timesteps | 882000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 431    |
|    time_elapsed    | 1436   |
|    total_timesteps | 882688 |
-------------------------------
box reached target
Eval num_timesteps=883000, episode_reward=0.57 +/- 2.45
Episode length: 276.00 +/- 48.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 276          |
|    mean_reward          | 0.574        |
| time/                   |              |
|    total_timesteps      | 883000       |
| train/                  |              |
|    approx_kl            | 0.0044433633 |
|    clip_fraction        | 0.0227       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.05        |
|    explained_variance   | 0.964        |
|    learning_rate        | 4.06e-05     |
|    loss                 | 0.0043       |
|    n_updates            | 4310         |
|    policy_gradient_loss | -0.0051      |
|    std                  | 0.674        |
|    value_loss           | 0.00528      |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=884000, episode_reward=1.89 +/- 2.86
Episode length: 249.20 +/- 67.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 249      |
|    mean_reward     | 1.89     |
| time/              |          |
|    total_timesteps | 884000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 432    |
|    time_elapsed    | 1439   |
|    total_timesteps | 884736 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=885000, episode_reward=1.80 +/- 3.01
Episode length: 262.80 +/- 48.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 263          |
|    mean_reward          | 1.8          |
| time/                   |              |
|    total_timesteps      | 885000       |
| train/                  |              |
|    approx_kl            | 0.0045650983 |
|    clip_fraction        | 0.0301       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.05        |
|    explained_variance   | 0.809        |
|    learning_rate        | 4.06e-05     |
|    loss                 | 0.00643      |
|    n_updates            | 4320         |
|    policy_gradient_loss | -0.0044      |
|    std                  | 0.675        |
|    value_loss           | 0.0292       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=886000, episode_reward=3.95 +/- 2.48
Episode length: 191.00 +/- 57.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 191      |
|    mean_reward     | 3.95     |
| time/              |          |
|    total_timesteps | 886000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 433    |
|    time_elapsed    | 1443   |
|    total_timesteps | 886784 |
-------------------------------
box reached target
Eval num_timesteps=887000, episode_reward=0.71 +/- 2.34
Episode length: 295.20 +/- 9.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 295         |
|    mean_reward          | 0.712       |
| time/                   |             |
|    total_timesteps      | 887000      |
| train/                  |             |
|    approx_kl            | 0.004240389 |
|    clip_fraction        | 0.0314      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.05       |
|    explained_variance   | 0.941       |
|    learning_rate        | 4.06e-05    |
|    loss                 | 0.0116      |
|    n_updates            | 4330        |
|    policy_gradient_loss | -0.00515    |
|    std                  | 0.672       |
|    value_loss           | 0.036       |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=888000, episode_reward=1.78 +/- 2.85
Episode length: 245.80 +/- 66.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 246      |
|    mean_reward     | 1.78     |
| time/              |          |
|    total_timesteps | 888000   |
---------------------------------
box reached target
box reached target
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 434    |
|    time_elapsed    | 1446   |
|    total_timesteps | 888832 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=889000, episode_reward=1.47 +/- 3.02
Episode length: 249.40 +/- 62.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 249          |
|    mean_reward          | 1.47         |
| time/                   |              |
|    total_timesteps      | 889000       |
| train/                  |              |
|    approx_kl            | 0.0035468796 |
|    clip_fraction        | 0.0186       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.04        |
|    explained_variance   | 0.852        |
|    learning_rate        | 4.07e-05     |
|    loss                 | 0.0107       |
|    n_updates            | 4340         |
|    policy_gradient_loss | -0.00425     |
|    std                  | 0.671        |
|    value_loss           | 0.0852       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=890000, episode_reward=-0.15 +/- 0.61
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.152   |
| time/              |          |
|    total_timesteps | 890000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 435    |
|    time_elapsed    | 1449   |
|    total_timesteps | 890880 |
-------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=891000, episode_reward=5.23 +/- 0.10
Episode length: 171.40 +/- 20.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 171          |
|    mean_reward          | 5.23         |
| time/                   |              |
|    total_timesteps      | 891000       |
| train/                  |              |
|    approx_kl            | 0.0037105966 |
|    clip_fraction        | 0.0156       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.04        |
|    explained_variance   | 0.85         |
|    learning_rate        | 4.07e-05     |
|    loss                 | -0.00622     |
|    n_updates            | 4350         |
|    policy_gradient_loss | -0.00521     |
|    std                  | 0.67         |
|    value_loss           | 0.0483       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=892000, episode_reward=2.28 +/- 2.33
Episode length: 249.60 +/- 62.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 250      |
|    mean_reward     | 2.28     |
| time/              |          |
|    total_timesteps | 892000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 436    |
|    time_elapsed    | 1452   |
|    total_timesteps | 892928 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=893000, episode_reward=1.68 +/- 3.02
Episode length: 265.60 +/- 43.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 266          |
|    mean_reward          | 1.68         |
| time/                   |              |
|    total_timesteps      | 893000       |
| train/                  |              |
|    approx_kl            | 0.0033976831 |
|    clip_fraction        | 0.0136       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.03        |
|    explained_variance   | 0.931        |
|    learning_rate        | 4.07e-05     |
|    loss                 | 0.0239       |
|    n_updates            | 4360         |
|    policy_gradient_loss | -0.00312     |
|    std                  | 0.669        |
|    value_loss           | 0.0337       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=894000, episode_reward=-0.40 +/- 0.74
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.396   |
| time/              |          |
|    total_timesteps | 894000   |
---------------------------------
box reached target
box reached target
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 437    |
|    time_elapsed    | 1456   |
|    total_timesteps | 894976 |
-------------------------------
Eval num_timesteps=895000, episode_reward=-0.65 +/- 0.69
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.653       |
| time/                   |              |
|    total_timesteps      | 895000       |
| train/                  |              |
|    approx_kl            | 0.0038347156 |
|    clip_fraction        | 0.0284       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.03        |
|    explained_variance   | 0.929        |
|    learning_rate        | 4.07e-05     |
|    loss                 | -0.00838     |
|    n_updates            | 4370         |
|    policy_gradient_loss | -0.00411     |
|    std                  | 0.67         |
|    value_loss           | 0.0446       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=896000, episode_reward=3.07 +/- 2.77
Episode length: 224.80 +/- 62.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 225      |
|    mean_reward     | 3.07     |
| time/              |          |
|    total_timesteps | 896000   |
---------------------------------
Eval num_timesteps=897000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 897000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 438    |
|    time_elapsed    | 1459   |
|    total_timesteps | 897024 |
-------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=898000, episode_reward=2.79 +/- 3.10
Episode length: 233.40 +/- 54.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 233         |
|    mean_reward          | 2.79        |
| time/                   |             |
|    total_timesteps      | 898000      |
| train/                  |             |
|    approx_kl            | 0.003995723 |
|    clip_fraction        | 0.02        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.03       |
|    explained_variance   | 0.792       |
|    learning_rate        | 4.08e-05    |
|    loss                 | -0.0382     |
|    n_updates            | 4380        |
|    policy_gradient_loss | -0.00436    |
|    std                  | 0.67        |
|    value_loss           | 0.0178      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=899000, episode_reward=2.98 +/- 2.75
Episode length: 221.00 +/- 71.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 221      |
|    mean_reward     | 2.98     |
| time/              |          |
|    total_timesteps | 899000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 439    |
|    time_elapsed    | 1463   |
|    total_timesteps | 899072 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=900000, episode_reward=1.80 +/- 2.80
Episode length: 252.20 +/- 58.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 252          |
|    mean_reward          | 1.8          |
| time/                   |              |
|    total_timesteps      | 900000       |
| train/                  |              |
|    approx_kl            | 0.0032539326 |
|    clip_fraction        | 0.0102       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.04        |
|    explained_variance   | 0.946        |
|    learning_rate        | 4.08e-05     |
|    loss                 | -0.00521     |
|    n_updates            | 4390         |
|    policy_gradient_loss | -0.00386     |
|    std                  | 0.671        |
|    value_loss           | 0.0165       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=901000, episode_reward=1.83 +/- 2.84
Episode length: 241.40 +/- 71.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 241      |
|    mean_reward     | 1.83     |
| time/              |          |
|    total_timesteps | 901000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 440    |
|    time_elapsed    | 1466   |
|    total_timesteps | 901120 |
-------------------------------
Eval num_timesteps=902000, episode_reward=-0.66 +/- 0.68
Episode length: 300.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 300        |
|    mean_reward          | -0.66      |
| time/                   |            |
|    total_timesteps      | 902000     |
| train/                  |            |
|    approx_kl            | 0.00438761 |
|    clip_fraction        | 0.0328     |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.04      |
|    explained_variance   | 0.899      |
|    learning_rate        | 4.08e-05   |
|    loss                 | -0.0104    |
|    n_updates            | 4400       |
|    policy_gradient_loss | -0.0036    |
|    std                  | 0.669      |
|    value_loss           | 0.0095     |
----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=903000, episode_reward=1.49 +/- 3.18
Episode length: 264.00 +/- 49.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 264      |
|    mean_reward     | 1.49     |
| time/              |          |
|    total_timesteps | 903000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 441    |
|    time_elapsed    | 1469   |
|    total_timesteps | 903168 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=904000, episode_reward=1.45 +/- 3.03
Episode length: 251.60 +/- 59.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 252         |
|    mean_reward          | 1.45        |
| time/                   |             |
|    total_timesteps      | 904000      |
| train/                  |             |
|    approx_kl            | 0.004442298 |
|    clip_fraction        | 0.0323      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.03       |
|    explained_variance   | 0.936       |
|    learning_rate        | 4.08e-05    |
|    loss                 | -0.0195     |
|    n_updates            | 4410        |
|    policy_gradient_loss | -0.00672    |
|    std                  | 0.67        |
|    value_loss           | 0.00668     |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=905000, episode_reward=3.15 +/- 2.84
Episode length: 244.00 +/- 49.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 244      |
|    mean_reward     | 3.15     |
| time/              |          |
|    total_timesteps | 905000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 442    |
|    time_elapsed    | 1472   |
|    total_timesteps | 905216 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=906000, episode_reward=1.61 +/- 2.84
Episode length: 252.60 +/- 58.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 253          |
|    mean_reward          | 1.61         |
| time/                   |              |
|    total_timesteps      | 906000       |
| train/                  |              |
|    approx_kl            | 0.0038708672 |
|    clip_fraction        | 0.0224       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.03        |
|    explained_variance   | 0.958        |
|    learning_rate        | 4.09e-05     |
|    loss                 | -0.0109      |
|    n_updates            | 4420         |
|    policy_gradient_loss | -0.00359     |
|    std                  | 0.669        |
|    value_loss           | 0.00902      |
------------------------------------------
box reached target
Eval num_timesteps=907000, episode_reward=-0.03 +/- 0.49
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.0317  |
| time/              |          |
|    total_timesteps | 907000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 443    |
|    time_elapsed    | 1476   |
|    total_timesteps | 907264 |
-------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=908000, episode_reward=1.56 +/- 3.01
Episode length: 248.80 +/- 63.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 249          |
|    mean_reward          | 1.56         |
| time/                   |              |
|    total_timesteps      | 908000       |
| train/                  |              |
|    approx_kl            | 0.0046442067 |
|    clip_fraction        | 0.0268       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.03        |
|    explained_variance   | 0.943        |
|    learning_rate        | 4.09e-05     |
|    loss                 | 0.000698     |
|    n_updates            | 4430         |
|    policy_gradient_loss | -0.00465     |
|    std                  | 0.666        |
|    value_loss           | 0.052        |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=909000, episode_reward=1.62 +/- 3.21
Episode length: 270.60 +/- 41.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 271      |
|    mean_reward     | 1.62     |
| time/              |          |
|    total_timesteps | 909000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 444    |
|    time_elapsed    | 1479   |
|    total_timesteps | 909312 |
-------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=910000, episode_reward=2.83 +/- 3.13
Episode length: 235.00 +/- 59.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 235         |
|    mean_reward          | 2.83        |
| time/                   |             |
|    total_timesteps      | 910000      |
| train/                  |             |
|    approx_kl            | 0.004405694 |
|    clip_fraction        | 0.024       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.02       |
|    explained_variance   | 0.959       |
|    learning_rate        | 4.09e-05    |
|    loss                 | -0.00091    |
|    n_updates            | 4440        |
|    policy_gradient_loss | -0.00267    |
|    std                  | 0.663       |
|    value_loss           | 0.0233      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=911000, episode_reward=2.88 +/- 3.17
Episode length: 243.00 +/- 54.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 243      |
|    mean_reward     | 2.88     |
| time/              |          |
|    total_timesteps | 911000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 445    |
|    time_elapsed    | 1482   |
|    total_timesteps | 911360 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=912000, episode_reward=2.06 +/- 2.70
Episode length: 248.20 +/- 63.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 248          |
|    mean_reward          | 2.06         |
| time/                   |              |
|    total_timesteps      | 912000       |
| train/                  |              |
|    approx_kl            | 0.0036728326 |
|    clip_fraction        | 0.0269       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.01        |
|    explained_variance   | 0.965        |
|    learning_rate        | 4.09e-05     |
|    loss                 | -0.0123      |
|    n_updates            | 4450         |
|    policy_gradient_loss | -0.00437     |
|    std                  | 0.661        |
|    value_loss           | 0.0164       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=913000, episode_reward=0.25 +/- 2.51
Episode length: 278.80 +/- 42.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 279      |
|    mean_reward     | 0.253    |
| time/              |          |
|    total_timesteps | 913000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 446    |
|    time_elapsed    | 1485   |
|    total_timesteps | 913408 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=914000, episode_reward=0.53 +/- 2.37
Episode length: 277.00 +/- 46.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 277          |
|    mean_reward          | 0.528        |
| time/                   |              |
|    total_timesteps      | 914000       |
| train/                  |              |
|    approx_kl            | 0.0054065753 |
|    clip_fraction        | 0.0417       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.01        |
|    explained_variance   | 0.979        |
|    learning_rate        | 4.1e-05      |
|    loss                 | -0.00525     |
|    n_updates            | 4460         |
|    policy_gradient_loss | -0.00478     |
|    std                  | 0.66         |
|    value_loss           | 0.0143       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=915000, episode_reward=0.74 +/- 2.32
Episode length: 268.00 +/- 64.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 268      |
|    mean_reward     | 0.743    |
| time/              |          |
|    total_timesteps | 915000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 447    |
|    time_elapsed    | 1489   |
|    total_timesteps | 915456 |
-------------------------------
Eval num_timesteps=916000, episode_reward=-0.49 +/- 0.71
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.491      |
| time/                   |             |
|    total_timesteps      | 916000      |
| train/                  |             |
|    approx_kl            | 0.004646735 |
|    clip_fraction        | 0.0294      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.01       |
|    explained_variance   | 0.907       |
|    learning_rate        | 4.1e-05     |
|    loss                 | 0.0153      |
|    n_updates            | 4470        |
|    policy_gradient_loss | -0.00489    |
|    std                  | 0.66        |
|    value_loss           | 0.0535      |
-----------------------------------------
box reached target
Eval num_timesteps=917000, episode_reward=-0.02 +/- 0.78
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.0233  |
| time/              |          |
|    total_timesteps | 917000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 448    |
|    time_elapsed    | 1492   |
|    total_timesteps | 917504 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=918000, episode_reward=1.49 +/- 3.05
Episode length: 239.40 +/- 74.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 239          |
|    mean_reward          | 1.49         |
| time/                   |              |
|    total_timesteps      | 918000       |
| train/                  |              |
|    approx_kl            | 0.0036330128 |
|    clip_fraction        | 0.0134       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2           |
|    explained_variance   | 0.794        |
|    learning_rate        | 4.1e-05      |
|    loss                 | -0.0147      |
|    n_updates            | 4480         |
|    policy_gradient_loss | -0.00329     |
|    std                  | 0.659        |
|    value_loss           | 0.0321       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=919000, episode_reward=2.93 +/- 3.21
Episode length: 251.00 +/- 53.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | 2.93     |
| time/              |          |
|    total_timesteps | 919000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 449    |
|    time_elapsed    | 1495   |
|    total_timesteps | 919552 |
-------------------------------
Eval num_timesteps=920000, episode_reward=-0.72 +/- 0.55
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.724      |
| time/                   |             |
|    total_timesteps      | 920000      |
| train/                  |             |
|    approx_kl            | 0.006183031 |
|    clip_fraction        | 0.0409      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2          |
|    explained_variance   | 0.769       |
|    learning_rate        | 4.1e-05     |
|    loss                 | 0.0554      |
|    n_updates            | 4490        |
|    policy_gradient_loss | -0.00352    |
|    std                  | 0.658       |
|    value_loss           | 0.0748      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=921000, episode_reward=1.57 +/- 2.94
Episode length: 243.80 +/- 69.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 244      |
|    mean_reward     | 1.57     |
| time/              |          |
|    total_timesteps | 921000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 450    |
|    time_elapsed    | 1498   |
|    total_timesteps | 921600 |
-------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=922000, episode_reward=2.74 +/- 3.06
Episode length: 217.20 +/- 68.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 217         |
|    mean_reward          | 2.74        |
| time/                   |             |
|    total_timesteps      | 922000      |
| train/                  |             |
|    approx_kl            | 0.005640493 |
|    clip_fraction        | 0.0272      |
|    clip_range           | 0.2         |
|    entropy_loss         | -2          |
|    explained_variance   | 0.955       |
|    learning_rate        | 4.11e-05    |
|    loss                 | -0.000936   |
|    n_updates            | 4500        |
|    policy_gradient_loss | -0.00582    |
|    std                  | 0.659       |
|    value_loss           | 0.0138      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=923000, episode_reward=4.35 +/- 1.90
Episode length: 210.40 +/- 49.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | 4.35     |
| time/              |          |
|    total_timesteps | 923000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 451    |
|    time_elapsed    | 1502   |
|    total_timesteps | 923648 |
-------------------------------
Eval num_timesteps=924000, episode_reward=-0.74 +/- 0.34
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.744       |
| time/                   |              |
|    total_timesteps      | 924000       |
| train/                  |              |
|    approx_kl            | 0.0050561572 |
|    clip_fraction        | 0.0303       |
|    clip_range           | 0.2          |
|    entropy_loss         | -2           |
|    explained_variance   | 0.945        |
|    learning_rate        | 4.11e-05     |
|    loss                 | 0.00787      |
|    n_updates            | 4510         |
|    policy_gradient_loss | -0.00615     |
|    std                  | 0.658        |
|    value_loss           | 0.03         |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=925000, episode_reward=2.76 +/- 3.07
Episode length: 220.60 +/- 66.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 221      |
|    mean_reward     | 2.76     |
| time/              |          |
|    total_timesteps | 925000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 452    |
|    time_elapsed    | 1505   |
|    total_timesteps | 925696 |
-------------------------------
Eval num_timesteps=926000, episode_reward=-0.19 +/- 0.72
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.194      |
| time/                   |             |
|    total_timesteps      | 926000      |
| train/                  |             |
|    approx_kl            | 0.003134726 |
|    clip_fraction        | 0.0202      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.99       |
|    explained_variance   | 0.988       |
|    learning_rate        | 4.11e-05    |
|    loss                 | -0.0121     |
|    n_updates            | 4520        |
|    policy_gradient_loss | -0.00335    |
|    std                  | 0.655       |
|    value_loss           | 0.00825     |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=927000, episode_reward=0.40 +/- 2.37
Episode length: 281.80 +/- 36.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 282      |
|    mean_reward     | 0.403    |
| time/              |          |
|    total_timesteps | 927000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 453    |
|    time_elapsed    | 1508   |
|    total_timesteps | 927744 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=928000, episode_reward=1.88 +/- 2.90
Episode length: 252.60 +/- 58.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 253          |
|    mean_reward          | 1.88         |
| time/                   |              |
|    total_timesteps      | 928000       |
| train/                  |              |
|    approx_kl            | 0.0051450925 |
|    clip_fraction        | 0.0333       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.99        |
|    explained_variance   | 0.803        |
|    learning_rate        | 4.11e-05     |
|    loss                 | -0.00539     |
|    n_updates            | 4530         |
|    policy_gradient_loss | -0.0068      |
|    std                  | 0.656        |
|    value_loss           | 0.0445       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=929000, episode_reward=0.79 +/- 2.37
Episode length: 273.60 +/- 52.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 274      |
|    mean_reward     | 0.789    |
| time/              |          |
|    total_timesteps | 929000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 454    |
|    time_elapsed    | 1511   |
|    total_timesteps | 929792 |
-------------------------------
Eval num_timesteps=930000, episode_reward=-0.70 +/- 0.42
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.703       |
| time/                   |              |
|    total_timesteps      | 930000       |
| train/                  |              |
|    approx_kl            | 0.0059452862 |
|    clip_fraction        | 0.0542       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.99        |
|    explained_variance   | 0.975        |
|    learning_rate        | 4.12e-05     |
|    loss                 | -0.00205     |
|    n_updates            | 4540         |
|    policy_gradient_loss | -0.00695     |
|    std                  | 0.656        |
|    value_loss           | 0.00883      |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=931000, episode_reward=2.86 +/- 2.94
Episode length: 228.20 +/- 59.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 228      |
|    mean_reward     | 2.86     |
| time/              |          |
|    total_timesteps | 931000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 615    |
|    iterations      | 455    |
|    time_elapsed    | 1515   |
|    total_timesteps | 931840 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=932000, episode_reward=1.62 +/- 3.21
Episode length: 269.20 +/- 44.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 269         |
|    mean_reward          | 1.62        |
| time/                   |             |
|    total_timesteps      | 932000      |
| train/                  |             |
|    approx_kl            | 0.005233891 |
|    clip_fraction        | 0.0387      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.99       |
|    explained_variance   | 0.966       |
|    learning_rate        | 4.12e-05    |
|    loss                 | 0.00637     |
|    n_updates            | 4550        |
|    policy_gradient_loss | -0.00387    |
|    std                  | 0.655       |
|    value_loss           | 0.0157      |
-----------------------------------------
box reached target
Eval num_timesteps=933000, episode_reward=0.56 +/- 2.44
Episode length: 279.80 +/- 40.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 280      |
|    mean_reward     | 0.561    |
| time/              |          |
|    total_timesteps | 933000   |
---------------------------------
box reached target
box reached target
-------------------------------
| time/              |        |
|    fps             | 615    |
|    iterations      | 456    |
|    time_elapsed    | 1518   |
|    total_timesteps | 933888 |
-------------------------------
box reached target
Eval num_timesteps=934000, episode_reward=0.50 +/- 2.39
Episode length: 276.00 +/- 48.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 276         |
|    mean_reward          | 0.501       |
| time/                   |             |
|    total_timesteps      | 934000      |
| train/                  |             |
|    approx_kl            | 0.003649979 |
|    clip_fraction        | 0.0168      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.99       |
|    explained_variance   | 0.928       |
|    learning_rate        | 4.12e-05    |
|    loss                 | -0.00437    |
|    n_updates            | 4560        |
|    policy_gradient_loss | -0.00246    |
|    std                  | 0.654       |
|    value_loss           | 0.0258      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=935000, episode_reward=0.51 +/- 2.56
Episode length: 282.00 +/- 36.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 282      |
|    mean_reward     | 0.505    |
| time/              |          |
|    total_timesteps | 935000   |
---------------------------------
box reached target
box reached target
-------------------------------
| time/              |        |
|    fps             | 615    |
|    iterations      | 457    |
|    time_elapsed    | 1521   |
|    total_timesteps | 935936 |
-------------------------------
box reached target
Eval num_timesteps=936000, episode_reward=0.52 +/- 2.44
Episode length: 282.00 +/- 36.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 282          |
|    mean_reward          | 0.521        |
| time/                   |              |
|    total_timesteps      | 936000       |
| train/                  |              |
|    approx_kl            | 0.0039703473 |
|    clip_fraction        | 0.0206       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.98        |
|    explained_variance   | 0.853        |
|    learning_rate        | 4.12e-05     |
|    loss                 | 0.0353       |
|    n_updates            | 4570         |
|    policy_gradient_loss | -0.00424     |
|    std                  | 0.653        |
|    value_loss           | 0.0626       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=937000, episode_reward=3.00 +/- 2.75
Episode length: 242.40 +/- 48.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 242      |
|    mean_reward     | 3        |
| time/              |          |
|    total_timesteps | 937000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 615    |
|    iterations      | 458    |
|    time_elapsed    | 1525   |
|    total_timesteps | 937984 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=938000, episode_reward=1.69 +/- 2.83
Episode length: 255.80 +/- 54.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 256          |
|    mean_reward          | 1.69         |
| time/                   |              |
|    total_timesteps      | 938000       |
| train/                  |              |
|    approx_kl            | 0.0031736786 |
|    clip_fraction        | 0.0133       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.98        |
|    explained_variance   | 0.86         |
|    learning_rate        | 4.13e-05     |
|    loss                 | 0.0108       |
|    n_updates            | 4580         |
|    policy_gradient_loss | -0.00201     |
|    std                  | 0.652        |
|    value_loss           | 0.0929       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=939000, episode_reward=1.57 +/- 3.07
Episode length: 260.00 +/- 50.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 260      |
|    mean_reward     | 1.57     |
| time/              |          |
|    total_timesteps | 939000   |
---------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=940000, episode_reward=3.98 +/- 2.49
Episode length: 201.40 +/- 53.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 201      |
|    mean_reward     | 3.98     |
| time/              |          |
|    total_timesteps | 940000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 459    |
|    time_elapsed    | 1528   |
|    total_timesteps | 940032 |
-------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=941000, episode_reward=3.12 +/- 2.91
Episode length: 234.20 +/- 54.48
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 234        |
|    mean_reward          | 3.12       |
| time/                   |            |
|    total_timesteps      | 941000     |
| train/                  |            |
|    approx_kl            | 0.00501745 |
|    clip_fraction        | 0.0345     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.98      |
|    explained_variance   | 0.769      |
|    learning_rate        | 4.13e-05   |
|    loss                 | 0.0132     |
|    n_updates            | 4590       |
|    policy_gradient_loss | -0.00371   |
|    std                  | 0.652      |
|    value_loss           | 0.0172     |
----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=942000, episode_reward=2.99 +/- 2.67
Episode length: 233.20 +/- 54.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 233      |
|    mean_reward     | 2.99     |
| time/              |          |
|    total_timesteps | 942000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 460    |
|    time_elapsed    | 1531   |
|    total_timesteps | 942080 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=943000, episode_reward=1.55 +/- 3.12
Episode length: 259.00 +/- 51.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 259          |
|    mean_reward          | 1.55         |
| time/                   |              |
|    total_timesteps      | 943000       |
| train/                  |              |
|    approx_kl            | 0.0039564315 |
|    clip_fraction        | 0.022        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.98        |
|    explained_variance   | 0.741        |
|    learning_rate        | 4.13e-05     |
|    loss                 | 0.0341       |
|    n_updates            | 4600         |
|    policy_gradient_loss | -0.00376     |
|    std                  | 0.649        |
|    value_loss           | 0.044        |
------------------------------------------
box reached target
Eval num_timesteps=944000, episode_reward=0.84 +/- 2.36
Episode length: 273.20 +/- 53.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 273      |
|    mean_reward     | 0.843    |
| time/              |          |
|    total_timesteps | 944000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 461    |
|    time_elapsed    | 1536   |
|    total_timesteps | 944128 |
-------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=945000, episode_reward=1.66 +/- 2.94
Episode length: 243.60 +/- 69.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 244         |
|    mean_reward          | 1.66        |
| time/                   |             |
|    total_timesteps      | 945000      |
| train/                  |             |
|    approx_kl            | 0.004736835 |
|    clip_fraction        | 0.0229      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.97       |
|    explained_variance   | 0.951       |
|    learning_rate        | 4.13e-05    |
|    loss                 | 0.0157      |
|    n_updates            | 4610        |
|    policy_gradient_loss | -0.00355    |
|    std                  | 0.65        |
|    value_loss           | 0.0126      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=946000, episode_reward=-0.77 +/- 0.48
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.769   |
| time/              |          |
|    total_timesteps | 946000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 462    |
|    time_elapsed    | 1539   |
|    total_timesteps | 946176 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=947000, episode_reward=0.25 +/- 2.49
Episode length: 275.60 +/- 48.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 276         |
|    mean_reward          | 0.246       |
| time/                   |             |
|    total_timesteps      | 947000      |
| train/                  |             |
|    approx_kl            | 0.004885318 |
|    clip_fraction        | 0.0303      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.97       |
|    explained_variance   | 0.985       |
|    learning_rate        | 4.14e-05    |
|    loss                 | 0.0129      |
|    n_updates            | 4620        |
|    policy_gradient_loss | -0.00472    |
|    std                  | 0.648       |
|    value_loss           | 0.0111      |
-----------------------------------------
box reached target
Eval num_timesteps=948000, episode_reward=0.27 +/- 2.55
Episode length: 288.60 +/- 22.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 289      |
|    mean_reward     | 0.275    |
| time/              |          |
|    total_timesteps | 948000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 463    |
|    time_elapsed    | 1543   |
|    total_timesteps | 948224 |
-------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=949000, episode_reward=4.11 +/- 2.57
Episode length: 215.00 +/- 46.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 215         |
|    mean_reward          | 4.11        |
| time/                   |             |
|    total_timesteps      | 949000      |
| train/                  |             |
|    approx_kl            | 0.004268976 |
|    clip_fraction        | 0.0285      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.97       |
|    explained_variance   | 0.96        |
|    learning_rate        | 4.14e-05    |
|    loss                 | -0.0165     |
|    n_updates            | 4630        |
|    policy_gradient_loss | -0.00555    |
|    std                  | 0.648       |
|    value_loss           | 0.00398     |
-----------------------------------------
box reached target
Eval num_timesteps=950000, episode_reward=0.56 +/- 2.41
Episode length: 269.60 +/- 60.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 270      |
|    mean_reward     | 0.562    |
| time/              |          |
|    total_timesteps | 950000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 464    |
|    time_elapsed    | 1546   |
|    total_timesteps | 950272 |
-------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=951000, episode_reward=0.40 +/- 2.54
Episode length: 285.60 +/- 28.80
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 286        |
|    mean_reward          | 0.401      |
| time/                   |            |
|    total_timesteps      | 951000     |
| train/                  |            |
|    approx_kl            | 0.00278922 |
|    clip_fraction        | 0.00801    |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.97      |
|    explained_variance   | 0.884      |
|    learning_rate        | 4.14e-05   |
|    loss                 | 0.00593    |
|    n_updates            | 4640       |
|    policy_gradient_loss | -0.00278   |
|    std                  | 0.646      |
|    value_loss           | 0.0346     |
----------------------------------------
Eval num_timesteps=952000, episode_reward=-0.56 +/- 0.75
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.557   |
| time/              |          |
|    total_timesteps | 952000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 465    |
|    time_elapsed    | 1549   |
|    total_timesteps | 952320 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=953000, episode_reward=-0.56 +/- 0.55
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.557       |
| time/                   |              |
|    total_timesteps      | 953000       |
| train/                  |              |
|    approx_kl            | 0.0055863685 |
|    clip_fraction        | 0.023        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.96        |
|    explained_variance   | 0.936        |
|    learning_rate        | 4.14e-05     |
|    loss                 | 0.00386      |
|    n_updates            | 4650         |
|    policy_gradient_loss | -0.00404     |
|    std                  | 0.647        |
|    value_loss           | 0.0355       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=954000, episode_reward=2.79 +/- 3.08
Episode length: 223.00 +/- 63.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 223      |
|    mean_reward     | 2.79     |
| time/              |          |
|    total_timesteps | 954000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 466    |
|    time_elapsed    | 1553   |
|    total_timesteps | 954368 |
-------------------------------
box reached target
Eval num_timesteps=955000, episode_reward=-0.82 +/- 0.42
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.82        |
| time/                   |              |
|    total_timesteps      | 955000       |
| train/                  |              |
|    approx_kl            | 0.0038545453 |
|    clip_fraction        | 0.0257       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.96        |
|    explained_variance   | 0.952        |
|    learning_rate        | 4.15e-05     |
|    loss                 | -0.00358     |
|    n_updates            | 4660         |
|    policy_gradient_loss | -0.00455     |
|    std                  | 0.646        |
|    value_loss           | 0.039        |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=956000, episode_reward=0.49 +/- 2.43
Episode length: 272.60 +/- 54.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 273      |
|    mean_reward     | 0.485    |
| time/              |          |
|    total_timesteps | 956000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 467    |
|    time_elapsed    | 1556   |
|    total_timesteps | 956416 |
-------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=957000, episode_reward=1.78 +/- 2.84
Episode length: 246.40 +/- 66.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 246          |
|    mean_reward          | 1.78         |
| time/                   |              |
|    total_timesteps      | 957000       |
| train/                  |              |
|    approx_kl            | 0.0054343687 |
|    clip_fraction        | 0.0317       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.96        |
|    explained_variance   | 0.975        |
|    learning_rate        | 4.15e-05     |
|    loss                 | -0.00886     |
|    n_updates            | 4670         |
|    policy_gradient_loss | -0.00637     |
|    std                  | 0.645        |
|    value_loss           | 0.00654      |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=958000, episode_reward=1.52 +/- 3.08
Episode length: 245.80 +/- 66.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 246      |
|    mean_reward     | 1.52     |
| time/              |          |
|    total_timesteps | 958000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 468    |
|    time_elapsed    | 1559   |
|    total_timesteps | 958464 |
-------------------------------
box reached target
Eval num_timesteps=959000, episode_reward=0.19 +/- 2.46
Episode length: 277.40 +/- 45.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 277          |
|    mean_reward          | 0.195        |
| time/                   |              |
|    total_timesteps      | 959000       |
| train/                  |              |
|    approx_kl            | 0.0038933689 |
|    clip_fraction        | 0.0173       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.96        |
|    explained_variance   | 0.949        |
|    learning_rate        | 4.15e-05     |
|    loss                 | 0.00804      |
|    n_updates            | 4680         |
|    policy_gradient_loss | -0.00212     |
|    std                  | 0.645        |
|    value_loss           | 0.022        |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=960000, episode_reward=2.91 +/- 2.77
Episode length: 222.20 +/- 64.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 222      |
|    mean_reward     | 2.91     |
| time/              |          |
|    total_timesteps | 960000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 469    |
|    time_elapsed    | 1562   |
|    total_timesteps | 960512 |
-------------------------------
Eval num_timesteps=961000, episode_reward=-0.99 +/- 0.03
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.987      |
| time/                   |             |
|    total_timesteps      | 961000      |
| train/                  |             |
|    approx_kl            | 0.005095281 |
|    clip_fraction        | 0.0375      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.96       |
|    explained_variance   | 0.889       |
|    learning_rate        | 4.15e-05    |
|    loss                 | -0.0254     |
|    n_updates            | 4690        |
|    policy_gradient_loss | -0.00399    |
|    std                  | 0.646       |
|    value_loss           | 0.0251      |
-----------------------------------------
box reached target
Eval num_timesteps=962000, episode_reward=0.54 +/- 2.37
Episode length: 281.20 +/- 37.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 281      |
|    mean_reward     | 0.542    |
| time/              |          |
|    total_timesteps | 962000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 470    |
|    time_elapsed    | 1566   |
|    total_timesteps | 962560 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=963000, episode_reward=0.67 +/- 2.33
Episode length: 271.00 +/- 58.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 271        |
|    mean_reward          | 0.665      |
| time/                   |            |
|    total_timesteps      | 963000     |
| train/                  |            |
|    approx_kl            | 0.00487576 |
|    clip_fraction        | 0.0301     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.96      |
|    explained_variance   | 0.945      |
|    learning_rate        | 4.16e-05   |
|    loss                 | 0.00125    |
|    n_updates            | 4700       |
|    policy_gradient_loss | -0.00395   |
|    std                  | 0.646      |
|    value_loss           | 0.0115     |
----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=964000, episode_reward=1.59 +/- 3.01
Episode length: 260.40 +/- 48.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 260      |
|    mean_reward     | 1.59     |
| time/              |          |
|    total_timesteps | 964000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 471    |
|    time_elapsed    | 1569   |
|    total_timesteps | 964608 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=965000, episode_reward=1.74 +/- 2.87
Episode length: 238.00 +/- 76.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 238          |
|    mean_reward          | 1.74         |
| time/                   |              |
|    total_timesteps      | 965000       |
| train/                  |              |
|    approx_kl            | 0.0031723357 |
|    clip_fraction        | 0.0111       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.96        |
|    explained_variance   | 0.875        |
|    learning_rate        | 4.16e-05     |
|    loss                 | 0.00556      |
|    n_updates            | 4710         |
|    policy_gradient_loss | -0.00329     |
|    std                  | 0.644        |
|    value_loss           | 0.067        |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=966000, episode_reward=1.65 +/- 2.99
Episode length: 244.80 +/- 68.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 245      |
|    mean_reward     | 1.65     |
| time/              |          |
|    total_timesteps | 966000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 472    |
|    time_elapsed    | 1572   |
|    total_timesteps | 966656 |
-------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=967000, episode_reward=3.04 +/- 2.70
Episode length: 229.60 +/- 57.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 230          |
|    mean_reward          | 3.04         |
| time/                   |              |
|    total_timesteps      | 967000       |
| train/                  |              |
|    approx_kl            | 0.0044704257 |
|    clip_fraction        | 0.0459       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.96        |
|    explained_variance   | 0.837        |
|    learning_rate        | 4.16e-05     |
|    loss                 | -0.0125      |
|    n_updates            | 4720         |
|    policy_gradient_loss | -0.00699     |
|    std                  | 0.645        |
|    value_loss           | 0.0399       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=968000, episode_reward=1.54 +/- 2.98
Episode length: 246.20 +/- 67.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 246      |
|    mean_reward     | 1.54     |
| time/              |          |
|    total_timesteps | 968000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 473    |
|    time_elapsed    | 1575   |
|    total_timesteps | 968704 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=969000, episode_reward=2.05 +/- 2.61
Episode length: 252.40 +/- 58.79
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 252        |
|    mean_reward          | 2.05       |
| time/                   |            |
|    total_timesteps      | 969000     |
| train/                  |            |
|    approx_kl            | 0.00306383 |
|    clip_fraction        | 0.0138     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.96      |
|    explained_variance   | 0.937      |
|    learning_rate        | 4.16e-05   |
|    loss                 | 0.00531    |
|    n_updates            | 4730       |
|    policy_gradient_loss | -0.00167   |
|    std                  | 0.645      |
|    value_loss           | 0.0634     |
----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=970000, episode_reward=0.22 +/- 2.44
Episode length: 272.80 +/- 54.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 273      |
|    mean_reward     | 0.218    |
| time/              |          |
|    total_timesteps | 970000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 474    |
|    time_elapsed    | 1579   |
|    total_timesteps | 970752 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=971000, episode_reward=2.86 +/- 3.16
Episode length: 238.80 +/- 67.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 239          |
|    mean_reward          | 2.86         |
| time/                   |              |
|    total_timesteps      | 971000       |
| train/                  |              |
|    approx_kl            | 0.0053102886 |
|    clip_fraction        | 0.0383       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.96        |
|    explained_variance   | 0.953        |
|    learning_rate        | 4.16e-05     |
|    loss                 | -0.0244      |
|    n_updates            | 4740         |
|    policy_gradient_loss | -0.0022      |
|    std                  | 0.644        |
|    value_loss           | 0.0426       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=972000, episode_reward=0.87 +/- 2.30
Episode length: 273.40 +/- 53.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 273      |
|    mean_reward     | 0.874    |
| time/              |          |
|    total_timesteps | 972000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 475    |
|    time_elapsed    | 1582   |
|    total_timesteps | 972800 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=973000, episode_reward=1.66 +/- 2.98
Episode length: 247.40 +/- 65.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 247          |
|    mean_reward          | 1.66         |
| time/                   |              |
|    total_timesteps      | 973000       |
| train/                  |              |
|    approx_kl            | 0.0045536757 |
|    clip_fraction        | 0.0333       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.96        |
|    explained_variance   | 0.774        |
|    learning_rate        | 4.17e-05     |
|    loss                 | 0.00024      |
|    n_updates            | 4750         |
|    policy_gradient_loss | -0.0047      |
|    std                  | 0.643        |
|    value_loss           | 0.0591       |
------------------------------------------
box reached target
Eval num_timesteps=974000, episode_reward=0.33 +/- 2.50
Episode length: 274.40 +/- 51.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 274      |
|    mean_reward     | 0.328    |
| time/              |          |
|    total_timesteps | 974000   |
---------------------------------
box reached target
box reached target
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 476    |
|    time_elapsed    | 1585   |
|    total_timesteps | 974848 |
-------------------------------
box reached target
Eval num_timesteps=975000, episode_reward=0.22 +/- 2.44
Episode length: 273.60 +/- 52.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 274          |
|    mean_reward          | 0.219        |
| time/                   |              |
|    total_timesteps      | 975000       |
| train/                  |              |
|    approx_kl            | 0.0040747467 |
|    clip_fraction        | 0.0229       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.95        |
|    explained_variance   | 0.957        |
|    learning_rate        | 4.17e-05     |
|    loss                 | 0.00223      |
|    n_updates            | 4760         |
|    policy_gradient_loss | -0.00636     |
|    std                  | 0.642        |
|    value_loss           | 0.0158       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=976000, episode_reward=3.20 +/- 2.67
Episode length: 225.80 +/- 62.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 226      |
|    mean_reward     | 3.2      |
| time/              |          |
|    total_timesteps | 976000   |
---------------------------------
box reached target
box reached target
box reached target
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 477    |
|    time_elapsed    | 1588   |
|    total_timesteps | 976896 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=977000, episode_reward=2.08 +/- 2.77
Episode length: 256.40 +/- 53.77
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 256        |
|    mean_reward          | 2.08       |
| time/                   |            |
|    total_timesteps      | 977000     |
| train/                  |            |
|    approx_kl            | 0.00511822 |
|    clip_fraction        | 0.0368     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.95      |
|    explained_variance   | 0.913      |
|    learning_rate        | 4.17e-05   |
|    loss                 | 0.00163    |
|    n_updates            | 4770       |
|    policy_gradient_loss | -0.00576   |
|    std                  | 0.641      |
|    value_loss           | 0.0682     |
----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=978000, episode_reward=1.47 +/- 3.02
Episode length: 247.20 +/- 65.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 247      |
|    mean_reward     | 1.47     |
| time/              |          |
|    total_timesteps | 978000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 478    |
|    time_elapsed    | 1591   |
|    total_timesteps | 978944 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=979000, episode_reward=1.43 +/- 2.98
Episode length: 255.00 +/- 55.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 255          |
|    mean_reward          | 1.43         |
| time/                   |              |
|    total_timesteps      | 979000       |
| train/                  |              |
|    approx_kl            | 0.0037267562 |
|    clip_fraction        | 0.0195       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.95        |
|    explained_variance   | 0.836        |
|    learning_rate        | 4.17e-05     |
|    loss                 | 0.0257       |
|    n_updates            | 4780         |
|    policy_gradient_loss | -0.00331     |
|    std                  | 0.64         |
|    value_loss           | 0.0406       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=980000, episode_reward=2.69 +/- 3.02
Episode length: 224.20 +/- 62.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 224      |
|    mean_reward     | 2.69     |
| time/              |          |
|    total_timesteps | 980000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 615    |
|    iterations      | 479    |
|    time_elapsed    | 1595   |
|    total_timesteps | 980992 |
-------------------------------
Eval num_timesteps=981000, episode_reward=-0.50 +/- 0.61
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.504       |
| time/                   |              |
|    total_timesteps      | 981000       |
| train/                  |              |
|    approx_kl            | 0.0048558847 |
|    clip_fraction        | 0.0327       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.94        |
|    explained_variance   | 0.718        |
|    learning_rate        | 4.18e-05     |
|    loss                 | -0.00738     |
|    n_updates            | 4790         |
|    policy_gradient_loss | -0.00577     |
|    std                  | 0.639        |
|    value_loss           | 0.0177       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=982000, episode_reward=1.72 +/- 2.96
Episode length: 254.40 +/- 56.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 254      |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 982000   |
---------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=983000, episode_reward=1.54 +/- 3.11
Episode length: 251.40 +/- 61.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | 1.54     |
| time/              |          |
|    total_timesteps | 983000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 480    |
|    time_elapsed    | 1598   |
|    total_timesteps | 983040 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=984000, episode_reward=2.64 +/- 3.05
Episode length: 224.00 +/- 62.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 224         |
|    mean_reward          | 2.64        |
| time/                   |             |
|    total_timesteps      | 984000      |
| train/                  |             |
|    approx_kl            | 0.006274149 |
|    clip_fraction        | 0.0486      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.94       |
|    explained_variance   | 0.933       |
|    learning_rate        | 4.18e-05    |
|    loss                 | 0.0255      |
|    n_updates            | 4800        |
|    policy_gradient_loss | -0.005      |
|    std                  | 0.64        |
|    value_loss           | 0.0227      |
-----------------------------------------
Eval num_timesteps=985000, episode_reward=-0.44 +/- 0.69
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.443   |
| time/              |          |
|    total_timesteps | 985000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 481    |
|    time_elapsed    | 1602   |
|    total_timesteps | 985088 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=986000, episode_reward=1.77 +/- 2.86
Episode length: 256.40 +/- 53.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 256         |
|    mean_reward          | 1.77        |
| time/                   |             |
|    total_timesteps      | 986000      |
| train/                  |             |
|    approx_kl            | 0.005681294 |
|    clip_fraction        | 0.0595      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.94       |
|    explained_variance   | 0.834       |
|    learning_rate        | 4.18e-05    |
|    loss                 | 0.00406     |
|    n_updates            | 4810        |
|    policy_gradient_loss | -0.00487    |
|    std                  | 0.64        |
|    value_loss           | 0.0102      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=987000, episode_reward=2.68 +/- 3.01
Episode length: 225.20 +/- 62.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 225      |
|    mean_reward     | 2.68     |
| time/              |          |
|    total_timesteps | 987000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 482    |
|    time_elapsed    | 1605   |
|    total_timesteps | 987136 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=988000, episode_reward=2.82 +/- 3.11
Episode length: 233.80 +/- 57.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 234          |
|    mean_reward          | 2.82         |
| time/                   |              |
|    total_timesteps      | 988000       |
| train/                  |              |
|    approx_kl            | 0.0036632996 |
|    clip_fraction        | 0.0143       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.94        |
|    explained_variance   | 0.858        |
|    learning_rate        | 4.18e-05     |
|    loss                 | -0.00345     |
|    n_updates            | 4820         |
|    policy_gradient_loss | -0.00221     |
|    std                  | 0.64         |
|    value_loss           | 0.034        |
------------------------------------------
box reached target
Eval num_timesteps=989000, episode_reward=-0.64 +/- 0.48
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.645   |
| time/              |          |
|    total_timesteps | 989000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 483    |
|    time_elapsed    | 1608   |
|    total_timesteps | 989184 |
-------------------------------
Eval num_timesteps=990000, episode_reward=-0.08 +/- 0.75
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.0806      |
| time/                   |              |
|    total_timesteps      | 990000       |
| train/                  |              |
|    approx_kl            | 0.0060637854 |
|    clip_fraction        | 0.0561       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.95        |
|    explained_variance   | 0.954        |
|    learning_rate        | 4.19e-05     |
|    loss                 | 0.0347       |
|    n_updates            | 4830         |
|    policy_gradient_loss | -0.00467     |
|    std                  | 0.641        |
|    value_loss           | 0.0139       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=991000, episode_reward=1.44 +/- 2.99
Episode length: 247.00 +/- 64.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 247      |
|    mean_reward     | 1.44     |
| time/              |          |
|    total_timesteps | 991000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 484    |
|    time_elapsed    | 1611   |
|    total_timesteps | 991232 |
-------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=992000, episode_reward=1.16 +/- 2.04
Episode length: 270.80 +/- 58.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 271         |
|    mean_reward          | 1.16        |
| time/                   |             |
|    total_timesteps      | 992000      |
| train/                  |             |
|    approx_kl            | 0.003987083 |
|    clip_fraction        | 0.0293      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.95       |
|    explained_variance   | 0.945       |
|    learning_rate        | 4.19e-05    |
|    loss                 | -0.0248     |
|    n_updates            | 4840        |
|    policy_gradient_loss | -0.00663    |
|    std                  | 0.641       |
|    value_loss           | 0.00438     |
-----------------------------------------
box reached target
Eval num_timesteps=993000, episode_reward=0.46 +/- 2.39
Episode length: 279.40 +/- 41.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 279      |
|    mean_reward     | 0.46     |
| time/              |          |
|    total_timesteps | 993000   |
---------------------------------
box reached target
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 485    |
|    time_elapsed    | 1615   |
|    total_timesteps | 993280 |
-------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=994000, episode_reward=2.66 +/- 3.14
Episode length: 231.80 +/- 58.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 232          |
|    mean_reward          | 2.66         |
| time/                   |              |
|    total_timesteps      | 994000       |
| train/                  |              |
|    approx_kl            | 0.0038621798 |
|    clip_fraction        | 0.0219       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.95        |
|    explained_variance   | 0.916        |
|    learning_rate        | 4.19e-05     |
|    loss                 | -0.000626    |
|    n_updates            | 4850         |
|    policy_gradient_loss | -0.00176     |
|    std                  | 0.643        |
|    value_loss           | 0.0228       |
------------------------------------------
box reached target
Eval num_timesteps=995000, episode_reward=0.52 +/- 2.60
Episode length: 284.80 +/- 30.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 285      |
|    mean_reward     | 0.522    |
| time/              |          |
|    total_timesteps | 995000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 486    |
|    time_elapsed    | 1618   |
|    total_timesteps | 995328 |
-------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=996000, episode_reward=3.08 +/- 2.73
Episode length: 229.80 +/- 57.37
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 230        |
|    mean_reward          | 3.08       |
| time/                   |            |
|    total_timesteps      | 996000     |
| train/                  |            |
|    approx_kl            | 0.00294394 |
|    clip_fraction        | 0.0187     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.95      |
|    explained_variance   | 0.871      |
|    learning_rate        | 4.19e-05   |
|    loss                 | -0.0124    |
|    n_updates            | 4860       |
|    policy_gradient_loss | -0.00382   |
|    std                  | 0.642      |
|    value_loss           | 0.0257     |
----------------------------------------
Eval num_timesteps=997000, episode_reward=-0.66 +/- 0.68
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.66    |
| time/              |          |
|    total_timesteps | 997000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 487    |
|    time_elapsed    | 1621   |
|    total_timesteps | 997376 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=998000, episode_reward=0.57 +/- 2.51
Episode length: 279.60 +/- 40.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 280         |
|    mean_reward          | 0.569       |
| time/                   |             |
|    total_timesteps      | 998000      |
| train/                  |             |
|    approx_kl            | 0.003770141 |
|    clip_fraction        | 0.0311      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.95       |
|    explained_variance   | 0.977       |
|    learning_rate        | 4.2e-05     |
|    loss                 | -0.00803    |
|    n_updates            | 4870        |
|    policy_gradient_loss | -0.00368    |
|    std                  | 0.643       |
|    value_loss           | 0.00852     |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=999000, episode_reward=0.26 +/- 2.46
Episode length: 278.40 +/- 43.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | 0.257    |
| time/              |          |
|    total_timesteps | 999000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 614    |
|    iterations      | 488    |
|    time_elapsed    | 1625   |
|    total_timesteps | 999424 |
-------------------------------
box reached target
box reached target
Eval num_timesteps=1000000, episode_reward=1.98 +/- 2.73
Episode length: 250.80 +/- 60.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 251          |
|    mean_reward          | 1.98         |
| time/                   |              |
|    total_timesteps      | 1000000      |
| train/                  |              |
|    approx_kl            | 0.0033143498 |
|    clip_fraction        | 0.0062       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.95        |
|    explained_variance   | 0.814        |
|    learning_rate        | 4.2e-05      |
|    loss                 | 0.00238      |
|    n_updates            | 4880         |
|    policy_gradient_loss | -0.00163     |
|    std                  | 0.643        |
|    value_loss           | 0.0594       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=1001000, episode_reward=0.55 +/- 2.40
Episode length: 267.20 +/- 65.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 267      |
|    mean_reward     | 0.545    |
| time/              |          |
|    total_timesteps | 1001000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 615     |
|    iterations      | 489     |
|    time_elapsed    | 1628    |
|    total_timesteps | 1001472 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1002000, episode_reward=1.42 +/- 3.02
Episode length: 251.00 +/- 60.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 251          |
|    mean_reward          | 1.42         |
| time/                   |              |
|    total_timesteps      | 1002000      |
| train/                  |              |
|    approx_kl            | 0.0050786133 |
|    clip_fraction        | 0.0419       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.95        |
|    explained_variance   | 0.95         |
|    learning_rate        | 4.2e-05      |
|    loss                 | 0.0232       |
|    n_updates            | 4890         |
|    policy_gradient_loss | -0.00634     |
|    std                  | 0.643        |
|    value_loss           | 0.0194       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=1003000, episode_reward=1.34 +/- 1.98
Episode length: 275.00 +/- 50.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 275      |
|    mean_reward     | 1.34     |
| time/              |          |
|    total_timesteps | 1003000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 615     |
|    iterations      | 490     |
|    time_elapsed    | 1631    |
|    total_timesteps | 1003520 |
--------------------------------
box reached target
Eval num_timesteps=1004000, episode_reward=-0.40 +/- 0.73
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.404      |
| time/                   |             |
|    total_timesteps      | 1004000     |
| train/                  |             |
|    approx_kl            | 0.004417844 |
|    clip_fraction        | 0.0289      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.95       |
|    explained_variance   | 0.864       |
|    learning_rate        | 4.2e-05     |
|    loss                 | -0.0126     |
|    n_updates            | 4900        |
|    policy_gradient_loss | -0.00358    |
|    std                  | 0.643       |
|    value_loss           | 0.0456      |
-----------------------------------------
box reached target
Eval num_timesteps=1005000, episode_reward=-0.75 +/- 0.59
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.753   |
| time/              |          |
|    total_timesteps | 1005000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 615     |
|    iterations      | 491     |
|    time_elapsed    | 1635    |
|    total_timesteps | 1005568 |
--------------------------------
Eval num_timesteps=1006000, episode_reward=-0.23 +/- 0.84
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.23        |
| time/                   |              |
|    total_timesteps      | 1006000      |
| train/                  |              |
|    approx_kl            | 0.0027515609 |
|    clip_fraction        | 0.0135       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.95        |
|    explained_variance   | 0.951        |
|    learning_rate        | 4.21e-05     |
|    loss                 | -0.00809     |
|    n_updates            | 4910         |
|    policy_gradient_loss | -0.00274     |
|    std                  | 0.641        |
|    value_loss           | 0.0177       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1007000, episode_reward=3.10 +/- 2.69
Episode length: 227.20 +/- 62.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 227      |
|    mean_reward     | 3.1      |
| time/              |          |
|    total_timesteps | 1007000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 615     |
|    iterations      | 492     |
|    time_elapsed    | 1638    |
|    total_timesteps | 1007616 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1008000, episode_reward=1.72 +/- 2.91
Episode length: 237.00 +/- 77.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 237          |
|    mean_reward          | 1.72         |
| time/                   |              |
|    total_timesteps      | 1008000      |
| train/                  |              |
|    approx_kl            | 0.0065045543 |
|    clip_fraction        | 0.0471       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.95        |
|    explained_variance   | 0.959        |
|    learning_rate        | 4.21e-05     |
|    loss                 | 0.0239       |
|    n_updates            | 4920         |
|    policy_gradient_loss | -0.00516     |
|    std                  | 0.641        |
|    value_loss           | 0.0167       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1009000, episode_reward=1.42 +/- 2.96
Episode length: 255.00 +/- 55.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 255      |
|    mean_reward     | 1.42     |
| time/              |          |
|    total_timesteps | 1009000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 615     |
|    iterations      | 493     |
|    time_elapsed    | 1641    |
|    total_timesteps | 1009664 |
--------------------------------
Eval num_timesteps=1010000, episode_reward=-0.18 +/- 0.56
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.178      |
| time/                   |             |
|    total_timesteps      | 1010000     |
| train/                  |             |
|    approx_kl            | 0.005564907 |
|    clip_fraction        | 0.033       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.95       |
|    explained_variance   | 0.965       |
|    learning_rate        | 4.21e-05    |
|    loss                 | 0.0157      |
|    n_updates            | 4930        |
|    policy_gradient_loss | -0.00318    |
|    std                  | 0.643       |
|    value_loss           | 0.0156      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1011000, episode_reward=0.67 +/- 2.37
Episode length: 278.20 +/- 43.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | 0.669    |
| time/              |          |
|    total_timesteps | 1011000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 615     |
|    iterations      | 494     |
|    time_elapsed    | 1644    |
|    total_timesteps | 1011712 |
--------------------------------
box reached target
Eval num_timesteps=1012000, episode_reward=0.41 +/- 2.42
Episode length: 274.60 +/- 50.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 275         |
|    mean_reward          | 0.412       |
| time/                   |             |
|    total_timesteps      | 1012000     |
| train/                  |             |
|    approx_kl            | 0.003830339 |
|    clip_fraction        | 0.0154      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.95       |
|    explained_variance   | 0.843       |
|    learning_rate        | 4.21e-05    |
|    loss                 | -0.0083     |
|    n_updates            | 4940        |
|    policy_gradient_loss | -0.00562    |
|    std                  | 0.641       |
|    value_loss           | 0.0607      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1013000, episode_reward=0.52 +/- 2.38
Episode length: 269.00 +/- 62.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 269      |
|    mean_reward     | 0.515    |
| time/              |          |
|    total_timesteps | 1013000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 615     |
|    iterations      | 495     |
|    time_elapsed    | 1648    |
|    total_timesteps | 1013760 |
--------------------------------
box reached target
Eval num_timesteps=1014000, episode_reward=0.40 +/- 2.43
Episode length: 272.60 +/- 54.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 273          |
|    mean_reward          | 0.403        |
| time/                   |              |
|    total_timesteps      | 1014000      |
| train/                  |              |
|    approx_kl            | 0.0059061684 |
|    clip_fraction        | 0.0313       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.95        |
|    explained_variance   | 0.907        |
|    learning_rate        | 4.22e-05     |
|    loss                 | 0.0302       |
|    n_updates            | 4950         |
|    policy_gradient_loss | -0.0043      |
|    std                  | 0.641        |
|    value_loss           | 0.0611       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1015000, episode_reward=3.19 +/- 2.48
Episode length: 214.20 +/- 70.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 214      |
|    mean_reward     | 3.19     |
| time/              |          |
|    total_timesteps | 1015000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 615     |
|    iterations      | 496     |
|    time_elapsed    | 1651    |
|    total_timesteps | 1015808 |
--------------------------------
box reached target
Eval num_timesteps=1016000, episode_reward=-0.79 +/- 0.41
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.794       |
| time/                   |              |
|    total_timesteps      | 1016000      |
| train/                  |              |
|    approx_kl            | 0.0032308926 |
|    clip_fraction        | 0.0219       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.95        |
|    explained_variance   | 0.883        |
|    learning_rate        | 4.22e-05     |
|    loss                 | -0.00761     |
|    n_updates            | 4960         |
|    policy_gradient_loss | -0.00424     |
|    std                  | 0.643        |
|    value_loss           | 0.0161       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1017000, episode_reward=0.69 +/- 2.36
Episode length: 267.60 +/- 64.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 268      |
|    mean_reward     | 0.691    |
| time/              |          |
|    total_timesteps | 1017000  |
---------------------------------
box reached target
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 615     |
|    iterations      | 497     |
|    time_elapsed    | 1654    |
|    total_timesteps | 1017856 |
--------------------------------
Eval num_timesteps=1018000, episode_reward=-0.60 +/- 0.51
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.6         |
| time/                   |              |
|    total_timesteps      | 1018000      |
| train/                  |              |
|    approx_kl            | 0.0043105087 |
|    clip_fraction        | 0.0288       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.95        |
|    explained_variance   | 0.883        |
|    learning_rate        | 4.22e-05     |
|    loss                 | 0.0356       |
|    n_updates            | 4970         |
|    policy_gradient_loss | -0.00632     |
|    std                  | 0.643        |
|    value_loss           | 0.0875       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1019000, episode_reward=0.24 +/- 2.48
Episode length: 270.40 +/- 59.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 270      |
|    mean_reward     | 0.242    |
| time/              |          |
|    total_timesteps | 1019000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 615     |
|    iterations      | 498     |
|    time_elapsed    | 1658    |
|    total_timesteps | 1019904 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1020000, episode_reward=4.06 +/- 2.53
Episode length: 217.40 +/- 51.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 217          |
|    mean_reward          | 4.06         |
| time/                   |              |
|    total_timesteps      | 1020000      |
| train/                  |              |
|    approx_kl            | 0.0059406096 |
|    clip_fraction        | 0.0377       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.95        |
|    explained_variance   | 0.95         |
|    learning_rate        | 4.22e-05     |
|    loss                 | -0.00869     |
|    n_updates            | 4980         |
|    policy_gradient_loss | -0.00621     |
|    std                  | 0.64         |
|    value_loss           | 0.0259       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=1021000, episode_reward=1.54 +/- 3.18
Episode length: 253.60 +/- 58.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 254      |
|    mean_reward     | 1.54     |
| time/              |          |
|    total_timesteps | 1021000  |
---------------------------------
box reached target
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 615     |
|    iterations      | 499     |
|    time_elapsed    | 1661    |
|    total_timesteps | 1021952 |
--------------------------------
box reached target
Eval num_timesteps=1022000, episode_reward=0.67 +/- 2.51
Episode length: 286.80 +/- 26.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 287          |
|    mean_reward          | 0.667        |
| time/                   |              |
|    total_timesteps      | 1022000      |
| train/                  |              |
|    approx_kl            | 0.0049876906 |
|    clip_fraction        | 0.0378       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.94        |
|    explained_variance   | 0.977        |
|    learning_rate        | 4.23e-05     |
|    loss                 | 0.00796      |
|    n_updates            | 4990         |
|    policy_gradient_loss | -0.0057      |
|    std                  | 0.637        |
|    value_loss           | 0.019        |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1023000, episode_reward=1.99 +/- 2.61
Episode length: 245.40 +/- 67.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 245      |
|    mean_reward     | 1.99     |
| time/              |          |
|    total_timesteps | 1023000  |
---------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1024000, episode_reward=0.52 +/- 2.37
Episode length: 272.40 +/- 55.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 272      |
|    mean_reward     | 0.516    |
| time/              |          |
|    total_timesteps | 1024000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 614     |
|    iterations      | 500     |
|    time_elapsed    | 1665    |
|    total_timesteps | 1024000 |
--------------------------------
Eval num_timesteps=1025000, episode_reward=-0.27 +/- 0.48
Episode length: 300.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 300        |
|    mean_reward          | -0.275     |
| time/                   |            |
|    total_timesteps      | 1025000    |
| train/                  |            |
|    approx_kl            | 0.00501058 |
|    clip_fraction        | 0.024      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.93      |
|    explained_variance   | 0.978      |
|    learning_rate        | 4.23e-05   |
|    loss                 | 0.000319   |
|    n_updates            | 5000       |
|    policy_gradient_loss | -0.00448   |
|    std                  | 0.636      |
|    value_loss           | 0.0126     |
----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1026000, episode_reward=2.82 +/- 3.12
Episode length: 242.20 +/- 51.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 242      |
|    mean_reward     | 2.82     |
| time/              |          |
|    total_timesteps | 1026000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 614     |
|    iterations      | 501     |
|    time_elapsed    | 1668    |
|    total_timesteps | 1026048 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1027000, episode_reward=1.48 +/- 3.04
Episode length: 252.40 +/- 58.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 252          |
|    mean_reward          | 1.48         |
| time/                   |              |
|    total_timesteps      | 1027000      |
| train/                  |              |
|    approx_kl            | 0.0049857255 |
|    clip_fraction        | 0.0408       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.93        |
|    explained_variance   | 0.795        |
|    learning_rate        | 4.23e-05     |
|    loss                 | -0.0067      |
|    n_updates            | 5010         |
|    policy_gradient_loss | -0.00533     |
|    std                  | 0.636        |
|    value_loss           | 0.0231       |
------------------------------------------
box reached target
Eval num_timesteps=1028000, episode_reward=-0.61 +/- 0.61
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.61    |
| time/              |          |
|    total_timesteps | 1028000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 615     |
|    iterations      | 502     |
|    time_elapsed    | 1671    |
|    total_timesteps | 1028096 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1029000, episode_reward=1.82 +/- 2.69
Episode length: 249.80 +/- 62.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 250          |
|    mean_reward          | 1.82         |
| time/                   |              |
|    total_timesteps      | 1029000      |
| train/                  |              |
|    approx_kl            | 0.0037729703 |
|    clip_fraction        | 0.0227       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.93        |
|    explained_variance   | 0.895        |
|    learning_rate        | 4.23e-05     |
|    loss                 | 0.0239       |
|    n_updates            | 5020         |
|    policy_gradient_loss | -0.00457     |
|    std                  | 0.636        |
|    value_loss           | 0.0577       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=1030000, episode_reward=2.24 +/- 2.60
Episode length: 246.00 +/- 66.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 246      |
|    mean_reward     | 2.24     |
| time/              |          |
|    total_timesteps | 1030000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 615     |
|    iterations      | 503     |
|    time_elapsed    | 1674    |
|    total_timesteps | 1030144 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1031000, episode_reward=2.71 +/- 3.10
Episode length: 215.80 +/- 70.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 216          |
|    mean_reward          | 2.71         |
| time/                   |              |
|    total_timesteps      | 1031000      |
| train/                  |              |
|    approx_kl            | 0.0037837767 |
|    clip_fraction        | 0.0202       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.93        |
|    explained_variance   | 0.94         |
|    learning_rate        | 4.24e-05     |
|    loss                 | -0.00811     |
|    n_updates            | 5030         |
|    policy_gradient_loss | -0.00337     |
|    std                  | 0.635        |
|    value_loss           | 0.00756      |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1032000, episode_reward=3.19 +/- 2.82
Episode length: 227.20 +/- 62.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 227      |
|    mean_reward     | 3.19     |
| time/              |          |
|    total_timesteps | 1032000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 615     |
|    iterations      | 504     |
|    time_elapsed    | 1677    |
|    total_timesteps | 1032192 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1033000, episode_reward=2.80 +/- 3.10
Episode length: 223.60 +/- 65.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 224          |
|    mean_reward          | 2.8          |
| time/                   |              |
|    total_timesteps      | 1033000      |
| train/                  |              |
|    approx_kl            | 0.0039356397 |
|    clip_fraction        | 0.0198       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.93        |
|    explained_variance   | 0.945        |
|    learning_rate        | 4.24e-05     |
|    loss                 | 0.0317       |
|    n_updates            | 5040         |
|    policy_gradient_loss | -0.00385     |
|    std                  | 0.635        |
|    value_loss           | 0.0145       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1034000, episode_reward=2.35 +/- 2.62
Episode length: 266.80 +/- 41.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 267      |
|    mean_reward     | 2.35     |
| time/              |          |
|    total_timesteps | 1034000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 615     |
|    iterations      | 505     |
|    time_elapsed    | 1681    |
|    total_timesteps | 1034240 |
--------------------------------
box reached target
Eval num_timesteps=1035000, episode_reward=0.62 +/- 2.30
Episode length: 275.80 +/- 48.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 276          |
|    mean_reward          | 0.621        |
| time/                   |              |
|    total_timesteps      | 1035000      |
| train/                  |              |
|    approx_kl            | 0.0040301178 |
|    clip_fraction        | 0.0292       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.93        |
|    explained_variance   | 0.975        |
|    learning_rate        | 4.24e-05     |
|    loss                 | -0.0121      |
|    n_updates            | 5050         |
|    policy_gradient_loss | -0.00449     |
|    std                  | 0.636        |
|    value_loss           | 0.0117       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1036000, episode_reward=3.00 +/- 2.84
Episode length: 230.00 +/- 57.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 230      |
|    mean_reward     | 3        |
| time/              |          |
|    total_timesteps | 1036000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 615     |
|    iterations      | 506     |
|    time_elapsed    | 1684    |
|    total_timesteps | 1036288 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1037000, episode_reward=0.29 +/- 2.56
Episode length: 288.20 +/- 23.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 288          |
|    mean_reward          | 0.291        |
| time/                   |              |
|    total_timesteps      | 1037000      |
| train/                  |              |
|    approx_kl            | 0.0027295388 |
|    clip_fraction        | 0.0114       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.93        |
|    explained_variance   | 0.949        |
|    learning_rate        | 4.24e-05     |
|    loss                 | 0.00188      |
|    n_updates            | 5060         |
|    policy_gradient_loss | -0.00438     |
|    std                  | 0.638        |
|    value_loss           | 0.00989      |
------------------------------------------
box reached target
Eval num_timesteps=1038000, episode_reward=0.86 +/- 2.39
Episode length: 290.00 +/- 20.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 290      |
|    mean_reward     | 0.863    |
| time/              |          |
|    total_timesteps | 1038000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 615     |
|    iterations      | 507     |
|    time_elapsed    | 1687    |
|    total_timesteps | 1038336 |
--------------------------------
box reached target
Eval num_timesteps=1039000, episode_reward=0.26 +/- 2.54
Episode length: 283.60 +/- 32.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 284          |
|    mean_reward          | 0.258        |
| time/                   |              |
|    total_timesteps      | 1039000      |
| train/                  |              |
|    approx_kl            | 0.0032241726 |
|    clip_fraction        | 0.0268       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.94        |
|    explained_variance   | 0.911        |
|    learning_rate        | 4.25e-05     |
|    loss                 | -0.00819     |
|    n_updates            | 5070         |
|    policy_gradient_loss | -0.00539     |
|    std                  | 0.639        |
|    value_loss           | 0.0164       |
------------------------------------------
box reached target
Eval num_timesteps=1040000, episode_reward=0.32 +/- 2.57
Episode length: 283.00 +/- 34.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 283      |
|    mean_reward     | 0.317    |
| time/              |          |
|    total_timesteps | 1040000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 615     |
|    iterations      | 508     |
|    time_elapsed    | 1691    |
|    total_timesteps | 1040384 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1041000, episode_reward=3.14 +/- 2.76
Episode length: 233.80 +/- 55.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 234          |
|    mean_reward          | 3.14         |
| time/                   |              |
|    total_timesteps      | 1041000      |
| train/                  |              |
|    approx_kl            | 0.0057113664 |
|    clip_fraction        | 0.0446       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.94        |
|    explained_variance   | 0.892        |
|    learning_rate        | 4.25e-05     |
|    loss                 | -0.0083      |
|    n_updates            | 5080         |
|    policy_gradient_loss | -0.00647     |
|    std                  | 0.638        |
|    value_loss           | 0.00658      |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1042000, episode_reward=1.15 +/- 2.17
Episode length: 277.20 +/- 45.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 277      |
|    mean_reward     | 1.15     |
| time/              |          |
|    total_timesteps | 1042000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 615     |
|    iterations      | 509     |
|    time_elapsed    | 1694    |
|    total_timesteps | 1042432 |
--------------------------------
Eval num_timesteps=1043000, episode_reward=-0.76 +/- 0.38
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.761       |
| time/                   |              |
|    total_timesteps      | 1043000      |
| train/                  |              |
|    approx_kl            | 0.0046557533 |
|    clip_fraction        | 0.0372       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.94        |
|    explained_variance   | 0.844        |
|    learning_rate        | 4.25e-05     |
|    loss                 | 0.0743       |
|    n_updates            | 5090         |
|    policy_gradient_loss | -0.00667     |
|    std                  | 0.638        |
|    value_loss           | 0.0939       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1044000, episode_reward=0.48 +/- 2.35
Episode length: 277.40 +/- 45.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 277      |
|    mean_reward     | 0.48     |
| time/              |          |
|    total_timesteps | 1044000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 615     |
|    iterations      | 510     |
|    time_elapsed    | 1697    |
|    total_timesteps | 1044480 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1045000, episode_reward=3.08 +/- 2.62
Episode length: 221.40 +/- 66.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 221          |
|    mean_reward          | 3.08         |
| time/                   |              |
|    total_timesteps      | 1045000      |
| train/                  |              |
|    approx_kl            | 0.0019511507 |
|    clip_fraction        | 0.00889      |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.94        |
|    explained_variance   | 0.853        |
|    learning_rate        | 4.25e-05     |
|    loss                 | 0.0136       |
|    n_updates            | 5100         |
|    policy_gradient_loss | -0.0014      |
|    std                  | 0.639        |
|    value_loss           | 0.0503       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1046000, episode_reward=2.72 +/- 3.01
Episode length: 215.00 +/- 71.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 215      |
|    mean_reward     | 2.72     |
| time/              |          |
|    total_timesteps | 1046000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 615     |
|    iterations      | 511     |
|    time_elapsed    | 1700    |
|    total_timesteps | 1046528 |
--------------------------------
Eval num_timesteps=1047000, episode_reward=-0.74 +/- 0.51
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.743       |
| time/                   |              |
|    total_timesteps      | 1047000      |
| train/                  |              |
|    approx_kl            | 0.0052037193 |
|    clip_fraction        | 0.0436       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.94        |
|    explained_variance   | 0.92         |
|    learning_rate        | 4.26e-05     |
|    loss                 | -0.0175      |
|    n_updates            | 5110         |
|    policy_gradient_loss | -0.00449     |
|    std                  | 0.638        |
|    value_loss           | 0.0363       |
------------------------------------------
box reached target
Eval num_timesteps=1048000, episode_reward=-0.99 +/- 0.23
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.987   |
| time/              |          |
|    total_timesteps | 1048000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 615     |
|    iterations      | 512     |
|    time_elapsed    | 1703    |
|    total_timesteps | 1048576 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1049000, episode_reward=1.46 +/- 3.02
Episode length: 245.20 +/- 67.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 245         |
|    mean_reward          | 1.46        |
| time/                   |             |
|    total_timesteps      | 1049000     |
| train/                  |             |
|    approx_kl            | 0.005224135 |
|    clip_fraction        | 0.0351      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.94       |
|    explained_variance   | 0.923       |
|    learning_rate        | 4.26e-05    |
|    loss                 | 0.00221     |
|    n_updates            | 5120        |
|    policy_gradient_loss | -0.00505    |
|    std                  | 0.638       |
|    value_loss           | 0.0364      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1050000, episode_reward=3.40 +/- 2.35
Episode length: 228.60 +/- 59.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 229      |
|    mean_reward     | 3.4      |
| time/              |          |
|    total_timesteps | 1050000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 615     |
|    iterations      | 513     |
|    time_elapsed    | 1706    |
|    total_timesteps | 1050624 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1051000, episode_reward=1.60 +/- 2.95
Episode length: 248.00 +/- 63.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 248          |
|    mean_reward          | 1.6          |
| time/                   |              |
|    total_timesteps      | 1051000      |
| train/                  |              |
|    approx_kl            | 0.0037983097 |
|    clip_fraction        | 0.0174       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.94        |
|    explained_variance   | 0.939        |
|    learning_rate        | 4.26e-05     |
|    loss                 | -0.00722     |
|    n_updates            | 5130         |
|    policy_gradient_loss | -0.00397     |
|    std                  | 0.638        |
|    value_loss           | 0.0315       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1052000, episode_reward=3.02 +/- 2.70
Episode length: 215.00 +/- 70.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 215      |
|    mean_reward     | 3.02     |
| time/              |          |
|    total_timesteps | 1052000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 615     |
|    iterations      | 514     |
|    time_elapsed    | 1709    |
|    total_timesteps | 1052672 |
--------------------------------
box reached target
Eval num_timesteps=1053000, episode_reward=0.77 +/- 2.39
Episode length: 276.00 +/- 48.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 276          |
|    mean_reward          | 0.772        |
| time/                   |              |
|    total_timesteps      | 1053000      |
| train/                  |              |
|    approx_kl            | 0.0038658003 |
|    clip_fraction        | 0.0203       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.94        |
|    explained_variance   | 0.972        |
|    learning_rate        | 4.26e-05     |
|    loss                 | -0.00197     |
|    n_updates            | 5140         |
|    policy_gradient_loss | -0.0049      |
|    std                  | 0.637        |
|    value_loss           | 0.0126       |
------------------------------------------
box reached target
Eval num_timesteps=1054000, episode_reward=0.40 +/- 2.43
Episode length: 275.00 +/- 50.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 275      |
|    mean_reward     | 0.403    |
| time/              |          |
|    total_timesteps | 1054000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 615     |
|    iterations      | 515     |
|    time_elapsed    | 1713    |
|    total_timesteps | 1054720 |
--------------------------------
box reached target
Eval num_timesteps=1055000, episode_reward=0.83 +/- 2.29
Episode length: 270.80 +/- 58.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 271          |
|    mean_reward          | 0.825        |
| time/                   |              |
|    total_timesteps      | 1055000      |
| train/                  |              |
|    approx_kl            | 0.0047094226 |
|    clip_fraction        | 0.021        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.94        |
|    explained_variance   | 0.919        |
|    learning_rate        | 4.27e-05     |
|    loss                 | -0.00608     |
|    n_updates            | 5150         |
|    policy_gradient_loss | -0.00326     |
|    std                  | 0.639        |
|    value_loss           | 0.0257       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1056000, episode_reward=0.75 +/- 2.24
Episode length: 274.80 +/- 50.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 275      |
|    mean_reward     | 0.749    |
| time/              |          |
|    total_timesteps | 1056000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 615     |
|    iterations      | 516     |
|    time_elapsed    | 1716    |
|    total_timesteps | 1056768 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1057000, episode_reward=1.47 +/- 3.03
Episode length: 252.60 +/- 58.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 253         |
|    mean_reward          | 1.47        |
| time/                   |             |
|    total_timesteps      | 1057000     |
| train/                  |             |
|    approx_kl            | 0.004491958 |
|    clip_fraction        | 0.0276      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.94       |
|    explained_variance   | 0.962       |
|    learning_rate        | 4.27e-05    |
|    loss                 | -0.00163    |
|    n_updates            | 5160        |
|    policy_gradient_loss | -0.00694    |
|    std                  | 0.637       |
|    value_loss           | 0.0194      |
-----------------------------------------
box reached target
Eval num_timesteps=1058000, episode_reward=0.86 +/- 2.27
Episode length: 276.80 +/- 46.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 277      |
|    mean_reward     | 0.863    |
| time/              |          |
|    total_timesteps | 1058000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 615     |
|    iterations      | 517     |
|    time_elapsed    | 1719    |
|    total_timesteps | 1058816 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1059000, episode_reward=0.54 +/- 2.56
Episode length: 283.60 +/- 32.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 284          |
|    mean_reward          | 0.543        |
| time/                   |              |
|    total_timesteps      | 1059000      |
| train/                  |              |
|    approx_kl            | 0.0044069146 |
|    clip_fraction        | 0.0224       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.94        |
|    explained_variance   | 0.852        |
|    learning_rate        | 4.27e-05     |
|    loss                 | -0.00207     |
|    n_updates            | 5170         |
|    policy_gradient_loss | -0.00378     |
|    std                  | 0.638        |
|    value_loss           | 0.0728       |
------------------------------------------
Eval num_timesteps=1060000, episode_reward=-0.27 +/- 0.62
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.272   |
| time/              |          |
|    total_timesteps | 1060000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 615     |
|    iterations      | 518     |
|    time_elapsed    | 1722    |
|    total_timesteps | 1060864 |
--------------------------------
box reached target
Eval num_timesteps=1061000, episode_reward=0.61 +/- 2.47
Episode length: 284.00 +/- 32.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 284          |
|    mean_reward          | 0.611        |
| time/                   |              |
|    total_timesteps      | 1061000      |
| train/                  |              |
|    approx_kl            | 0.0040371395 |
|    clip_fraction        | 0.0206       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.94        |
|    explained_variance   | 0.974        |
|    learning_rate        | 4.27e-05     |
|    loss                 | -0.0054      |
|    n_updates            | 5180         |
|    policy_gradient_loss | -0.00349     |
|    std                  | 0.638        |
|    value_loss           | 0.00874      |
------------------------------------------
box reached target
Eval num_timesteps=1062000, episode_reward=0.34 +/- 2.46
Episode length: 277.60 +/- 44.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | 0.341    |
| time/              |          |
|    total_timesteps | 1062000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 615     |
|    iterations      | 519     |
|    time_elapsed    | 1726    |
|    total_timesteps | 1062912 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1063000, episode_reward=1.65 +/- 2.95
Episode length: 245.20 +/- 68.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 245         |
|    mean_reward          | 1.65        |
| time/                   |             |
|    total_timesteps      | 1063000     |
| train/                  |             |
|    approx_kl            | 0.008143936 |
|    clip_fraction        | 0.0738      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.94       |
|    explained_variance   | 0.877       |
|    learning_rate        | 4.28e-05    |
|    loss                 | -0.0104     |
|    n_updates            | 5190        |
|    policy_gradient_loss | -0.00811    |
|    std                  | 0.638       |
|    value_loss           | 0.0246      |
-----------------------------------------
Eval num_timesteps=1064000, episode_reward=-0.86 +/- 0.27
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.859   |
| time/              |          |
|    total_timesteps | 1064000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 615     |
|    iterations      | 520     |
|    time_elapsed    | 1729    |
|    total_timesteps | 1064960 |
--------------------------------
box reached target
Eval num_timesteps=1065000, episode_reward=0.23 +/- 2.46
Episode length: 268.80 +/- 62.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 269         |
|    mean_reward          | 0.231       |
| time/                   |             |
|    total_timesteps      | 1065000     |
| train/                  |             |
|    approx_kl            | 0.005552207 |
|    clip_fraction        | 0.0319      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.94       |
|    explained_variance   | 0.919       |
|    learning_rate        | 4.28e-05    |
|    loss                 | -0.00405    |
|    n_updates            | 5200        |
|    policy_gradient_loss | -0.00525    |
|    std                  | 0.638       |
|    value_loss           | 0.0397      |
-----------------------------------------
Eval num_timesteps=1066000, episode_reward=-1.05 +/- 0.11
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1.05    |
| time/              |          |
|    total_timesteps | 1066000  |
---------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1067000, episode_reward=0.59 +/- 2.31
Episode length: 279.60 +/- 40.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 280      |
|    mean_reward     | 0.593    |
| time/              |          |
|    total_timesteps | 1067000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 615     |
|    iterations      | 521     |
|    time_elapsed    | 1733    |
|    total_timesteps | 1067008 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1068000, episode_reward=3.06 +/- 2.77
Episode length: 226.20 +/- 64.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 226          |
|    mean_reward          | 3.06         |
| time/                   |              |
|    total_timesteps      | 1068000      |
| train/                  |              |
|    approx_kl            | 0.0033568528 |
|    clip_fraction        | 0.0328       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.94        |
|    explained_variance   | 0.969        |
|    learning_rate        | 4.28e-05     |
|    loss                 | -0.00826     |
|    n_updates            | 5210         |
|    policy_gradient_loss | -0.00496     |
|    std                  | 0.637        |
|    value_loss           | 0.0171       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1069000, episode_reward=-0.79 +/- 0.32
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.786   |
| time/              |          |
|    total_timesteps | 1069000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 615     |
|    iterations      | 522     |
|    time_elapsed    | 1737    |
|    total_timesteps | 1069056 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1070000, episode_reward=1.49 +/- 3.05
Episode length: 253.00 +/- 58.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 253          |
|    mean_reward          | 1.49         |
| time/                   |              |
|    total_timesteps      | 1070000      |
| train/                  |              |
|    approx_kl            | 0.0019585397 |
|    clip_fraction        | 0.0083       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.93        |
|    explained_variance   | 0.979        |
|    learning_rate        | 4.28e-05     |
|    loss                 | 0.0251       |
|    n_updates            | 5220         |
|    policy_gradient_loss | -0.00166     |
|    std                  | 0.637        |
|    value_loss           | 0.0224       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=1071000, episode_reward=1.72 +/- 2.86
Episode length: 249.80 +/- 61.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 250      |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 1071000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 615     |
|    iterations      | 523     |
|    time_elapsed    | 1740    |
|    total_timesteps | 1071104 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1072000, episode_reward=1.94 +/- 2.64
Episode length: 242.60 +/- 70.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 243         |
|    mean_reward          | 1.94        |
| time/                   |             |
|    total_timesteps      | 1072000     |
| train/                  |             |
|    approx_kl            | 0.005788079 |
|    clip_fraction        | 0.0449      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.93       |
|    explained_variance   | 0.957       |
|    learning_rate        | 4.29e-05    |
|    loss                 | 0.00413     |
|    n_updates            | 5230        |
|    policy_gradient_loss | -0.00491    |
|    std                  | 0.634       |
|    value_loss           | 0.02        |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1073000, episode_reward=3.11 +/- 2.82
Episode length: 228.20 +/- 64.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 228      |
|    mean_reward     | 3.11     |
| time/              |          |
|    total_timesteps | 1073000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 615     |
|    iterations      | 524     |
|    time_elapsed    | 1743    |
|    total_timesteps | 1073152 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1074000, episode_reward=4.26 +/- 1.90
Episode length: 201.00 +/- 51.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 201          |
|    mean_reward          | 4.26         |
| time/                   |              |
|    total_timesteps      | 1074000      |
| train/                  |              |
|    approx_kl            | 0.0050228513 |
|    clip_fraction        | 0.0344       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.93        |
|    explained_variance   | 0.893        |
|    learning_rate        | 4.29e-05     |
|    loss                 | 0.0339       |
|    n_updates            | 5240         |
|    policy_gradient_loss | -0.00498     |
|    std                  | 0.634        |
|    value_loss           | 0.032        |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1075000, episode_reward=4.04 +/- 2.37
Episode length: 208.60 +/- 46.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 209      |
|    mean_reward     | 4.04     |
| time/              |          |
|    total_timesteps | 1075000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 615     |
|    iterations      | 525     |
|    time_elapsed    | 1746    |
|    total_timesteps | 1075200 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1076000, episode_reward=1.40 +/- 3.01
Episode length: 248.80 +/- 63.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 249          |
|    mean_reward          | 1.4          |
| time/                   |              |
|    total_timesteps      | 1076000      |
| train/                  |              |
|    approx_kl            | 0.0055940603 |
|    clip_fraction        | 0.0459       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.93        |
|    explained_variance   | 0.967        |
|    learning_rate        | 4.29e-05     |
|    loss                 | 0.00514      |
|    n_updates            | 5250         |
|    policy_gradient_loss | -0.00513     |
|    std                  | 0.634        |
|    value_loss           | 0.02         |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1077000, episode_reward=1.48 +/- 3.04
Episode length: 256.20 +/- 55.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 256      |
|    mean_reward     | 1.48     |
| time/              |          |
|    total_timesteps | 1077000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 615     |
|    iterations      | 526     |
|    time_elapsed    | 1749    |
|    total_timesteps | 1077248 |
--------------------------------
box reached target
Eval num_timesteps=1078000, episode_reward=0.26 +/- 2.51
Episode length: 270.00 +/- 60.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 270          |
|    mean_reward          | 0.257        |
| time/                   |              |
|    total_timesteps      | 1078000      |
| train/                  |              |
|    approx_kl            | 0.0057905586 |
|    clip_fraction        | 0.0525       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.93        |
|    explained_variance   | 0.927        |
|    learning_rate        | 4.29e-05     |
|    loss                 | 0.000514     |
|    n_updates            | 5260         |
|    policy_gradient_loss | -0.00544     |
|    std                  | 0.634        |
|    value_loss           | 0.0599       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1079000, episode_reward=4.03 +/- 2.52
Episode length: 200.60 +/- 57.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 201      |
|    mean_reward     | 4.03     |
| time/              |          |
|    total_timesteps | 1079000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 615     |
|    iterations      | 527     |
|    time_elapsed    | 1752    |
|    total_timesteps | 1079296 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1080000, episode_reward=1.76 +/- 2.86
Episode length: 251.00 +/- 60.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 251          |
|    mean_reward          | 1.76         |
| time/                   |              |
|    total_timesteps      | 1080000      |
| train/                  |              |
|    approx_kl            | 0.0056581125 |
|    clip_fraction        | 0.0374       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.93        |
|    explained_variance   | 0.797        |
|    learning_rate        | 4.3e-05      |
|    loss                 | -0.0344      |
|    n_updates            | 5270         |
|    policy_gradient_loss | -0.00571     |
|    std                  | 0.636        |
|    value_loss           | 0.0182       |
------------------------------------------
box reached target
Eval num_timesteps=1081000, episode_reward=0.50 +/- 2.44
Episode length: 276.40 +/- 47.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 276      |
|    mean_reward     | 0.502    |
| time/              |          |
|    total_timesteps | 1081000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 615     |
|    iterations      | 528     |
|    time_elapsed    | 1756    |
|    total_timesteps | 1081344 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1082000, episode_reward=1.46 +/- 3.01
Episode length: 254.00 +/- 56.85
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 254        |
|    mean_reward          | 1.46       |
| time/                   |            |
|    total_timesteps      | 1082000    |
| train/                  |            |
|    approx_kl            | 0.00491067 |
|    clip_fraction        | 0.0279     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.93      |
|    explained_variance   | 0.862      |
|    learning_rate        | 4.3e-05    |
|    loss                 | -0.00597   |
|    n_updates            | 5280       |
|    policy_gradient_loss | -0.00293   |
|    std                  | 0.636      |
|    value_loss           | 0.0242     |
----------------------------------------
Eval num_timesteps=1083000, episode_reward=-0.92 +/- 0.16
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.92    |
| time/              |          |
|    total_timesteps | 1083000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 615     |
|    iterations      | 529     |
|    time_elapsed    | 1759    |
|    total_timesteps | 1083392 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1084000, episode_reward=2.76 +/- 3.07
Episode length: 230.40 +/- 62.30
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 230        |
|    mean_reward          | 2.76       |
| time/                   |            |
|    total_timesteps      | 1084000    |
| train/                  |            |
|    approx_kl            | 0.00367131 |
|    clip_fraction        | 0.0303     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.93      |
|    explained_variance   | 0.892      |
|    learning_rate        | 4.3e-05    |
|    loss                 | -0.00434   |
|    n_updates            | 5290       |
|    policy_gradient_loss | -0.00487   |
|    std                  | 0.636      |
|    value_loss           | 0.007      |
----------------------------------------
box reached target
Eval num_timesteps=1085000, episode_reward=0.22 +/- 2.43
Episode length: 276.20 +/- 47.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 276      |
|    mean_reward     | 0.215    |
| time/              |          |
|    total_timesteps | 1085000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 615     |
|    iterations      | 530     |
|    time_elapsed    | 1762    |
|    total_timesteps | 1085440 |
--------------------------------
Eval num_timesteps=1086000, episode_reward=-1.00 +/- 0.00
Episode length: 300.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 300        |
|    mean_reward          | -1         |
| time/                   |            |
|    total_timesteps      | 1086000    |
| train/                  |            |
|    approx_kl            | 0.00506951 |
|    clip_fraction        | 0.0343     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.93      |
|    explained_variance   | 0.943      |
|    learning_rate        | 4.3e-05    |
|    loss                 | -0.017     |
|    n_updates            | 5300       |
|    policy_gradient_loss | -0.0037    |
|    std                  | 0.635      |
|    value_loss           | 0.00641    |
----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1087000, episode_reward=4.27 +/- 2.12
Episode length: 200.40 +/- 53.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 200      |
|    mean_reward     | 4.27     |
| time/              |          |
|    total_timesteps | 1087000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 615     |
|    iterations      | 531     |
|    time_elapsed    | 1765    |
|    total_timesteps | 1087488 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1088000, episode_reward=1.84 +/- 2.81
Episode length: 253.40 +/- 57.12
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 253        |
|    mean_reward          | 1.84       |
| time/                   |            |
|    total_timesteps      | 1088000    |
| train/                  |            |
|    approx_kl            | 0.00456283 |
|    clip_fraction        | 0.0323     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.93      |
|    explained_variance   | 0.827      |
|    learning_rate        | 4.3e-05    |
|    loss                 | 0.0214     |
|    n_updates            | 5310       |
|    policy_gradient_loss | -0.00454   |
|    std                  | 0.634      |
|    value_loss           | 0.0598     |
----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1089000, episode_reward=1.82 +/- 2.81
Episode length: 251.40 +/- 59.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | 1.82     |
| time/              |          |
|    total_timesteps | 1089000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 616     |
|    iterations      | 532     |
|    time_elapsed    | 1768    |
|    total_timesteps | 1089536 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1090000, episode_reward=2.68 +/- 3.16
Episode length: 220.20 +/- 65.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 220         |
|    mean_reward          | 2.68        |
| time/                   |             |
|    total_timesteps      | 1090000     |
| train/                  |             |
|    approx_kl            | 0.006239801 |
|    clip_fraction        | 0.0375      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.93       |
|    explained_variance   | 0.889       |
|    learning_rate        | 4.31e-05    |
|    loss                 | -0.0308     |
|    n_updates            | 5320        |
|    policy_gradient_loss | -0.00488    |
|    std                  | 0.634       |
|    value_loss           | 0.0225      |
-----------------------------------------
Eval num_timesteps=1091000, episode_reward=-0.47 +/- 0.78
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.468   |
| time/              |          |
|    total_timesteps | 1091000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 616     |
|    iterations      | 533     |
|    time_elapsed    | 1771    |
|    total_timesteps | 1091584 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1092000, episode_reward=1.67 +/- 2.86
Episode length: 251.20 +/- 60.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 251          |
|    mean_reward          | 1.67         |
| time/                   |              |
|    total_timesteps      | 1092000      |
| train/                  |              |
|    approx_kl            | 0.0035988549 |
|    clip_fraction        | 0.03         |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.93        |
|    explained_variance   | 0.813        |
|    learning_rate        | 4.31e-05     |
|    loss                 | -0.0161      |
|    n_updates            | 5330         |
|    policy_gradient_loss | -0.00497     |
|    std                  | 0.633        |
|    value_loss           | 0.0126       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=1093000, episode_reward=0.47 +/- 2.44
Episode length: 278.00 +/- 44.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | 0.471    |
| time/              |          |
|    total_timesteps | 1093000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 616     |
|    iterations      | 534     |
|    time_elapsed    | 1774    |
|    total_timesteps | 1093632 |
--------------------------------
box reached target
Eval num_timesteps=1094000, episode_reward=0.64 +/- 2.42
Episode length: 277.20 +/- 45.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 277          |
|    mean_reward          | 0.643        |
| time/                   |              |
|    total_timesteps      | 1094000      |
| train/                  |              |
|    approx_kl            | 0.0064225504 |
|    clip_fraction        | 0.0579       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.92        |
|    explained_variance   | 0.933        |
|    learning_rate        | 4.31e-05     |
|    loss                 | 0.0101       |
|    n_updates            | 5340         |
|    policy_gradient_loss | -0.00602     |
|    std                  | 0.631        |
|    value_loss           | 0.00833      |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1095000, episode_reward=4.16 +/- 1.98
Episode length: 198.40 +/- 51.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 198      |
|    mean_reward     | 4.16     |
| time/              |          |
|    total_timesteps | 1095000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 616     |
|    iterations      | 535     |
|    time_elapsed    | 1777    |
|    total_timesteps | 1095680 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1096000, episode_reward=0.21 +/- 2.46
Episode length: 280.40 +/- 39.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 280          |
|    mean_reward          | 0.207        |
| time/                   |              |
|    total_timesteps      | 1096000      |
| train/                  |              |
|    approx_kl            | 0.0048134215 |
|    clip_fraction        | 0.0419       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.92        |
|    explained_variance   | 0.889        |
|    learning_rate        | 4.31e-05     |
|    loss                 | -0.0069      |
|    n_updates            | 5350         |
|    policy_gradient_loss | -0.00789     |
|    std                  | 0.63         |
|    value_loss           | 0.0251       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1097000, episode_reward=0.48 +/- 2.32
Episode length: 279.40 +/- 41.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 279      |
|    mean_reward     | 0.484    |
| time/              |          |
|    total_timesteps | 1097000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 616     |
|    iterations      | 536     |
|    time_elapsed    | 1781    |
|    total_timesteps | 1097728 |
--------------------------------
Eval num_timesteps=1098000, episode_reward=-1.02 +/- 0.04
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -1.02       |
| time/                   |             |
|    total_timesteps      | 1098000     |
| train/                  |             |
|    approx_kl            | 0.004090593 |
|    clip_fraction        | 0.0167      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.91       |
|    explained_variance   | 0.868       |
|    learning_rate        | 4.32e-05    |
|    loss                 | 0.0233      |
|    n_updates            | 5360        |
|    policy_gradient_loss | -0.00267    |
|    std                  | 0.631       |
|    value_loss           | 0.0958      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=1099000, episode_reward=0.58 +/- 2.53
Episode length: 283.40 +/- 33.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 283      |
|    mean_reward     | 0.578    |
| time/              |          |
|    total_timesteps | 1099000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 616     |
|    iterations      | 537     |
|    time_elapsed    | 1784    |
|    total_timesteps | 1099776 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1100000, episode_reward=1.94 +/- 2.74
Episode length: 239.20 +/- 74.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 239          |
|    mean_reward          | 1.94         |
| time/                   |              |
|    total_timesteps      | 1100000      |
| train/                  |              |
|    approx_kl            | 0.0053517693 |
|    clip_fraction        | 0.0561       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.91        |
|    explained_variance   | 0.697        |
|    learning_rate        | 4.32e-05     |
|    loss                 | 0.00335      |
|    n_updates            | 5370         |
|    policy_gradient_loss | -0.00965     |
|    std                  | 0.63         |
|    value_loss           | 0.124        |
------------------------------------------
box reached target
Eval num_timesteps=1101000, episode_reward=-1.00 +/- 0.17
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 1101000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 616     |
|    iterations      | 538     |
|    time_elapsed    | 1787    |
|    total_timesteps | 1101824 |
--------------------------------
Eval num_timesteps=1102000, episode_reward=-0.34 +/- 0.81
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.342       |
| time/                   |              |
|    total_timesteps      | 1102000      |
| train/                  |              |
|    approx_kl            | 0.0030016112 |
|    clip_fraction        | 0.022        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.92        |
|    explained_variance   | 0.89         |
|    learning_rate        | 4.32e-05     |
|    loss                 | 0.00667      |
|    n_updates            | 5380         |
|    policy_gradient_loss | -0.00274     |
|    std                  | 0.631        |
|    value_loss           | 0.0683       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1103000, episode_reward=2.03 +/- 2.61
Episode length: 255.00 +/- 55.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 255      |
|    mean_reward     | 2.03     |
| time/              |          |
|    total_timesteps | 1103000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 616     |
|    iterations      | 539     |
|    time_elapsed    | 1790    |
|    total_timesteps | 1103872 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1104000, episode_reward=2.91 +/- 3.20
Episode length: 255.40 +/- 43.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 255         |
|    mean_reward          | 2.91        |
| time/                   |             |
|    total_timesteps      | 1104000     |
| train/                  |             |
|    approx_kl            | 0.005935846 |
|    clip_fraction        | 0.0573      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.91       |
|    explained_variance   | 0.959       |
|    learning_rate        | 4.32e-05    |
|    loss                 | -0.00649    |
|    n_updates            | 5390        |
|    policy_gradient_loss | -0.00476    |
|    std                  | 0.629       |
|    value_loss           | 0.017       |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1105000, episode_reward=1.60 +/- 3.19
Episode length: 256.40 +/- 59.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 256      |
|    mean_reward     | 1.6      |
| time/              |          |
|    total_timesteps | 1105000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 616     |
|    iterations      | 540     |
|    time_elapsed    | 1794    |
|    total_timesteps | 1105920 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1106000, episode_reward=2.75 +/- 3.07
Episode length: 226.20 +/- 63.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 226          |
|    mean_reward          | 2.75         |
| time/                   |              |
|    total_timesteps      | 1106000      |
| train/                  |              |
|    approx_kl            | 0.0035093317 |
|    clip_fraction        | 0.0222       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.91        |
|    explained_variance   | 0.975        |
|    learning_rate        | 4.33e-05     |
|    loss                 | -0.00401     |
|    n_updates            | 5400         |
|    policy_gradient_loss | -0.00275     |
|    std                  | 0.629        |
|    value_loss           | 0.00882      |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1107000, episode_reward=2.90 +/- 2.89
Episode length: 218.40 +/- 66.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 218      |
|    mean_reward     | 2.9      |
| time/              |          |
|    total_timesteps | 1107000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 616     |
|    iterations      | 541     |
|    time_elapsed    | 1797    |
|    total_timesteps | 1107968 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1108000, episode_reward=1.59 +/- 3.03
Episode length: 249.60 +/- 63.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 250          |
|    mean_reward          | 1.59         |
| time/                   |              |
|    total_timesteps      | 1108000      |
| train/                  |              |
|    approx_kl            | 0.0051579634 |
|    clip_fraction        | 0.0496       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.91        |
|    explained_variance   | 0.925        |
|    learning_rate        | 4.33e-05     |
|    loss                 | -0.0093      |
|    n_updates            | 5410         |
|    policy_gradient_loss | -0.00491     |
|    std                  | 0.628        |
|    value_loss           | 0.0273       |
------------------------------------------
Eval num_timesteps=1109000, episode_reward=-0.50 +/- 0.67
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.496   |
| time/              |          |
|    total_timesteps | 1109000  |
---------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1110000, episode_reward=1.47 +/- 3.03
Episode length: 241.80 +/- 71.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 242      |
|    mean_reward     | 1.47     |
| time/              |          |
|    total_timesteps | 1110000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 616     |
|    iterations      | 542     |
|    time_elapsed    | 1800    |
|    total_timesteps | 1110016 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1111000, episode_reward=2.08 +/- 2.56
Episode length: 251.60 +/- 59.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 252          |
|    mean_reward          | 2.08         |
| time/                   |              |
|    total_timesteps      | 1111000      |
| train/                  |              |
|    approx_kl            | 0.0044252463 |
|    clip_fraction        | 0.0296       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.91        |
|    explained_variance   | 0.944        |
|    learning_rate        | 4.33e-05     |
|    loss                 | 0.0205       |
|    n_updates            | 5420         |
|    policy_gradient_loss | -0.00455     |
|    std                  | 0.628        |
|    value_loss           | 0.0456       |
------------------------------------------
box reached target
Eval num_timesteps=1112000, episode_reward=0.85 +/- 2.35
Episode length: 278.00 +/- 44.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | 0.854    |
| time/              |          |
|    total_timesteps | 1112000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 616     |
|    iterations      | 543     |
|    time_elapsed    | 1803    |
|    total_timesteps | 1112064 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1113000, episode_reward=0.66 +/- 2.33
Episode length: 272.20 +/- 55.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 272         |
|    mean_reward          | 0.659       |
| time/                   |             |
|    total_timesteps      | 1113000     |
| train/                  |             |
|    approx_kl            | 0.005480136 |
|    clip_fraction        | 0.0335      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.91       |
|    explained_variance   | 0.961       |
|    learning_rate        | 4.33e-05    |
|    loss                 | -0.00457    |
|    n_updates            | 5430        |
|    policy_gradient_loss | -0.00372    |
|    std                  | 0.627       |
|    value_loss           | 0.0111      |
-----------------------------------------
box reached target
Eval num_timesteps=1114000, episode_reward=0.55 +/- 2.35
Episode length: 276.20 +/- 47.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 276      |
|    mean_reward     | 0.553    |
| time/              |          |
|    total_timesteps | 1114000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 616     |
|    iterations      | 544     |
|    time_elapsed    | 1807    |
|    total_timesteps | 1114112 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1115000, episode_reward=3.24 +/- 2.54
Episode length: 223.60 +/- 68.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 224          |
|    mean_reward          | 3.24         |
| time/                   |              |
|    total_timesteps      | 1115000      |
| train/                  |              |
|    approx_kl            | 0.0034813052 |
|    clip_fraction        | 0.0201       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.9         |
|    explained_variance   | 0.957        |
|    learning_rate        | 4.34e-05     |
|    loss                 | -0.0171      |
|    n_updates            | 5440         |
|    policy_gradient_loss | -0.00529     |
|    std                  | 0.627        |
|    value_loss           | 0.0196       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1116000, episode_reward=2.00 +/- 2.59
Episode length: 253.60 +/- 57.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 254      |
|    mean_reward     | 2        |
| time/              |          |
|    total_timesteps | 1116000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 616     |
|    iterations      | 545     |
|    time_elapsed    | 1810    |
|    total_timesteps | 1116160 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1117000, episode_reward=1.86 +/- 2.95
Episode length: 261.20 +/- 50.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 261         |
|    mean_reward          | 1.86        |
| time/                   |             |
|    total_timesteps      | 1117000     |
| train/                  |             |
|    approx_kl            | 0.008329258 |
|    clip_fraction        | 0.0926      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.91       |
|    explained_variance   | 0.895       |
|    learning_rate        | 4.34e-05    |
|    loss                 | 0.0747      |
|    n_updates            | 5450        |
|    policy_gradient_loss | -0.00804    |
|    std                  | 0.628       |
|    value_loss           | 0.0539      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1118000, episode_reward=1.91 +/- 2.92
Episode length: 264.60 +/- 46.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 265      |
|    mean_reward     | 1.91     |
| time/              |          |
|    total_timesteps | 1118000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 616     |
|    iterations      | 546     |
|    time_elapsed    | 1813    |
|    total_timesteps | 1118208 |
--------------------------------
Eval num_timesteps=1119000, episode_reward=-0.74 +/- 0.42
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.736       |
| time/                   |              |
|    total_timesteps      | 1119000      |
| train/                  |              |
|    approx_kl            | 0.0016357367 |
|    clip_fraction        | 0.00205      |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.9         |
|    explained_variance   | 0.729        |
|    learning_rate        | 4.34e-05     |
|    loss                 | 0.00874      |
|    n_updates            | 5460         |
|    policy_gradient_loss | -0.000694    |
|    std                  | 0.625        |
|    value_loss           | 0.109        |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=1120000, episode_reward=1.72 +/- 2.87
Episode length: 240.80 +/- 73.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 241      |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 1120000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 616     |
|    iterations      | 547     |
|    time_elapsed    | 1816    |
|    total_timesteps | 1120256 |
--------------------------------
Eval num_timesteps=1121000, episode_reward=-1.02 +/- 0.15
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -1.02        |
| time/                   |              |
|    total_timesteps      | 1121000      |
| train/                  |              |
|    approx_kl            | 0.0039502466 |
|    clip_fraction        | 0.0343       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.9         |
|    explained_variance   | 0.901        |
|    learning_rate        | 4.34e-05     |
|    loss                 | -0.000351    |
|    n_updates            | 5470         |
|    policy_gradient_loss | -0.00647     |
|    std                  | 0.626        |
|    value_loss           | 0.024        |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1122000, episode_reward=3.06 +/- 2.70
Episode length: 227.60 +/- 64.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 228      |
|    mean_reward     | 3.06     |
| time/              |          |
|    total_timesteps | 1122000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 616     |
|    iterations      | 548     |
|    time_elapsed    | 1819    |
|    total_timesteps | 1122304 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1123000, episode_reward=1.53 +/- 3.10
Episode length: 256.00 +/- 55.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 256          |
|    mean_reward          | 1.53         |
| time/                   |              |
|    total_timesteps      | 1123000      |
| train/                  |              |
|    approx_kl            | 0.0064664753 |
|    clip_fraction        | 0.0446       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.9         |
|    explained_variance   | 0.782        |
|    learning_rate        | 4.35e-05     |
|    loss                 | 0.0252       |
|    n_updates            | 5480         |
|    policy_gradient_loss | -0.00421     |
|    std                  | 0.625        |
|    value_loss           | 0.0359       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1124000, episode_reward=2.68 +/- 3.12
Episode length: 213.40 +/- 71.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 213      |
|    mean_reward     | 2.68     |
| time/              |          |
|    total_timesteps | 1124000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 616     |
|    iterations      | 549     |
|    time_elapsed    | 1822    |
|    total_timesteps | 1124352 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1125000, episode_reward=0.60 +/- 2.29
Episode length: 269.80 +/- 60.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 270         |
|    mean_reward          | 0.604       |
| time/                   |             |
|    total_timesteps      | 1125000     |
| train/                  |             |
|    approx_kl            | 0.005631182 |
|    clip_fraction        | 0.0482      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.89       |
|    explained_variance   | 0.964       |
|    learning_rate        | 4.35e-05    |
|    loss                 | 0.00367     |
|    n_updates            | 5490        |
|    policy_gradient_loss | -0.00659    |
|    std                  | 0.622       |
|    value_loss           | 0.0167      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1126000, episode_reward=0.59 +/- 2.33
Episode length: 273.80 +/- 52.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 274      |
|    mean_reward     | 0.585    |
| time/              |          |
|    total_timesteps | 1126000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 616     |
|    iterations      | 550     |
|    time_elapsed    | 1825    |
|    total_timesteps | 1126400 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1127000, episode_reward=1.64 +/- 2.84
Episode length: 249.60 +/- 61.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 250          |
|    mean_reward          | 1.64         |
| time/                   |              |
|    total_timesteps      | 1127000      |
| train/                  |              |
|    approx_kl            | 0.0050863447 |
|    clip_fraction        | 0.0383       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.89        |
|    explained_variance   | 0.955        |
|    learning_rate        | 4.35e-05     |
|    loss                 | -0.00554     |
|    n_updates            | 5500         |
|    policy_gradient_loss | -0.00588     |
|    std                  | 0.621        |
|    value_loss           | 0.0351       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1128000, episode_reward=3.16 +/- 2.78
Episode length: 230.60 +/- 69.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 231      |
|    mean_reward     | 3.16     |
| time/              |          |
|    total_timesteps | 1128000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 616     |
|    iterations      | 551     |
|    time_elapsed    | 1829    |
|    total_timesteps | 1128448 |
--------------------------------
box reached target
Eval num_timesteps=1129000, episode_reward=0.91 +/- 2.50
Episode length: 293.40 +/- 13.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 293          |
|    mean_reward          | 0.905        |
| time/                   |              |
|    total_timesteps      | 1129000      |
| train/                  |              |
|    approx_kl            | 0.0048178425 |
|    clip_fraction        | 0.0358       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.88        |
|    explained_variance   | 0.966        |
|    learning_rate        | 4.35e-05     |
|    loss                 | -0.02        |
|    n_updates            | 5510         |
|    policy_gradient_loss | -0.00667     |
|    std                  | 0.619        |
|    value_loss           | 0.0316       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1130000, episode_reward=3.22 +/- 2.48
Episode length: 225.60 +/- 62.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 226      |
|    mean_reward     | 3.22     |
| time/              |          |
|    total_timesteps | 1130000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 617     |
|    iterations      | 552     |
|    time_elapsed    | 1832    |
|    total_timesteps | 1130496 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1131000, episode_reward=1.92 +/- 2.86
Episode length: 266.60 +/- 42.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 267         |
|    mean_reward          | 1.92        |
| time/                   |             |
|    total_timesteps      | 1131000     |
| train/                  |             |
|    approx_kl            | 0.002898075 |
|    clip_fraction        | 0.017       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.88       |
|    explained_variance   | 0.756       |
|    learning_rate        | 4.36e-05    |
|    loss                 | -0.013      |
|    n_updates            | 5520        |
|    policy_gradient_loss | -0.00301    |
|    std                  | 0.618       |
|    value_loss           | 0.00727     |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=1132000, episode_reward=1.70 +/- 2.95
Episode length: 263.80 +/- 51.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 264      |
|    mean_reward     | 1.7      |
| time/              |          |
|    total_timesteps | 1132000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 616     |
|    iterations      | 553     |
|    time_elapsed    | 1836    |
|    total_timesteps | 1132544 |
--------------------------------
box reached target
Eval num_timesteps=1133000, episode_reward=0.24 +/- 2.48
Episode length: 274.00 +/- 52.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 274         |
|    mean_reward          | 0.239       |
| time/                   |             |
|    total_timesteps      | 1133000     |
| train/                  |             |
|    approx_kl            | 0.005598165 |
|    clip_fraction        | 0.0419      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.88       |
|    explained_variance   | 0.963       |
|    learning_rate        | 4.36e-05    |
|    loss                 | -0.00301    |
|    n_updates            | 5530        |
|    policy_gradient_loss | -0.00391    |
|    std                  | 0.619       |
|    value_loss           | 0.00478     |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=1134000, episode_reward=0.47 +/- 2.42
Episode length: 275.40 +/- 49.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 275      |
|    mean_reward     | 0.465    |
| time/              |          |
|    total_timesteps | 1134000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 616     |
|    iterations      | 554     |
|    time_elapsed    | 1840    |
|    total_timesteps | 1134592 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1135000, episode_reward=1.48 +/- 3.04
Episode length: 243.60 +/- 69.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 244          |
|    mean_reward          | 1.48         |
| time/                   |              |
|    total_timesteps      | 1135000      |
| train/                  |              |
|    approx_kl            | 0.0039705737 |
|    clip_fraction        | 0.0272       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.87        |
|    explained_variance   | 0.959        |
|    learning_rate        | 4.36e-05     |
|    loss                 | -0.00134     |
|    n_updates            | 5540         |
|    policy_gradient_loss | -0.00733     |
|    std                  | 0.617        |
|    value_loss           | 0.0208       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1136000, episode_reward=1.59 +/- 3.12
Episode length: 252.80 +/- 64.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 253      |
|    mean_reward     | 1.59     |
| time/              |          |
|    total_timesteps | 1136000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 616     |
|    iterations      | 555     |
|    time_elapsed    | 1843    |
|    total_timesteps | 1136640 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1137000, episode_reward=2.70 +/- 3.02
Episode length: 217.40 +/- 67.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 217          |
|    mean_reward          | 2.7          |
| time/                   |              |
|    total_timesteps      | 1137000      |
| train/                  |              |
|    approx_kl            | 0.0026657667 |
|    clip_fraction        | 0.0141       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.87        |
|    explained_variance   | 0.935        |
|    learning_rate        | 4.36e-05     |
|    loss                 | -0.00522     |
|    n_updates            | 5550         |
|    policy_gradient_loss | -0.00325     |
|    std                  | 0.616        |
|    value_loss           | 0.00977      |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1138000, episode_reward=2.87 +/- 3.16
Episode length: 241.60 +/- 51.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 242      |
|    mean_reward     | 2.87     |
| time/              |          |
|    total_timesteps | 1138000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 616     |
|    iterations      | 556     |
|    time_elapsed    | 1846    |
|    total_timesteps | 1138688 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1139000, episode_reward=2.94 +/- 2.82
Episode length: 232.20 +/- 61.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 232          |
|    mean_reward          | 2.94         |
| time/                   |              |
|    total_timesteps      | 1139000      |
| train/                  |              |
|    approx_kl            | 0.0053224256 |
|    clip_fraction        | 0.0458       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.87        |
|    explained_variance   | 0.93         |
|    learning_rate        | 4.37e-05     |
|    loss                 | -0.0199      |
|    n_updates            | 5560         |
|    policy_gradient_loss | -0.00521     |
|    std                  | 0.618        |
|    value_loss           | 0.00792      |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1140000, episode_reward=1.75 +/- 3.03
Episode length: 272.80 +/- 42.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 273      |
|    mean_reward     | 1.75     |
| time/              |          |
|    total_timesteps | 1140000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 616     |
|    iterations      | 557     |
|    time_elapsed    | 1849    |
|    total_timesteps | 1140736 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1141000, episode_reward=2.84 +/- 3.14
Episode length: 226.20 +/- 61.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 226          |
|    mean_reward          | 2.84         |
| time/                   |              |
|    total_timesteps      | 1141000      |
| train/                  |              |
|    approx_kl            | 0.0033823166 |
|    clip_fraction        | 0.0193       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.87        |
|    explained_variance   | 0.969        |
|    learning_rate        | 4.37e-05     |
|    loss                 | -0.0152      |
|    n_updates            | 5570         |
|    policy_gradient_loss | -0.00419     |
|    std                  | 0.615        |
|    value_loss           | 0.0211       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1142000, episode_reward=2.00 +/- 2.68
Episode length: 240.00 +/- 74.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 240      |
|    mean_reward     | 2        |
| time/              |          |
|    total_timesteps | 1142000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 616     |
|    iterations      | 558     |
|    time_elapsed    | 1852    |
|    total_timesteps | 1142784 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1143000, episode_reward=1.81 +/- 2.87
Episode length: 254.80 +/- 55.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 255         |
|    mean_reward          | 1.81        |
| time/                   |             |
|    total_timesteps      | 1143000     |
| train/                  |             |
|    approx_kl            | 0.004166439 |
|    clip_fraction        | 0.0322      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.86       |
|    explained_variance   | 0.891       |
|    learning_rate        | 4.37e-05    |
|    loss                 | -0.00266    |
|    n_updates            | 5580        |
|    policy_gradient_loss | -0.00596    |
|    std                  | 0.613       |
|    value_loss           | 0.0247      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1144000, episode_reward=1.73 +/- 3.03
Episode length: 251.20 +/- 61.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | 1.73     |
| time/              |          |
|    total_timesteps | 1144000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 616     |
|    iterations      | 559     |
|    time_elapsed    | 1855    |
|    total_timesteps | 1144832 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1145000, episode_reward=0.27 +/- 2.54
Episode length: 281.80 +/- 36.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 282         |
|    mean_reward          | 0.271       |
| time/                   |             |
|    total_timesteps      | 1145000     |
| train/                  |             |
|    approx_kl            | 0.005429578 |
|    clip_fraction        | 0.0518      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.86       |
|    explained_variance   | 0.913       |
|    learning_rate        | 4.37e-05    |
|    loss                 | -0.00355    |
|    n_updates            | 5590        |
|    policy_gradient_loss | -0.00657    |
|    std                  | 0.613       |
|    value_loss           | 0.0423      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1146000, episode_reward=0.51 +/- 2.37
Episode length: 274.80 +/- 50.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 275      |
|    mean_reward     | 0.506    |
| time/              |          |
|    total_timesteps | 1146000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 617     |
|    iterations      | 560     |
|    time_elapsed    | 1858    |
|    total_timesteps | 1146880 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1147000, episode_reward=1.78 +/- 3.11
Episode length: 264.00 +/- 44.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 264          |
|    mean_reward          | 1.78         |
| time/                   |              |
|    total_timesteps      | 1147000      |
| train/                  |              |
|    approx_kl            | 0.0045208894 |
|    clip_fraction        | 0.028        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.86        |
|    explained_variance   | 0.917        |
|    learning_rate        | 4.38e-05     |
|    loss                 | -0.00528     |
|    n_updates            | 5600         |
|    policy_gradient_loss | -0.00312     |
|    std                  | 0.615        |
|    value_loss           | 0.0171       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1148000, episode_reward=1.90 +/- 2.80
Episode length: 268.60 +/- 41.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 269      |
|    mean_reward     | 1.9      |
| time/              |          |
|    total_timesteps | 1148000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 617     |
|    iterations      | 561     |
|    time_elapsed    | 1861    |
|    total_timesteps | 1148928 |
--------------------------------
Eval num_timesteps=1149000, episode_reward=-0.85 +/- 0.29
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.853       |
| time/                   |              |
|    total_timesteps      | 1149000      |
| train/                  |              |
|    approx_kl            | 0.0035142181 |
|    clip_fraction        | 0.0248       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.87        |
|    explained_variance   | 0.92         |
|    learning_rate        | 4.38e-05     |
|    loss                 | 0.0177       |
|    n_updates            | 5610         |
|    policy_gradient_loss | -0.0042      |
|    std                  | 0.616        |
|    value_loss           | 0.0338       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=1150000, episode_reward=0.80 +/- 2.25
Episode length: 277.00 +/- 46.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 277      |
|    mean_reward     | 0.802    |
| time/              |          |
|    total_timesteps | 1150000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 617     |
|    iterations      | 562     |
|    time_elapsed    | 1865    |
|    total_timesteps | 1150976 |
--------------------------------
box reached target
Eval num_timesteps=1151000, episode_reward=0.18 +/- 2.52
Episode length: 280.20 +/- 39.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 280          |
|    mean_reward          | 0.182        |
| time/                   |              |
|    total_timesteps      | 1151000      |
| train/                  |              |
|    approx_kl            | 0.0040765163 |
|    clip_fraction        | 0.0197       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.87        |
|    explained_variance   | 0.863        |
|    learning_rate        | 4.38e-05     |
|    loss                 | 0.0195       |
|    n_updates            | 5620         |
|    policy_gradient_loss | -0.00371     |
|    std                  | 0.616        |
|    value_loss           | 0.0702       |
------------------------------------------
box reached target
Eval num_timesteps=1152000, episode_reward=0.58 +/- 2.42
Episode length: 276.60 +/- 46.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 277      |
|    mean_reward     | 0.585    |
| time/              |          |
|    total_timesteps | 1152000  |
---------------------------------
box reached target
Eval num_timesteps=1153000, episode_reward=0.75 +/- 2.23
Episode length: 270.00 +/- 60.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 270      |
|    mean_reward     | 0.751    |
| time/              |          |
|    total_timesteps | 1153000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 616     |
|    iterations      | 563     |
|    time_elapsed    | 1869    |
|    total_timesteps | 1153024 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1154000, episode_reward=2.18 +/- 2.72
Episode length: 259.00 +/- 53.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 259          |
|    mean_reward          | 2.18         |
| time/                   |              |
|    total_timesteps      | 1154000      |
| train/                  |              |
|    approx_kl            | 0.0039746957 |
|    clip_fraction        | 0.028        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.87        |
|    explained_variance   | 0.735        |
|    learning_rate        | 4.38e-05     |
|    loss                 | -0.0113      |
|    n_updates            | 5630         |
|    policy_gradient_loss | -0.0061      |
|    std                  | 0.617        |
|    value_loss           | 0.0301       |
------------------------------------------
box reached target
Eval num_timesteps=1155000, episode_reward=-0.88 +/- 0.23
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.884   |
| time/              |          |
|    total_timesteps | 1155000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 616     |
|    iterations      | 564     |
|    time_elapsed    | 1872    |
|    total_timesteps | 1155072 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1156000, episode_reward=1.10 +/- 2.20
Episode length: 272.60 +/- 54.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 273         |
|    mean_reward          | 1.1         |
| time/                   |             |
|    total_timesteps      | 1156000     |
| train/                  |             |
|    approx_kl            | 0.005319714 |
|    clip_fraction        | 0.0356      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.87       |
|    explained_variance   | 0.915       |
|    learning_rate        | 4.39e-05    |
|    loss                 | -0.00835    |
|    n_updates            | 5640        |
|    policy_gradient_loss | -0.00359    |
|    std                  | 0.618       |
|    value_loss           | 0.0209      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1157000, episode_reward=0.63 +/- 2.52
Episode length: 283.40 +/- 33.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 283      |
|    mean_reward     | 0.63     |
| time/              |          |
|    total_timesteps | 1157000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 616     |
|    iterations      | 565     |
|    time_elapsed    | 1875    |
|    total_timesteps | 1157120 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1158000, episode_reward=1.55 +/- 1.78
Episode length: 276.80 +/- 46.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 277          |
|    mean_reward          | 1.55         |
| time/                   |              |
|    total_timesteps      | 1158000      |
| train/                  |              |
|    approx_kl            | 0.0062603126 |
|    clip_fraction        | 0.0573       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.87        |
|    explained_variance   | 0.961        |
|    learning_rate        | 4.39e-05     |
|    loss                 | -0.00629     |
|    n_updates            | 5650         |
|    policy_gradient_loss | -0.00698     |
|    std                  | 0.617        |
|    value_loss           | 0.0252       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1159000, episode_reward=0.39 +/- 2.54
Episode length: 282.80 +/- 34.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 283      |
|    mean_reward     | 0.386    |
| time/              |          |
|    total_timesteps | 1159000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 617     |
|    iterations      | 566     |
|    time_elapsed    | 1878    |
|    total_timesteps | 1159168 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1160000, episode_reward=1.80 +/- 2.85
Episode length: 256.40 +/- 54.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 256          |
|    mean_reward          | 1.8          |
| time/                   |              |
|    total_timesteps      | 1160000      |
| train/                  |              |
|    approx_kl            | 0.0060258443 |
|    clip_fraction        | 0.0616       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.87        |
|    explained_variance   | 0.893        |
|    learning_rate        | 4.39e-05     |
|    loss                 | 0.0717       |
|    n_updates            | 5660         |
|    policy_gradient_loss | -0.00858     |
|    std                  | 0.616        |
|    value_loss           | 0.0655       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1161000, episode_reward=0.25 +/- 2.54
Episode length: 270.40 +/- 59.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 270      |
|    mean_reward     | 0.252    |
| time/              |          |
|    total_timesteps | 1161000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 617     |
|    iterations      | 567     |
|    time_elapsed    | 1881    |
|    total_timesteps | 1161216 |
--------------------------------
Eval num_timesteps=1162000, episode_reward=-0.54 +/- 0.59
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.536       |
| time/                   |              |
|    total_timesteps      | 1162000      |
| train/                  |              |
|    approx_kl            | 0.0023865215 |
|    clip_fraction        | 0.00952      |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.87        |
|    explained_variance   | 0.911        |
|    learning_rate        | 4.39e-05     |
|    loss                 | -0.0115      |
|    n_updates            | 5670         |
|    policy_gradient_loss | -0.00213     |
|    std                  | 0.616        |
|    value_loss           | 0.0418       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1163000, episode_reward=3.10 +/- 2.71
Episode length: 227.60 +/- 67.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 228      |
|    mean_reward     | 3.1      |
| time/              |          |
|    total_timesteps | 1163000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 617     |
|    iterations      | 568     |
|    time_elapsed    | 1884    |
|    total_timesteps | 1163264 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1164000, episode_reward=3.13 +/- 2.67
Episode length: 216.60 +/- 71.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 217          |
|    mean_reward          | 3.13         |
| time/                   |              |
|    total_timesteps      | 1164000      |
| train/                  |              |
|    approx_kl            | 0.0044027455 |
|    clip_fraction        | 0.0271       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.87        |
|    explained_variance   | 0.757        |
|    learning_rate        | 4.4e-05      |
|    loss                 | 0.0163       |
|    n_updates            | 5680         |
|    policy_gradient_loss | -0.00295     |
|    std                  | 0.615        |
|    value_loss           | 0.103        |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1165000, episode_reward=1.48 +/- 3.04
Episode length: 241.20 +/- 72.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 241      |
|    mean_reward     | 1.48     |
| time/              |          |
|    total_timesteps | 1165000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 617     |
|    iterations      | 569     |
|    time_elapsed    | 1887    |
|    total_timesteps | 1165312 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1166000, episode_reward=0.20 +/- 2.62
Episode length: 277.20 +/- 45.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 277         |
|    mean_reward          | 0.204       |
| time/                   |             |
|    total_timesteps      | 1166000     |
| train/                  |             |
|    approx_kl            | 0.005417088 |
|    clip_fraction        | 0.0497      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.86       |
|    explained_variance   | 0.965       |
|    learning_rate        | 4.4e-05     |
|    loss                 | 0.0349      |
|    n_updates            | 5690        |
|    policy_gradient_loss | -0.00721    |
|    std                  | 0.615       |
|    value_loss           | 0.0365      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1167000, episode_reward=3.03 +/- 2.72
Episode length: 216.60 +/- 69.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 217      |
|    mean_reward     | 3.03     |
| time/              |          |
|    total_timesteps | 1167000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 617     |
|    iterations      | 570     |
|    time_elapsed    | 1890    |
|    total_timesteps | 1167360 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1168000, episode_reward=2.97 +/- 2.86
Episode length: 228.40 +/- 60.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 228          |
|    mean_reward          | 2.97         |
| time/                   |              |
|    total_timesteps      | 1168000      |
| train/                  |              |
|    approx_kl            | 0.0059362855 |
|    clip_fraction        | 0.0538       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.86        |
|    explained_variance   | 0.982        |
|    learning_rate        | 4.4e-05      |
|    loss                 | 0.00396      |
|    n_updates            | 5700         |
|    policy_gradient_loss | -0.00622     |
|    std                  | 0.614        |
|    value_loss           | 0.0137       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=1169000, episode_reward=1.80 +/- 2.82
Episode length: 241.80 +/- 71.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 242      |
|    mean_reward     | 1.8      |
| time/              |          |
|    total_timesteps | 1169000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 617     |
|    iterations      | 571     |
|    time_elapsed    | 1894    |
|    total_timesteps | 1169408 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1170000, episode_reward=0.44 +/- 2.44
Episode length: 274.00 +/- 52.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 274         |
|    mean_reward          | 0.444       |
| time/                   |             |
|    total_timesteps      | 1170000     |
| train/                  |             |
|    approx_kl            | 0.004874804 |
|    clip_fraction        | 0.0355      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.86       |
|    explained_variance   | 0.84        |
|    learning_rate        | 4.4e-05     |
|    loss                 | -0.00846    |
|    n_updates            | 5710        |
|    policy_gradient_loss | -0.00325    |
|    std                  | 0.615       |
|    value_loss           | 0.0168      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1171000, episode_reward=0.54 +/- 2.37
Episode length: 273.00 +/- 54.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 273      |
|    mean_reward     | 0.541    |
| time/              |          |
|    total_timesteps | 1171000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 617     |
|    iterations      | 572     |
|    time_elapsed    | 1897    |
|    total_timesteps | 1171456 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1172000, episode_reward=3.07 +/- 2.79
Episode length: 226.20 +/- 64.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 226          |
|    mean_reward          | 3.07         |
| time/                   |              |
|    total_timesteps      | 1172000      |
| train/                  |              |
|    approx_kl            | 0.0051288963 |
|    clip_fraction        | 0.0542       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.87        |
|    explained_variance   | 0.96         |
|    learning_rate        | 4.41e-05     |
|    loss                 | -0.00543     |
|    n_updates            | 5720         |
|    policy_gradient_loss | -0.00666     |
|    std                  | 0.615        |
|    value_loss           | 0.00989      |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1173000, episode_reward=4.06 +/- 2.53
Episode length: 229.20 +/- 48.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 229      |
|    mean_reward     | 4.06     |
| time/              |          |
|    total_timesteps | 1173000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 617     |
|    iterations      | 573     |
|    time_elapsed    | 1900    |
|    total_timesteps | 1173504 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1174000, episode_reward=3.00 +/- 2.70
Episode length: 222.40 +/- 63.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 222          |
|    mean_reward          | 3            |
| time/                   |              |
|    total_timesteps      | 1174000      |
| train/                  |              |
|    approx_kl            | 0.0033821552 |
|    clip_fraction        | 0.0249       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.87        |
|    explained_variance   | 0.901        |
|    learning_rate        | 4.41e-05     |
|    loss                 | 0.00277      |
|    n_updates            | 5730         |
|    policy_gradient_loss | -0.00242     |
|    std                  | 0.616        |
|    value_loss           | 0.046        |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1175000, episode_reward=2.95 +/- 2.75
Episode length: 218.00 +/- 67.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 218      |
|    mean_reward     | 2.95     |
| time/              |          |
|    total_timesteps | 1175000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 617     |
|    iterations      | 574     |
|    time_elapsed    | 1903    |
|    total_timesteps | 1175552 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1176000, episode_reward=2.81 +/- 2.96
Episode length: 215.00 +/- 69.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 215         |
|    mean_reward          | 2.81        |
| time/                   |             |
|    total_timesteps      | 1176000     |
| train/                  |             |
|    approx_kl            | 0.005058185 |
|    clip_fraction        | 0.0468      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.87       |
|    explained_variance   | 0.918       |
|    learning_rate        | 4.41e-05    |
|    loss                 | -0.0292     |
|    n_updates            | 5740        |
|    policy_gradient_loss | -0.00842    |
|    std                  | 0.616       |
|    value_loss           | 0.0203      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1177000, episode_reward=1.61 +/- 3.19
Episode length: 268.60 +/- 40.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 269      |
|    mean_reward     | 1.61     |
| time/              |          |
|    total_timesteps | 1177000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 617     |
|    iterations      | 575     |
|    time_elapsed    | 1906    |
|    total_timesteps | 1177600 |
--------------------------------
box reached target
Eval num_timesteps=1178000, episode_reward=0.34 +/- 2.39
Episode length: 287.20 +/- 25.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 287          |
|    mean_reward          | 0.335        |
| time/                   |              |
|    total_timesteps      | 1178000      |
| train/                  |              |
|    approx_kl            | 0.0041378653 |
|    clip_fraction        | 0.0331       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.87        |
|    explained_variance   | 0.953        |
|    learning_rate        | 4.41e-05     |
|    loss                 | -0.0152      |
|    n_updates            | 5750         |
|    policy_gradient_loss | -0.00556     |
|    std                  | 0.617        |
|    value_loss           | 0.0228       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1179000, episode_reward=0.62 +/- 2.38
Episode length: 272.00 +/- 56.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 272      |
|    mean_reward     | 0.622    |
| time/              |          |
|    total_timesteps | 1179000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 617     |
|    iterations      | 576     |
|    time_elapsed    | 1909    |
|    total_timesteps | 1179648 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1180000, episode_reward=2.88 +/- 2.88
Episode length: 235.40 +/- 57.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 235         |
|    mean_reward          | 2.88        |
| time/                   |             |
|    total_timesteps      | 1180000     |
| train/                  |             |
|    approx_kl            | 0.006731096 |
|    clip_fraction        | 0.0535      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.87       |
|    explained_variance   | 0.865       |
|    learning_rate        | 4.42e-05    |
|    loss                 | 0.00729     |
|    n_updates            | 5760        |
|    policy_gradient_loss | -0.0051     |
|    std                  | 0.617       |
|    value_loss           | 0.0744      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1181000, episode_reward=2.91 +/- 2.76
Episode length: 217.40 +/- 67.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 217      |
|    mean_reward     | 2.91     |
| time/              |          |
|    total_timesteps | 1181000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 617     |
|    iterations      | 577     |
|    time_elapsed    | 1912    |
|    total_timesteps | 1181696 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1182000, episode_reward=1.82 +/- 2.75
Episode length: 236.80 +/- 77.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 237         |
|    mean_reward          | 1.82        |
| time/                   |             |
|    total_timesteps      | 1182000     |
| train/                  |             |
|    approx_kl            | 0.003911838 |
|    clip_fraction        | 0.0224      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.87       |
|    explained_variance   | 0.861       |
|    learning_rate        | 4.42e-05    |
|    loss                 | -0.0192     |
|    n_updates            | 5770        |
|    policy_gradient_loss | -0.00371    |
|    std                  | 0.619       |
|    value_loss           | 0.00877     |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=1183000, episode_reward=2.15 +/- 2.71
Episode length: 252.60 +/- 59.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 253      |
|    mean_reward     | 2.15     |
| time/              |          |
|    total_timesteps | 1183000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 618     |
|    iterations      | 578     |
|    time_elapsed    | 1915    |
|    total_timesteps | 1183744 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1184000, episode_reward=2.97 +/- 2.88
Episode length: 232.00 +/- 66.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 232         |
|    mean_reward          | 2.97        |
| time/                   |             |
|    total_timesteps      | 1184000     |
| train/                  |             |
|    approx_kl            | 0.003251439 |
|    clip_fraction        | 0.0166      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.87       |
|    explained_variance   | 0.97        |
|    learning_rate        | 4.42e-05    |
|    loss                 | 0.00187     |
|    n_updates            | 5780        |
|    policy_gradient_loss | -0.00225    |
|    std                  | 0.617       |
|    value_loss           | 0.00854     |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=1185000, episode_reward=1.07 +/- 2.16
Episode length: 272.20 +/- 55.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 272      |
|    mean_reward     | 1.07     |
| time/              |          |
|    total_timesteps | 1185000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 618     |
|    iterations      | 579     |
|    time_elapsed    | 1918    |
|    total_timesteps | 1185792 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1186000, episode_reward=2.85 +/- 3.10
Episode length: 242.00 +/- 48.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 242          |
|    mean_reward          | 2.85         |
| time/                   |              |
|    total_timesteps      | 1186000      |
| train/                  |              |
|    approx_kl            | 0.0041731177 |
|    clip_fraction        | 0.0292       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.87        |
|    explained_variance   | 0.936        |
|    learning_rate        | 4.42e-05     |
|    loss                 | 0.0112       |
|    n_updates            | 5790         |
|    policy_gradient_loss | -0.00435     |
|    std                  | 0.615        |
|    value_loss           | 0.0169       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=1187000, episode_reward=2.01 +/- 2.63
Episode length: 253.20 +/- 59.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 253      |
|    mean_reward     | 2.01     |
| time/              |          |
|    total_timesteps | 1187000  |
---------------------------------
box reached target
box reached target
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 618     |
|    iterations      | 580     |
|    time_elapsed    | 1921    |
|    total_timesteps | 1187840 |
--------------------------------
Eval num_timesteps=1188000, episode_reward=-0.34 +/- 0.66
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.343      |
| time/                   |             |
|    total_timesteps      | 1188000     |
| train/                  |             |
|    approx_kl            | 0.006636326 |
|    clip_fraction        | 0.0643      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.87       |
|    explained_variance   | 0.954       |
|    learning_rate        | 4.43e-05    |
|    loss                 | 0.0198      |
|    n_updates            | 5800        |
|    policy_gradient_loss | -0.00908    |
|    std                  | 0.615       |
|    value_loss           | 0.0257      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=1189000, episode_reward=1.53 +/- 3.10
Episode length: 252.40 +/- 58.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 252      |
|    mean_reward     | 1.53     |
| time/              |          |
|    total_timesteps | 1189000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 618     |
|    iterations      | 581     |
|    time_elapsed    | 1924    |
|    total_timesteps | 1189888 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1190000, episode_reward=1.47 +/- 3.12
Episode length: 254.60 +/- 58.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 255         |
|    mean_reward          | 1.47        |
| time/                   |             |
|    total_timesteps      | 1190000     |
| train/                  |             |
|    approx_kl            | 0.005125883 |
|    clip_fraction        | 0.0325      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.86       |
|    explained_variance   | 0.941       |
|    learning_rate        | 4.43e-05    |
|    loss                 | 0.0041      |
|    n_updates            | 5810        |
|    policy_gradient_loss | -0.00451    |
|    std                  | 0.614       |
|    value_loss           | 0.00943     |
-----------------------------------------
box reached target
Eval num_timesteps=1191000, episode_reward=0.41 +/- 2.37
Episode length: 270.00 +/- 60.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 270      |
|    mean_reward     | 0.41     |
| time/              |          |
|    total_timesteps | 1191000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 618     |
|    iterations      | 582     |
|    time_elapsed    | 1927    |
|    total_timesteps | 1191936 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1192000, episode_reward=0.51 +/- 2.47
Episode length: 281.40 +/- 37.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 281          |
|    mean_reward          | 0.505        |
| time/                   |              |
|    total_timesteps      | 1192000      |
| train/                  |              |
|    approx_kl            | 0.0052437214 |
|    clip_fraction        | 0.0304       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.86        |
|    explained_variance   | 0.981        |
|    learning_rate        | 4.43e-05     |
|    loss                 | 0.0141       |
|    n_updates            | 5820         |
|    policy_gradient_loss | -0.00406     |
|    std                  | 0.615        |
|    value_loss           | 0.0136       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=1193000, episode_reward=1.78 +/- 2.84
Episode length: 262.00 +/- 48.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 262      |
|    mean_reward     | 1.78     |
| time/              |          |
|    total_timesteps | 1193000  |
---------------------------------
box reached target
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 618     |
|    iterations      | 583     |
|    time_elapsed    | 1930    |
|    total_timesteps | 1193984 |
--------------------------------
box reached target
Eval num_timesteps=1194000, episode_reward=0.53 +/- 2.53
Episode length: 273.00 +/- 54.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 273          |
|    mean_reward          | 0.529        |
| time/                   |              |
|    total_timesteps      | 1194000      |
| train/                  |              |
|    approx_kl            | 0.0032589105 |
|    clip_fraction        | 0.02         |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.86        |
|    explained_variance   | 0.917        |
|    learning_rate        | 4.43e-05     |
|    loss                 | 0.00972      |
|    n_updates            | 5830         |
|    policy_gradient_loss | -0.00162     |
|    std                  | 0.615        |
|    value_loss           | 0.0282       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1195000, episode_reward=1.56 +/- 3.14
Episode length: 261.40 +/- 50.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 261      |
|    mean_reward     | 1.56     |
| time/              |          |
|    total_timesteps | 1195000  |
---------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1196000, episode_reward=1.75 +/- 2.87
Episode length: 247.00 +/- 64.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 247      |
|    mean_reward     | 1.75     |
| time/              |          |
|    total_timesteps | 1196000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 618     |
|    iterations      | 584     |
|    time_elapsed    | 1934    |
|    total_timesteps | 1196032 |
--------------------------------
box reached target
Eval num_timesteps=1197000, episode_reward=0.40 +/- 2.36
Episode length: 270.40 +/- 59.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 270         |
|    mean_reward          | 0.404       |
| time/                   |             |
|    total_timesteps      | 1197000     |
| train/                  |             |
|    approx_kl            | 0.004516933 |
|    clip_fraction        | 0.0307      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.86       |
|    explained_variance   | 0.915       |
|    learning_rate        | 4.44e-05    |
|    loss                 | -0.0132     |
|    n_updates            | 5840        |
|    policy_gradient_loss | -0.00447    |
|    std                  | 0.614       |
|    value_loss           | 0.0546      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1198000, episode_reward=2.00 +/- 2.93
Episode length: 268.00 +/- 47.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 268      |
|    mean_reward     | 2        |
| time/              |          |
|    total_timesteps | 1198000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 618     |
|    iterations      | 585     |
|    time_elapsed    | 1937    |
|    total_timesteps | 1198080 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1199000, episode_reward=4.41 +/- 1.99
Episode length: 233.00 +/- 37.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 233         |
|    mean_reward          | 4.41        |
| time/                   |             |
|    total_timesteps      | 1199000     |
| train/                  |             |
|    approx_kl            | 0.005066794 |
|    clip_fraction        | 0.0311      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.86       |
|    explained_variance   | 0.872       |
|    learning_rate        | 4.44e-05    |
|    loss                 | 0.00478     |
|    n_updates            | 5850        |
|    policy_gradient_loss | -0.00456    |
|    std                  | 0.613       |
|    value_loss           | 0.0299      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1200000, episode_reward=2.77 +/- 3.13
Episode length: 214.20 +/- 71.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 214      |
|    mean_reward     | 2.77     |
| time/              |          |
|    total_timesteps | 1200000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 618     |
|    iterations      | 586     |
|    time_elapsed    | 1940    |
|    total_timesteps | 1200128 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1201000, episode_reward=0.49 +/- 2.50
Episode length: 275.40 +/- 49.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 275          |
|    mean_reward          | 0.492        |
| time/                   |              |
|    total_timesteps      | 1201000      |
| train/                  |              |
|    approx_kl            | 0.0052260645 |
|    clip_fraction        | 0.0326       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.86        |
|    explained_variance   | 0.902        |
|    learning_rate        | 4.44e-05     |
|    loss                 | -0.0238      |
|    n_updates            | 5860         |
|    policy_gradient_loss | -0.00635     |
|    std                  | 0.612        |
|    value_loss           | 0.0289       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1202000, episode_reward=4.27 +/- 1.90
Episode length: 192.80 +/- 55.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 193      |
|    mean_reward     | 4.27     |
| time/              |          |
|    total_timesteps | 1202000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 618     |
|    iterations      | 587     |
|    time_elapsed    | 1943    |
|    total_timesteps | 1202176 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1203000, episode_reward=1.41 +/- 3.09
Episode length: 235.60 +/- 78.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 236         |
|    mean_reward          | 1.41        |
| time/                   |             |
|    total_timesteps      | 1203000     |
| train/                  |             |
|    approx_kl            | 0.006837852 |
|    clip_fraction        | 0.0487      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.85       |
|    explained_variance   | 0.971       |
|    learning_rate        | 4.44e-05    |
|    loss                 | -0.0077     |
|    n_updates            | 5870        |
|    policy_gradient_loss | -0.00972    |
|    std                  | 0.611       |
|    value_loss           | 0.0233      |
-----------------------------------------
box reached target
Eval num_timesteps=1204000, episode_reward=0.66 +/- 2.43
Episode length: 283.60 +/- 32.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 284      |
|    mean_reward     | 0.664    |
| time/              |          |
|    total_timesteps | 1204000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 618     |
|    iterations      | 588     |
|    time_elapsed    | 1946    |
|    total_timesteps | 1204224 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1205000, episode_reward=1.45 +/- 3.14
Episode length: 250.00 +/- 64.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 250          |
|    mean_reward          | 1.45         |
| time/                   |              |
|    total_timesteps      | 1205000      |
| train/                  |              |
|    approx_kl            | 0.0028119348 |
|    clip_fraction        | 0.0255       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.85        |
|    explained_variance   | 0.858        |
|    learning_rate        | 4.45e-05     |
|    loss                 | 0.00795      |
|    n_updates            | 5880         |
|    policy_gradient_loss | -0.00338     |
|    std                  | 0.61         |
|    value_loss           | 0.0467       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1206000, episode_reward=2.63 +/- 3.15
Episode length: 218.80 +/- 66.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 219      |
|    mean_reward     | 2.63     |
| time/              |          |
|    total_timesteps | 1206000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 618     |
|    iterations      | 589     |
|    time_elapsed    | 1949    |
|    total_timesteps | 1206272 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1207000, episode_reward=0.48 +/- 2.46
Episode length: 273.80 +/- 52.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 274         |
|    mean_reward          | 0.476       |
| time/                   |             |
|    total_timesteps      | 1207000     |
| train/                  |             |
|    approx_kl            | 0.003591882 |
|    clip_fraction        | 0.024       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.85       |
|    explained_variance   | 0.817       |
|    learning_rate        | 4.45e-05    |
|    loss                 | 0.0409      |
|    n_updates            | 5890        |
|    policy_gradient_loss | -0.00383    |
|    std                  | 0.609       |
|    value_loss           | 0.064       |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1208000, episode_reward=3.91 +/- 2.71
Episode length: 199.60 +/- 50.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 200      |
|    mean_reward     | 3.91     |
| time/              |          |
|    total_timesteps | 1208000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 618     |
|    iterations      | 590     |
|    time_elapsed    | 1952    |
|    total_timesteps | 1208320 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1209000, episode_reward=0.46 +/- 2.32
Episode length: 272.00 +/- 56.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 272         |
|    mean_reward          | 0.461       |
| time/                   |             |
|    total_timesteps      | 1209000     |
| train/                  |             |
|    approx_kl            | 0.004697713 |
|    clip_fraction        | 0.0215      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.85       |
|    explained_variance   | 0.9         |
|    learning_rate        | 4.45e-05    |
|    loss                 | -0.00433    |
|    n_updates            | 5900        |
|    policy_gradient_loss | -0.00282    |
|    std                  | 0.61        |
|    value_loss           | 0.027       |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=1210000, episode_reward=0.73 +/- 2.34
Episode length: 273.40 +/- 53.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 273      |
|    mean_reward     | 0.729    |
| time/              |          |
|    total_timesteps | 1210000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 618     |
|    iterations      | 591     |
|    time_elapsed    | 1955    |
|    total_timesteps | 1210368 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1211000, episode_reward=1.82 +/- 2.95
Episode length: 256.40 +/- 53.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 256          |
|    mean_reward          | 1.82         |
| time/                   |              |
|    total_timesteps      | 1211000      |
| train/                  |              |
|    approx_kl            | 0.0038687359 |
|    clip_fraction        | 0.0354       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.85        |
|    explained_variance   | 0.873        |
|    learning_rate        | 4.45e-05     |
|    loss                 | 0.000405     |
|    n_updates            | 5910         |
|    policy_gradient_loss | -0.00544     |
|    std                  | 0.61         |
|    value_loss           | 0.0523       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1212000, episode_reward=2.67 +/- 2.99
Episode length: 215.60 +/- 71.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 216      |
|    mean_reward     | 2.67     |
| time/              |          |
|    total_timesteps | 1212000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 619     |
|    iterations      | 592     |
|    time_elapsed    | 1958    |
|    total_timesteps | 1212416 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1213000, episode_reward=0.76 +/- 2.33
Episode length: 280.40 +/- 39.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 280         |
|    mean_reward          | 0.759       |
| time/                   |             |
|    total_timesteps      | 1213000     |
| train/                  |             |
|    approx_kl            | 0.004228673 |
|    clip_fraction        | 0.0322      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.85       |
|    explained_variance   | 0.92        |
|    learning_rate        | 4.45e-05    |
|    loss                 | -0.00099    |
|    n_updates            | 5920        |
|    policy_gradient_loss | -0.00468    |
|    std                  | 0.611       |
|    value_loss           | 0.033       |
-----------------------------------------
box reached target
Eval num_timesteps=1214000, episode_reward=0.54 +/- 2.33
Episode length: 277.20 +/- 45.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 277      |
|    mean_reward     | 0.542    |
| time/              |          |
|    total_timesteps | 1214000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 619     |
|    iterations      | 593     |
|    time_elapsed    | 1961    |
|    total_timesteps | 1214464 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1215000, episode_reward=3.03 +/- 2.87
Episode length: 234.80 +/- 54.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 235          |
|    mean_reward          | 3.03         |
| time/                   |              |
|    total_timesteps      | 1215000      |
| train/                  |              |
|    approx_kl            | 0.0043647424 |
|    clip_fraction        | 0.0254       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.86        |
|    explained_variance   | 0.942        |
|    learning_rate        | 4.46e-05     |
|    loss                 | -0.0197      |
|    n_updates            | 5930         |
|    policy_gradient_loss | -0.004       |
|    std                  | 0.613        |
|    value_loss           | 0.0315       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1216000, episode_reward=1.50 +/- 3.07
Episode length: 247.20 +/- 64.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 247      |
|    mean_reward     | 1.5      |
| time/              |          |
|    total_timesteps | 1216000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 619     |
|    iterations      | 594     |
|    time_elapsed    | 1964    |
|    total_timesteps | 1216512 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1217000, episode_reward=2.07 +/- 2.73
Episode length: 258.40 +/- 53.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 258          |
|    mean_reward          | 2.07         |
| time/                   |              |
|    total_timesteps      | 1217000      |
| train/                  |              |
|    approx_kl            | 0.0055517326 |
|    clip_fraction        | 0.0374       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.86        |
|    explained_variance   | 0.948        |
|    learning_rate        | 4.46e-05     |
|    loss                 | 0.0141       |
|    n_updates            | 5940         |
|    policy_gradient_loss | -0.0053      |
|    std                  | 0.612        |
|    value_loss           | 0.031        |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1218000, episode_reward=1.71 +/- 3.08
Episode length: 264.20 +/- 43.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 264      |
|    mean_reward     | 1.71     |
| time/              |          |
|    total_timesteps | 1218000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 619     |
|    iterations      | 595     |
|    time_elapsed    | 1967    |
|    total_timesteps | 1218560 |
--------------------------------
box reached target
Eval num_timesteps=1219000, episode_reward=1.20 +/- 2.23
Episode length: 281.60 +/- 36.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 282          |
|    mean_reward          | 1.2          |
| time/                   |              |
|    total_timesteps      | 1219000      |
| train/                  |              |
|    approx_kl            | 0.0068281284 |
|    clip_fraction        | 0.072        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.86        |
|    explained_variance   | 0.942        |
|    learning_rate        | 4.46e-05     |
|    loss                 | -0.00163     |
|    n_updates            | 5950         |
|    policy_gradient_loss | -0.00941     |
|    std                  | 0.613        |
|    value_loss           | 0.0149       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=1220000, episode_reward=2.21 +/- 2.80
Episode length: 258.00 +/- 56.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 258      |
|    mean_reward     | 2.21     |
| time/              |          |
|    total_timesteps | 1220000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 619     |
|    iterations      | 596     |
|    time_elapsed    | 1970    |
|    total_timesteps | 1220608 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1221000, episode_reward=1.43 +/- 3.04
Episode length: 249.60 +/- 61.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 250          |
|    mean_reward          | 1.43         |
| time/                   |              |
|    total_timesteps      | 1221000      |
| train/                  |              |
|    approx_kl            | 0.0046749194 |
|    clip_fraction        | 0.0368       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.86        |
|    explained_variance   | 0.918        |
|    learning_rate        | 4.46e-05     |
|    loss                 | -0.0337      |
|    n_updates            | 5960         |
|    policy_gradient_loss | -0.00543     |
|    std                  | 0.613        |
|    value_loss           | 0.004        |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=1222000, episode_reward=0.90 +/- 2.29
Episode length: 274.80 +/- 50.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 275      |
|    mean_reward     | 0.902    |
| time/              |          |
|    total_timesteps | 1222000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 619     |
|    iterations      | 597     |
|    time_elapsed    | 1974    |
|    total_timesteps | 1222656 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1223000, episode_reward=2.92 +/- 2.84
Episode length: 216.00 +/- 69.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 216          |
|    mean_reward          | 2.92         |
| time/                   |              |
|    total_timesteps      | 1223000      |
| train/                  |              |
|    approx_kl            | 0.0051631583 |
|    clip_fraction        | 0.0302       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.86        |
|    explained_variance   | 0.945        |
|    learning_rate        | 4.47e-05     |
|    loss                 | 0.0306       |
|    n_updates            | 5970         |
|    policy_gradient_loss | -0.00426     |
|    std                  | 0.613        |
|    value_loss           | 0.0231       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1224000, episode_reward=3.16 +/- 2.70
Episode length: 221.40 +/- 64.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 221      |
|    mean_reward     | 3.16     |
| time/              |          |
|    total_timesteps | 1224000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 619     |
|    iterations      | 598     |
|    time_elapsed    | 1976    |
|    total_timesteps | 1224704 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1225000, episode_reward=2.10 +/- 2.80
Episode length: 258.00 +/- 51.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 258         |
|    mean_reward          | 2.1         |
| time/                   |             |
|    total_timesteps      | 1225000     |
| train/                  |             |
|    approx_kl            | 0.005439045 |
|    clip_fraction        | 0.0266      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.86       |
|    explained_variance   | 0.944       |
|    learning_rate        | 4.47e-05    |
|    loss                 | -0.00795    |
|    n_updates            | 5980        |
|    policy_gradient_loss | -0.00451    |
|    std                  | 0.613       |
|    value_loss           | 0.0117      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=1226000, episode_reward=1.51 +/- 3.08
Episode length: 261.60 +/- 47.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 262      |
|    mean_reward     | 1.51     |
| time/              |          |
|    total_timesteps | 1226000  |
---------------------------------
box reached target
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 619     |
|    iterations      | 599     |
|    time_elapsed    | 1979    |
|    total_timesteps | 1226752 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1227000, episode_reward=3.01 +/- 2.74
Episode length: 226.40 +/- 61.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 226          |
|    mean_reward          | 3.01         |
| time/                   |              |
|    total_timesteps      | 1227000      |
| train/                  |              |
|    approx_kl            | 0.0052042445 |
|    clip_fraction        | 0.025        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.85        |
|    explained_variance   | 0.983        |
|    learning_rate        | 4.47e-05     |
|    loss                 | 0.00193      |
|    n_updates            | 5990         |
|    policy_gradient_loss | -0.0037      |
|    std                  | 0.61         |
|    value_loss           | 0.0127       |
------------------------------------------
box reached target
Eval num_timesteps=1228000, episode_reward=0.21 +/- 2.47
Episode length: 274.00 +/- 52.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 274      |
|    mean_reward     | 0.211    |
| time/              |          |
|    total_timesteps | 1228000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 619     |
|    iterations      | 600     |
|    time_elapsed    | 1983    |
|    total_timesteps | 1228800 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1229000, episode_reward=2.78 +/- 3.05
Episode length: 227.20 +/- 60.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 227         |
|    mean_reward          | 2.78        |
| time/                   |             |
|    total_timesteps      | 1229000     |
| train/                  |             |
|    approx_kl            | 0.004438523 |
|    clip_fraction        | 0.0279      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.85       |
|    explained_variance   | 0.917       |
|    learning_rate        | 4.47e-05    |
|    loss                 | 0.000693    |
|    n_updates            | 6000        |
|    policy_gradient_loss | -0.0062     |
|    std                  | 0.609       |
|    value_loss           | 0.032       |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=1230000, episode_reward=1.84 +/- 2.97
Episode length: 249.20 +/- 64.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 249      |
|    mean_reward     | 1.84     |
| time/              |          |
|    total_timesteps | 1230000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 619     |
|    iterations      | 601     |
|    time_elapsed    | 1985    |
|    total_timesteps | 1230848 |
--------------------------------
box reached target
Eval num_timesteps=1231000, episode_reward=-0.78 +/- 0.71
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.775       |
| time/                   |              |
|    total_timesteps      | 1231000      |
| train/                  |              |
|    approx_kl            | 0.0040273764 |
|    clip_fraction        | 0.0345       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.85        |
|    explained_variance   | 0.912        |
|    learning_rate        | 4.48e-05     |
|    loss                 | -0.0166      |
|    n_updates            | 6010         |
|    policy_gradient_loss | -0.00568     |
|    std                  | 0.611        |
|    value_loss           | 0.02         |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=1232000, episode_reward=2.15 +/- 2.70
Episode length: 252.20 +/- 62.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 252      |
|    mean_reward     | 2.15     |
| time/              |          |
|    total_timesteps | 1232000  |
---------------------------------
box reached target
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 619     |
|    iterations      | 602     |
|    time_elapsed    | 1989    |
|    total_timesteps | 1232896 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1233000, episode_reward=1.54 +/- 3.05
Episode length: 243.60 +/- 69.43
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 244        |
|    mean_reward          | 1.54       |
| time/                   |            |
|    total_timesteps      | 1233000    |
| train/                  |            |
|    approx_kl            | 0.00623357 |
|    clip_fraction        | 0.0464     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.85      |
|    explained_variance   | 0.927      |
|    learning_rate        | 4.48e-05   |
|    loss                 | -0.00243   |
|    n_updates            | 6020       |
|    policy_gradient_loss | -0.0043    |
|    std                  | 0.609      |
|    value_loss           | 0.0208     |
----------------------------------------
box reached target
box reached target
Eval num_timesteps=1234000, episode_reward=0.29 +/- 2.46
Episode length: 273.00 +/- 54.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 273      |
|    mean_reward     | 0.288    |
| time/              |          |
|    total_timesteps | 1234000  |
---------------------------------
box reached target
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 619     |
|    iterations      | 603     |
|    time_elapsed    | 1992    |
|    total_timesteps | 1234944 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1235000, episode_reward=2.64 +/- 3.17
Episode length: 226.60 +/- 60.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 227          |
|    mean_reward          | 2.64         |
| time/                   |              |
|    total_timesteps      | 1235000      |
| train/                  |              |
|    approx_kl            | 0.0042766742 |
|    clip_fraction        | 0.0386       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.85        |
|    explained_variance   | 0.978        |
|    learning_rate        | 4.48e-05     |
|    loss                 | -0.00667     |
|    n_updates            | 6030         |
|    policy_gradient_loss | -0.00534     |
|    std                  | 0.608        |
|    value_loss           | 0.0133       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1236000, episode_reward=1.79 +/- 2.92
Episode length: 248.60 +/- 62.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 249      |
|    mean_reward     | 1.79     |
| time/              |          |
|    total_timesteps | 1236000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 620     |
|    iterations      | 604     |
|    time_elapsed    | 1995    |
|    total_timesteps | 1236992 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1237000, episode_reward=3.13 +/- 2.71
Episode length: 218.20 +/- 67.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 218          |
|    mean_reward          | 3.13         |
| time/                   |              |
|    total_timesteps      | 1237000      |
| train/                  |              |
|    approx_kl            | 0.0029978198 |
|    clip_fraction        | 0.00996      |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.84        |
|    explained_variance   | 0.921        |
|    learning_rate        | 4.48e-05     |
|    loss                 | 0.0334       |
|    n_updates            | 6040         |
|    policy_gradient_loss | -0.0017      |
|    std                  | 0.609        |
|    value_loss           | 0.0794       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1238000, episode_reward=0.56 +/- 2.33
Episode length: 269.20 +/- 61.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 269      |
|    mean_reward     | 0.559    |
| time/              |          |
|    total_timesteps | 1238000  |
---------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1239000, episode_reward=1.41 +/- 3.04
Episode length: 246.00 +/- 67.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 246      |
|    mean_reward     | 1.41     |
| time/              |          |
|    total_timesteps | 1239000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 619     |
|    iterations      | 605     |
|    time_elapsed    | 1998    |
|    total_timesteps | 1239040 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1240000, episode_reward=3.21 +/- 2.56
Episode length: 216.00 +/- 70.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 216         |
|    mean_reward          | 3.21        |
| time/                   |             |
|    total_timesteps      | 1240000     |
| train/                  |             |
|    approx_kl            | 0.004568137 |
|    clip_fraction        | 0.0353      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.85       |
|    explained_variance   | 0.882       |
|    learning_rate        | 4.49e-05    |
|    loss                 | 0.0397      |
|    n_updates            | 6050        |
|    policy_gradient_loss | -0.0054     |
|    std                  | 0.61        |
|    value_loss           | 0.0554      |
-----------------------------------------
box reached target
Eval num_timesteps=1241000, episode_reward=0.90 +/- 2.41
Episode length: 282.40 +/- 35.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 282      |
|    mean_reward     | 0.897    |
| time/              |          |
|    total_timesteps | 1241000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 620     |
|    iterations      | 606     |
|    time_elapsed    | 2001    |
|    total_timesteps | 1241088 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1242000, episode_reward=1.76 +/- 2.85
Episode length: 253.40 +/- 57.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 253         |
|    mean_reward          | 1.76        |
| time/                   |             |
|    total_timesteps      | 1242000     |
| train/                  |             |
|    approx_kl            | 0.005046235 |
|    clip_fraction        | 0.0342      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.85       |
|    explained_variance   | 0.916       |
|    learning_rate        | 4.49e-05    |
|    loss                 | 0.00449     |
|    n_updates            | 6060        |
|    policy_gradient_loss | -0.00398    |
|    std                  | 0.609       |
|    value_loss           | 0.0629      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=1243000, episode_reward=0.35 +/- 2.45
Episode length: 274.00 +/- 52.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 274      |
|    mean_reward     | 0.346    |
| time/              |          |
|    total_timesteps | 1243000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 620     |
|    iterations      | 607     |
|    time_elapsed    | 2004    |
|    total_timesteps | 1243136 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1244000, episode_reward=1.44 +/- 2.99
Episode length: 253.40 +/- 57.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 253         |
|    mean_reward          | 1.44        |
| time/                   |             |
|    total_timesteps      | 1244000     |
| train/                  |             |
|    approx_kl            | 0.005393025 |
|    clip_fraction        | 0.0507      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.84       |
|    explained_variance   | 0.919       |
|    learning_rate        | 4.49e-05    |
|    loss                 | 0.00799     |
|    n_updates            | 6070        |
|    policy_gradient_loss | -0.00736    |
|    std                  | 0.608       |
|    value_loss           | 0.0656      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1245000, episode_reward=0.84 +/- 2.28
Episode length: 274.00 +/- 52.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 274      |
|    mean_reward     | 0.84     |
| time/              |          |
|    total_timesteps | 1245000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 620     |
|    iterations      | 608     |
|    time_elapsed    | 2007    |
|    total_timesteps | 1245184 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1246000, episode_reward=1.50 +/- 3.06
Episode length: 234.20 +/- 80.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 234         |
|    mean_reward          | 1.5         |
| time/                   |             |
|    total_timesteps      | 1246000     |
| train/                  |             |
|    approx_kl            | 0.005130971 |
|    clip_fraction        | 0.0273      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.84       |
|    explained_variance   | 0.954       |
|    learning_rate        | 4.49e-05    |
|    loss                 | 0.00868     |
|    n_updates            | 6080        |
|    policy_gradient_loss | -0.00371    |
|    std                  | 0.607       |
|    value_loss           | 0.018       |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1247000, episode_reward=2.67 +/- 3.10
Episode length: 211.00 +/- 72.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 211      |
|    mean_reward     | 2.67     |
| time/              |          |
|    total_timesteps | 1247000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 620     |
|    iterations      | 609     |
|    time_elapsed    | 2010    |
|    total_timesteps | 1247232 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1248000, episode_reward=1.92 +/- 2.98
Episode length: 249.80 +/- 65.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 250          |
|    mean_reward          | 1.92         |
| time/                   |              |
|    total_timesteps      | 1248000      |
| train/                  |              |
|    approx_kl            | 0.0039074244 |
|    clip_fraction        | 0.0183       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.84        |
|    explained_variance   | 0.848        |
|    learning_rate        | 4.5e-05      |
|    loss                 | 0.0371       |
|    n_updates            | 6090         |
|    policy_gradient_loss | -0.00283     |
|    std                  | 0.607        |
|    value_loss           | 0.0869       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1249000, episode_reward=2.72 +/- 3.04
Episode length: 229.60 +/- 62.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 230      |
|    mean_reward     | 2.72     |
| time/              |          |
|    total_timesteps | 1249000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 620     |
|    iterations      | 610     |
|    time_elapsed    | 2013    |
|    total_timesteps | 1249280 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1250000, episode_reward=0.54 +/- 2.37
Episode length: 268.20 +/- 63.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 268          |
|    mean_reward          | 0.536        |
| time/                   |              |
|    total_timesteps      | 1250000      |
| train/                  |              |
|    approx_kl            | 0.0039369417 |
|    clip_fraction        | 0.0266       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.84        |
|    explained_variance   | 0.935        |
|    learning_rate        | 4.5e-05      |
|    loss                 | -0.0046      |
|    n_updates            | 6100         |
|    policy_gradient_loss | -0.00349     |
|    std                  | 0.608        |
|    value_loss           | 0.0375       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1251000, episode_reward=2.79 +/- 3.10
Episode length: 230.60 +/- 58.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 231      |
|    mean_reward     | 2.79     |
| time/              |          |
|    total_timesteps | 1251000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 620     |
|    iterations      | 611     |
|    time_elapsed    | 2016    |
|    total_timesteps | 1251328 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1252000, episode_reward=2.06 +/- 2.62
Episode length: 244.40 +/- 68.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 244          |
|    mean_reward          | 2.06         |
| time/                   |              |
|    total_timesteps      | 1252000      |
| train/                  |              |
|    approx_kl            | 0.0056622764 |
|    clip_fraction        | 0.0443       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.84        |
|    explained_variance   | 0.966        |
|    learning_rate        | 4.5e-05      |
|    loss                 | 0.0154       |
|    n_updates            | 6110         |
|    policy_gradient_loss | -0.00678     |
|    std                  | 0.607        |
|    value_loss           | 0.0231       |
------------------------------------------
box reached target
Eval num_timesteps=1253000, episode_reward=0.89 +/- 2.42
Episode length: 285.60 +/- 28.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 286      |
|    mean_reward     | 0.892    |
| time/              |          |
|    total_timesteps | 1253000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 620     |
|    iterations      | 612     |
|    time_elapsed    | 2019    |
|    total_timesteps | 1253376 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1254000, episode_reward=0.76 +/- 2.35
Episode length: 281.40 +/- 37.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 281         |
|    mean_reward          | 0.758       |
| time/                   |             |
|    total_timesteps      | 1254000     |
| train/                  |             |
|    approx_kl            | 0.003821102 |
|    clip_fraction        | 0.0346      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.84       |
|    explained_variance   | 0.805       |
|    learning_rate        | 4.5e-05     |
|    loss                 | 0.00447     |
|    n_updates            | 6120        |
|    policy_gradient_loss | -0.00589    |
|    std                  | 0.607       |
|    value_loss           | 0.0505      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1255000, episode_reward=4.38 +/- 1.85
Episode length: 213.40 +/- 48.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 213      |
|    mean_reward     | 4.38     |
| time/              |          |
|    total_timesteps | 1255000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 620     |
|    iterations      | 613     |
|    time_elapsed    | 2022    |
|    total_timesteps | 1255424 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1256000, episode_reward=2.78 +/- 3.10
Episode length: 226.00 +/- 65.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 226          |
|    mean_reward          | 2.78         |
| time/                   |              |
|    total_timesteps      | 1256000      |
| train/                  |              |
|    approx_kl            | 0.0061540073 |
|    clip_fraction        | 0.0656       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.84        |
|    explained_variance   | 0.929        |
|    learning_rate        | 4.51e-05     |
|    loss                 | -0.000623    |
|    n_updates            | 6130         |
|    policy_gradient_loss | -0.00799     |
|    std                  | 0.609        |
|    value_loss           | 0.0129       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1257000, episode_reward=4.01 +/- 2.51
Episode length: 207.80 +/- 49.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 208      |
|    mean_reward     | 4.01     |
| time/              |          |
|    total_timesteps | 1257000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 620     |
|    iterations      | 614     |
|    time_elapsed    | 2025    |
|    total_timesteps | 1257472 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1258000, episode_reward=4.36 +/- 2.18
Episode length: 199.00 +/- 59.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 199         |
|    mean_reward          | 4.36        |
| time/                   |             |
|    total_timesteps      | 1258000     |
| train/                  |             |
|    approx_kl            | 0.003953753 |
|    clip_fraction        | 0.0243      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.85       |
|    explained_variance   | 0.975       |
|    learning_rate        | 4.51e-05    |
|    loss                 | 0.0124      |
|    n_updates            | 6140        |
|    policy_gradient_loss | -0.00285    |
|    std                  | 0.609       |
|    value_loss           | 0.0212      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1259000, episode_reward=2.84 +/- 2.93
Episode length: 240.00 +/- 49.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 240      |
|    mean_reward     | 2.84     |
| time/              |          |
|    total_timesteps | 1259000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 620     |
|    iterations      | 615     |
|    time_elapsed    | 2028    |
|    total_timesteps | 1259520 |
--------------------------------
box reached target
Eval num_timesteps=1260000, episode_reward=0.25 +/- 2.66
Episode length: 285.00 +/- 30.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 285          |
|    mean_reward          | 0.251        |
| time/                   |              |
|    total_timesteps      | 1260000      |
| train/                  |              |
|    approx_kl            | 0.0037918976 |
|    clip_fraction        | 0.031        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.85        |
|    explained_variance   | 0.799        |
|    learning_rate        | 4.51e-05     |
|    loss                 | -0.00564     |
|    n_updates            | 6150         |
|    policy_gradient_loss | -0.00564     |
|    std                  | 0.61         |
|    value_loss           | 0.0738       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1261000, episode_reward=3.13 +/- 2.73
Episode length: 232.20 +/- 56.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 232      |
|    mean_reward     | 3.13     |
| time/              |          |
|    total_timesteps | 1261000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 620     |
|    iterations      | 616     |
|    time_elapsed    | 2031    |
|    total_timesteps | 1261568 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1262000, episode_reward=1.75 +/- 2.85
Episode length: 251.00 +/- 60.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 251          |
|    mean_reward          | 1.75         |
| time/                   |              |
|    total_timesteps      | 1262000      |
| train/                  |              |
|    approx_kl            | 0.0043012057 |
|    clip_fraction        | 0.0349       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.85        |
|    explained_variance   | 0.715        |
|    learning_rate        | 4.51e-05     |
|    loss                 | -0.0133      |
|    n_updates            | 6160         |
|    policy_gradient_loss | -0.00477     |
|    std                  | 0.612        |
|    value_loss           | 0.0287       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1263000, episode_reward=3.11 +/- 2.69
Episode length: 229.20 +/- 62.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 229      |
|    mean_reward     | 3.11     |
| time/              |          |
|    total_timesteps | 1263000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 621     |
|    iterations      | 617     |
|    time_elapsed    | 2034    |
|    total_timesteps | 1263616 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1264000, episode_reward=4.00 +/- 2.50
Episode length: 210.40 +/- 52.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | 4            |
| time/                   |              |
|    total_timesteps      | 1264000      |
| train/                  |              |
|    approx_kl            | 0.0058826385 |
|    clip_fraction        | 0.046        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.86        |
|    explained_variance   | 0.786        |
|    learning_rate        | 4.52e-05     |
|    loss                 | 0.00826      |
|    n_updates            | 6170         |
|    policy_gradient_loss | -0.00724     |
|    std                  | 0.612        |
|    value_loss           | 0.0203       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1265000, episode_reward=-0.10 +/- 0.74
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.104   |
| time/              |          |
|    total_timesteps | 1265000  |
---------------------------------
box reached target
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 621     |
|    iterations      | 618     |
|    time_elapsed    | 2037    |
|    total_timesteps | 1265664 |
--------------------------------
box reached target
Eval num_timesteps=1266000, episode_reward=0.41 +/- 2.39
Episode length: 268.40 +/- 63.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 268        |
|    mean_reward          | 0.412      |
| time/                   |            |
|    total_timesteps      | 1266000    |
| train/                  |            |
|    approx_kl            | 0.00396347 |
|    clip_fraction        | 0.0299     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.85      |
|    explained_variance   | 0.853      |
|    learning_rate        | 4.52e-05   |
|    loss                 | -0.0115    |
|    n_updates            | 6180       |
|    policy_gradient_loss | -0.00657   |
|    std                  | 0.609      |
|    value_loss           | 0.0548     |
----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1267000, episode_reward=0.18 +/- 2.46
Episode length: 278.40 +/- 43.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | 0.177    |
| time/              |          |
|    total_timesteps | 1267000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 621     |
|    iterations      | 619     |
|    time_elapsed    | 2040    |
|    total_timesteps | 1267712 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1268000, episode_reward=1.55 +/- 3.10
Episode length: 268.20 +/- 44.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 268         |
|    mean_reward          | 1.55        |
| time/                   |             |
|    total_timesteps      | 1268000     |
| train/                  |             |
|    approx_kl            | 0.004964041 |
|    clip_fraction        | 0.0431      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.85       |
|    explained_variance   | 0.898       |
|    learning_rate        | 4.52e-05    |
|    loss                 | -0.0134     |
|    n_updates            | 6190        |
|    policy_gradient_loss | -0.00689    |
|    std                  | 0.609       |
|    value_loss           | 0.0377      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1269000, episode_reward=1.70 +/- 2.85
Episode length: 243.80 +/- 69.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 244      |
|    mean_reward     | 1.7      |
| time/              |          |
|    total_timesteps | 1269000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 621     |
|    iterations      | 620     |
|    time_elapsed    | 2043    |
|    total_timesteps | 1269760 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1270000, episode_reward=4.11 +/- 2.56
Episode length: 203.80 +/- 52.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 204          |
|    mean_reward          | 4.11         |
| time/                   |              |
|    total_timesteps      | 1270000      |
| train/                  |              |
|    approx_kl            | 0.0059418026 |
|    clip_fraction        | 0.0509       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.84        |
|    explained_variance   | 0.981        |
|    learning_rate        | 4.52e-05     |
|    loss                 | -0.0104      |
|    n_updates            | 6200         |
|    policy_gradient_loss | -0.00706     |
|    std                  | 0.608        |
|    value_loss           | 0.00617      |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=1271000, episode_reward=0.33 +/- 2.42
Episode length: 272.20 +/- 55.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 272      |
|    mean_reward     | 0.332    |
| time/              |          |
|    total_timesteps | 1271000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 621     |
|    iterations      | 621     |
|    time_elapsed    | 2046    |
|    total_timesteps | 1271808 |
--------------------------------
box reached target
Eval num_timesteps=1272000, episode_reward=-0.47 +/- 0.63
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.472      |
| time/                   |             |
|    total_timesteps      | 1272000     |
| train/                  |             |
|    approx_kl            | 0.004075164 |
|    clip_fraction        | 0.0209      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.84       |
|    explained_variance   | 0.888       |
|    learning_rate        | 4.53e-05    |
|    loss                 | 0.018       |
|    n_updates            | 6210        |
|    policy_gradient_loss | -0.0039     |
|    std                  | 0.608       |
|    value_loss           | 0.0541      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1273000, episode_reward=1.78 +/- 3.07
Episode length: 251.40 +/- 60.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | 1.78     |
| time/              |          |
|    total_timesteps | 1273000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 621     |
|    iterations      | 622     |
|    time_elapsed    | 2049    |
|    total_timesteps | 1273856 |
--------------------------------
Eval num_timesteps=1274000, episode_reward=-0.50 +/- 0.77
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.501       |
| time/                   |              |
|    total_timesteps      | 1274000      |
| train/                  |              |
|    approx_kl            | 0.0049253865 |
|    clip_fraction        | 0.035        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.84        |
|    explained_variance   | 0.951        |
|    learning_rate        | 4.53e-05     |
|    loss                 | 0.00628      |
|    n_updates            | 6220         |
|    policy_gradient_loss | -0.00516     |
|    std                  | 0.607        |
|    value_loss           | 0.0305       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=1275000, episode_reward=-0.87 +/- 0.43
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.869   |
| time/              |          |
|    total_timesteps | 1275000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 621     |
|    iterations      | 623     |
|    time_elapsed    | 2053    |
|    total_timesteps | 1275904 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1276000, episode_reward=3.11 +/- 2.87
Episode length: 259.40 +/- 51.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 259         |
|    mean_reward          | 3.11        |
| time/                   |             |
|    total_timesteps      | 1276000     |
| train/                  |             |
|    approx_kl            | 0.006324484 |
|    clip_fraction        | 0.0448      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.84       |
|    explained_variance   | 0.925       |
|    learning_rate        | 4.53e-05    |
|    loss                 | -0.0328     |
|    n_updates            | 6230        |
|    policy_gradient_loss | -0.00654    |
|    std                  | 0.607       |
|    value_loss           | 0.0196      |
-----------------------------------------
box reached target
Eval num_timesteps=1277000, episode_reward=-0.47 +/- 0.51
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.473   |
| time/              |          |
|    total_timesteps | 1277000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 621     |
|    iterations      | 624     |
|    time_elapsed    | 2056    |
|    total_timesteps | 1277952 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1278000, episode_reward=1.91 +/- 2.79
Episode length: 243.60 +/- 70.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 244          |
|    mean_reward          | 1.91         |
| time/                   |              |
|    total_timesteps      | 1278000      |
| train/                  |              |
|    approx_kl            | 0.0034859725 |
|    clip_fraction        | 0.023        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.84        |
|    explained_variance   | 0.877        |
|    learning_rate        | 4.53e-05     |
|    loss                 | 0.0114       |
|    n_updates            | 6240         |
|    policy_gradient_loss | -0.00288     |
|    std                  | 0.605        |
|    value_loss           | 0.0221       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1279000, episode_reward=1.60 +/- 2.90
Episode length: 249.40 +/- 62.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 249      |
|    mean_reward     | 1.6      |
| time/              |          |
|    total_timesteps | 1279000  |
---------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1280000, episode_reward=1.98 +/- 2.76
Episode length: 261.00 +/- 47.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 261      |
|    mean_reward     | 1.98     |
| time/              |          |
|    total_timesteps | 1280000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 621     |
|    iterations      | 625     |
|    time_elapsed    | 2059    |
|    total_timesteps | 1280000 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1281000, episode_reward=-0.43 +/- 0.60
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.434      |
| time/                   |             |
|    total_timesteps      | 1281000     |
| train/                  |             |
|    approx_kl            | 0.007844303 |
|    clip_fraction        | 0.0788      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.83       |
|    explained_variance   | 0.849       |
|    learning_rate        | 4.54e-05    |
|    loss                 | 0.104       |
|    n_updates            | 6250        |
|    policy_gradient_loss | -0.00845    |
|    std                  | 0.604       |
|    value_loss           | 0.0975      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1282000, episode_reward=2.12 +/- 2.67
Episode length: 253.00 +/- 57.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 253      |
|    mean_reward     | 2.12     |
| time/              |          |
|    total_timesteps | 1282000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 621     |
|    iterations      | 626     |
|    time_elapsed    | 2062    |
|    total_timesteps | 1282048 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1283000, episode_reward=2.83 +/- 2.94
Episode length: 224.80 +/- 63.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 225         |
|    mean_reward          | 2.83        |
| time/                   |             |
|    total_timesteps      | 1283000     |
| train/                  |             |
|    approx_kl            | 0.004067899 |
|    clip_fraction        | 0.0201      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.83       |
|    explained_variance   | 0.898       |
|    learning_rate        | 4.54e-05    |
|    loss                 | -0.0161     |
|    n_updates            | 6260        |
|    policy_gradient_loss | -0.00376    |
|    std                  | 0.604       |
|    value_loss           | 0.031       |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1284000, episode_reward=3.01 +/- 2.94
Episode length: 218.20 +/- 71.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 218      |
|    mean_reward     | 3.01     |
| time/              |          |
|    total_timesteps | 1284000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 621     |
|    iterations      | 627     |
|    time_elapsed    | 2065    |
|    total_timesteps | 1284096 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1285000, episode_reward=-0.60 +/- 0.58
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.601      |
| time/                   |             |
|    total_timesteps      | 1285000     |
| train/                  |             |
|    approx_kl            | 0.003636518 |
|    clip_fraction        | 0.0247      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.83       |
|    explained_variance   | 0.973       |
|    learning_rate        | 4.54e-05    |
|    loss                 | -0.0186     |
|    n_updates            | 6270        |
|    policy_gradient_loss | -0.00376    |
|    std                  | 0.605       |
|    value_loss           | 0.0147      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1286000, episode_reward=0.17 +/- 2.69
Episode length: 281.20 +/- 37.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 281      |
|    mean_reward     | 0.17     |
| time/              |          |
|    total_timesteps | 1286000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 621     |
|    iterations      | 628     |
|    time_elapsed    | 2069    |
|    total_timesteps | 1286144 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1287000, episode_reward=1.65 +/- 2.90
Episode length: 242.20 +/- 71.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 242          |
|    mean_reward          | 1.65         |
| time/                   |              |
|    total_timesteps      | 1287000      |
| train/                  |              |
|    approx_kl            | 0.0016633853 |
|    clip_fraction        | 0.00483      |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.83        |
|    explained_variance   | 0.788        |
|    learning_rate        | 4.54e-05     |
|    loss                 | 0.0217       |
|    n_updates            | 6280         |
|    policy_gradient_loss | -0.00124     |
|    std                  | 0.605        |
|    value_loss           | 0.0955       |
------------------------------------------
box reached target
Eval num_timesteps=1288000, episode_reward=0.17 +/- 2.48
Episode length: 269.60 +/- 60.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 270      |
|    mean_reward     | 0.169    |
| time/              |          |
|    total_timesteps | 1288000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 621     |
|    iterations      | 629     |
|    time_elapsed    | 2072    |
|    total_timesteps | 1288192 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1289000, episode_reward=3.21 +/- 2.78
Episode length: 248.40 +/- 44.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 248          |
|    mean_reward          | 3.21         |
| time/                   |              |
|    total_timesteps      | 1289000      |
| train/                  |              |
|    approx_kl            | 0.0059623923 |
|    clip_fraction        | 0.0427       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.83        |
|    explained_variance   | 0.931        |
|    learning_rate        | 4.55e-05     |
|    loss                 | 0.00346      |
|    n_updates            | 6290         |
|    policy_gradient_loss | -0.00671     |
|    std                  | 0.605        |
|    value_loss           | 0.0191       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=1290000, episode_reward=2.10 +/- 2.88
Episode length: 264.00 +/- 44.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 264      |
|    mean_reward     | 2.1      |
| time/              |          |
|    total_timesteps | 1290000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 621     |
|    iterations      | 630     |
|    time_elapsed    | 2075    |
|    total_timesteps | 1290240 |
--------------------------------
Eval num_timesteps=1291000, episode_reward=-0.75 +/- 0.31
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.753       |
| time/                   |              |
|    total_timesteps      | 1291000      |
| train/                  |              |
|    approx_kl            | 0.0074164965 |
|    clip_fraction        | 0.0674       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.83        |
|    explained_variance   | 0.838        |
|    learning_rate        | 4.55e-05     |
|    loss                 | 0.00119      |
|    n_updates            | 6300         |
|    policy_gradient_loss | -0.0111      |
|    std                  | 0.604        |
|    value_loss           | 0.0213       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1292000, episode_reward=2.17 +/- 2.66
Episode length: 261.80 +/- 55.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 262      |
|    mean_reward     | 2.17     |
| time/              |          |
|    total_timesteps | 1292000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 621     |
|    iterations      | 631     |
|    time_elapsed    | 2078    |
|    total_timesteps | 1292288 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1293000, episode_reward=1.72 +/- 2.89
Episode length: 248.40 +/- 63.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 248         |
|    mean_reward          | 1.72        |
| time/                   |             |
|    total_timesteps      | 1293000     |
| train/                  |             |
|    approx_kl            | 0.007092842 |
|    clip_fraction        | 0.0611      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.83       |
|    explained_variance   | 0.95        |
|    learning_rate        | 4.55e-05    |
|    loss                 | -0.015      |
|    n_updates            | 6310        |
|    policy_gradient_loss | -0.0069     |
|    std                  | 0.603       |
|    value_loss           | 0.0125      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1294000, episode_reward=3.17 +/- 2.41
Episode length: 220.20 +/- 66.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 220      |
|    mean_reward     | 3.17     |
| time/              |          |
|    total_timesteps | 1294000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 621     |
|    iterations      | 632     |
|    time_elapsed    | 2081    |
|    total_timesteps | 1294336 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1295000, episode_reward=2.17 +/- 2.59
Episode length: 253.80 +/- 58.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 254          |
|    mean_reward          | 2.17         |
| time/                   |              |
|    total_timesteps      | 1295000      |
| train/                  |              |
|    approx_kl            | 0.0054573696 |
|    clip_fraction        | 0.0409       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.83        |
|    explained_variance   | 0.969        |
|    learning_rate        | 4.55e-05     |
|    loss                 | -0.00637     |
|    n_updates            | 6320         |
|    policy_gradient_loss | -0.00682     |
|    std                  | 0.605        |
|    value_loss           | 0.00543      |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1296000, episode_reward=1.70 +/- 2.87
Episode length: 248.20 +/- 64.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 248      |
|    mean_reward     | 1.7      |
| time/              |          |
|    total_timesteps | 1296000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 622     |
|    iterations      | 633     |
|    time_elapsed    | 2084    |
|    total_timesteps | 1296384 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1297000, episode_reward=3.05 +/- 2.82
Episode length: 226.40 +/- 60.95
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 226        |
|    mean_reward          | 3.05       |
| time/                   |            |
|    total_timesteps      | 1297000    |
| train/                  |            |
|    approx_kl            | 0.00410328 |
|    clip_fraction        | 0.0285     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.83      |
|    explained_variance   | 0.946      |
|    learning_rate        | 4.56e-05   |
|    loss                 | -0.0165    |
|    n_updates            | 6330       |
|    policy_gradient_loss | -0.00536   |
|    std                  | 0.605      |
|    value_loss           | 0.0305     |
----------------------------------------
box reached target
box reached target
Eval num_timesteps=1298000, episode_reward=1.89 +/- 2.91
Episode length: 263.80 +/- 46.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 264      |
|    mean_reward     | 1.89     |
| time/              |          |
|    total_timesteps | 1298000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 622     |
|    iterations      | 634     |
|    time_elapsed    | 2087    |
|    total_timesteps | 1298432 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1299000, episode_reward=1.63 +/- 2.89
Episode length: 243.60 +/- 69.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 244          |
|    mean_reward          | 1.63         |
| time/                   |              |
|    total_timesteps      | 1299000      |
| train/                  |              |
|    approx_kl            | 0.0037862617 |
|    clip_fraction        | 0.0203       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.83        |
|    explained_variance   | 0.886        |
|    learning_rate        | 4.56e-05     |
|    loss                 | -0.000632    |
|    n_updates            | 6340         |
|    policy_gradient_loss | -0.00463     |
|    std                  | 0.604        |
|    value_loss           | 0.013        |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=1300000, episode_reward=0.52 +/- 2.41
Episode length: 275.40 +/- 49.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 275      |
|    mean_reward     | 0.519    |
| time/              |          |
|    total_timesteps | 1300000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 622     |
|    iterations      | 635     |
|    time_elapsed    | 2090    |
|    total_timesteps | 1300480 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1301000, episode_reward=0.62 +/- 2.47
Episode length: 285.80 +/- 28.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 286          |
|    mean_reward          | 0.623        |
| time/                   |              |
|    total_timesteps      | 1301000      |
| train/                  |              |
|    approx_kl            | 0.0037036927 |
|    clip_fraction        | 0.0189       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.83        |
|    explained_variance   | 0.945        |
|    learning_rate        | 4.56e-05     |
|    loss                 | -0.00783     |
|    n_updates            | 6350         |
|    policy_gradient_loss | -0.00353     |
|    std                  | 0.605        |
|    value_loss           | 0.014        |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1302000, episode_reward=2.80 +/- 2.82
Episode length: 218.80 +/- 67.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 219      |
|    mean_reward     | 2.8      |
| time/              |          |
|    total_timesteps | 1302000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 622     |
|    iterations      | 636     |
|    time_elapsed    | 2093    |
|    total_timesteps | 1302528 |
--------------------------------
box reached target
Eval num_timesteps=1303000, episode_reward=0.59 +/- 2.34
Episode length: 268.60 +/- 62.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 269         |
|    mean_reward          | 0.592       |
| time/                   |             |
|    total_timesteps      | 1303000     |
| train/                  |             |
|    approx_kl            | 0.004688424 |
|    clip_fraction        | 0.0288      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.83       |
|    explained_variance   | 0.91        |
|    learning_rate        | 4.56e-05    |
|    loss                 | -0.00686    |
|    n_updates            | 6360        |
|    policy_gradient_loss | -0.00465    |
|    std                  | 0.604       |
|    value_loss           | 0.0304      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=1304000, episode_reward=1.74 +/- 2.85
Episode length: 239.20 +/- 74.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 239      |
|    mean_reward     | 1.74     |
| time/              |          |
|    total_timesteps | 1304000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 622     |
|    iterations      | 637     |
|    time_elapsed    | 2096    |
|    total_timesteps | 1304576 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1305000, episode_reward=3.94 +/- 2.47
Episode length: 185.00 +/- 58.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 185         |
|    mean_reward          | 3.94        |
| time/                   |             |
|    total_timesteps      | 1305000     |
| train/                  |             |
|    approx_kl            | 0.002767704 |
|    clip_fraction        | 0.00889     |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.83       |
|    explained_variance   | 0.798       |
|    learning_rate        | 4.57e-05    |
|    loss                 | 0.00307     |
|    n_updates            | 6370        |
|    policy_gradient_loss | -0.0025     |
|    std                  | 0.605       |
|    value_loss           | 0.0251      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1306000, episode_reward=2.77 +/- 3.05
Episode length: 226.80 +/- 60.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 227      |
|    mean_reward     | 2.77     |
| time/              |          |
|    total_timesteps | 1306000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 622     |
|    iterations      | 638     |
|    time_elapsed    | 2099    |
|    total_timesteps | 1306624 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1307000, episode_reward=1.41 +/- 3.10
Episode length: 240.60 +/- 72.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 241          |
|    mean_reward          | 1.41         |
| time/                   |              |
|    total_timesteps      | 1307000      |
| train/                  |              |
|    approx_kl            | 0.0060154907 |
|    clip_fraction        | 0.0353       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.83        |
|    explained_variance   | 0.829        |
|    learning_rate        | 4.57e-05     |
|    loss                 | -0.0128      |
|    n_updates            | 6380         |
|    policy_gradient_loss | -0.00509     |
|    std                  | 0.604        |
|    value_loss           | 0.0275       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1308000, episode_reward=4.38 +/- 1.86
Episode length: 206.40 +/- 49.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | 4.38     |
| time/              |          |
|    total_timesteps | 1308000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 622     |
|    iterations      | 639     |
|    time_elapsed    | 2102    |
|    total_timesteps | 1308672 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1309000, episode_reward=1.42 +/- 3.07
Episode length: 251.20 +/- 60.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 251          |
|    mean_reward          | 1.42         |
| time/                   |              |
|    total_timesteps      | 1309000      |
| train/                  |              |
|    approx_kl            | 0.0027495031 |
|    clip_fraction        | 0.019        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.83        |
|    explained_variance   | 0.756        |
|    learning_rate        | 4.57e-05     |
|    loss                 | -0.0144      |
|    n_updates            | 6390         |
|    policy_gradient_loss | -0.00305     |
|    std                  | 0.603        |
|    value_loss           | 0.0736       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1310000, episode_reward=3.95 +/- 2.48
Episode length: 191.20 +/- 54.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 191      |
|    mean_reward     | 3.95     |
| time/              |          |
|    total_timesteps | 1310000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 622     |
|    iterations      | 640     |
|    time_elapsed    | 2104    |
|    total_timesteps | 1310720 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1311000, episode_reward=2.07 +/- 2.65
Episode length: 246.60 +/- 66.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 247          |
|    mean_reward          | 2.07         |
| time/                   |              |
|    total_timesteps      | 1311000      |
| train/                  |              |
|    approx_kl            | 0.0037759412 |
|    clip_fraction        | 0.0203       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.82        |
|    explained_variance   | 0.898        |
|    learning_rate        | 4.57e-05     |
|    loss                 | 0.00502      |
|    n_updates            | 6400         |
|    policy_gradient_loss | -0.00297     |
|    std                  | 0.601        |
|    value_loss           | 0.0408       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1312000, episode_reward=-0.36 +/- 0.69
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.355   |
| time/              |          |
|    total_timesteps | 1312000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 622     |
|    iterations      | 641     |
|    time_elapsed    | 2108    |
|    total_timesteps | 1312768 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1313000, episode_reward=1.93 +/- 2.72
Episode length: 246.60 +/- 66.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 247          |
|    mean_reward          | 1.93         |
| time/                   |              |
|    total_timesteps      | 1313000      |
| train/                  |              |
|    approx_kl            | 0.0046145353 |
|    clip_fraction        | 0.0384       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.82        |
|    explained_variance   | 0.851        |
|    learning_rate        | 4.58e-05     |
|    loss                 | 0.00675      |
|    n_updates            | 6410         |
|    policy_gradient_loss | -0.00574     |
|    std                  | 0.6          |
|    value_loss           | 0.0755       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1314000, episode_reward=3.09 +/- 2.69
Episode length: 221.00 +/- 65.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 221      |
|    mean_reward     | 3.09     |
| time/              |          |
|    total_timesteps | 1314000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 622     |
|    iterations      | 642     |
|    time_elapsed    | 2111    |
|    total_timesteps | 1314816 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1315000, episode_reward=1.85 +/- 2.99
Episode length: 264.00 +/- 44.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 264         |
|    mean_reward          | 1.85        |
| time/                   |             |
|    total_timesteps      | 1315000     |
| train/                  |             |
|    approx_kl            | 0.003466817 |
|    clip_fraction        | 0.0226      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.82       |
|    explained_variance   | 0.89        |
|    learning_rate        | 4.58e-05    |
|    loss                 | -0.00704    |
|    n_updates            | 6420        |
|    policy_gradient_loss | -0.00412    |
|    std                  | 0.602       |
|    value_loss           | 0.0188      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1316000, episode_reward=2.85 +/- 2.93
Episode length: 232.40 +/- 55.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 232      |
|    mean_reward     | 2.85     |
| time/              |          |
|    total_timesteps | 1316000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 622     |
|    iterations      | 643     |
|    time_elapsed    | 2114    |
|    total_timesteps | 1316864 |
--------------------------------
box reached target
Eval num_timesteps=1317000, episode_reward=-0.42 +/- 0.91
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.417       |
| time/                   |              |
|    total_timesteps      | 1317000      |
| train/                  |              |
|    approx_kl            | 0.0045731775 |
|    clip_fraction        | 0.0482       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.82        |
|    explained_variance   | 0.843        |
|    learning_rate        | 4.58e-05     |
|    loss                 | -0.0277      |
|    n_updates            | 6430         |
|    policy_gradient_loss | -0.00582     |
|    std                  | 0.6          |
|    value_loss           | 0.0311       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1318000, episode_reward=1.64 +/- 3.05
Episode length: 253.40 +/- 60.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 253      |
|    mean_reward     | 1.64     |
| time/              |          |
|    total_timesteps | 1318000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 622     |
|    iterations      | 644     |
|    time_elapsed    | 2117    |
|    total_timesteps | 1318912 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1319000, episode_reward=1.86 +/- 2.85
Episode length: 246.80 +/- 67.36
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 247        |
|    mean_reward          | 1.86       |
| time/                   |            |
|    total_timesteps      | 1319000    |
| train/                  |            |
|    approx_kl            | 0.00561205 |
|    clip_fraction        | 0.0367     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.82      |
|    explained_variance   | 0.894      |
|    learning_rate        | 4.58e-05   |
|    loss                 | 0.0199     |
|    n_updates            | 6440       |
|    policy_gradient_loss | -0.00554   |
|    std                  | 0.6        |
|    value_loss           | 0.0406     |
----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1320000, episode_reward=2.80 +/- 3.10
Episode length: 236.40 +/- 52.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 236      |
|    mean_reward     | 2.8      |
| time/              |          |
|    total_timesteps | 1320000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 623     |
|    iterations      | 645     |
|    time_elapsed    | 2120    |
|    total_timesteps | 1320960 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1321000, episode_reward=2.10 +/- 2.62
Episode length: 240.80 +/- 73.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 241         |
|    mean_reward          | 2.1         |
| time/                   |             |
|    total_timesteps      | 1321000     |
| train/                  |             |
|    approx_kl            | 0.005158614 |
|    clip_fraction        | 0.0404      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.81       |
|    explained_variance   | 0.863       |
|    learning_rate        | 4.59e-05    |
|    loss                 | -0.0021     |
|    n_updates            | 6450        |
|    policy_gradient_loss | -0.0055     |
|    std                  | 0.599       |
|    value_loss           | 0.043       |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1322000, episode_reward=3.11 +/- 2.78
Episode length: 232.20 +/- 62.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 232      |
|    mean_reward     | 3.11     |
| time/              |          |
|    total_timesteps | 1322000  |
---------------------------------
box reached target
box reached target
Eval num_timesteps=1323000, episode_reward=1.48 +/- 3.21
Episode length: 249.40 +/- 61.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 249      |
|    mean_reward     | 1.48     |
| time/              |          |
|    total_timesteps | 1323000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 622     |
|    iterations      | 646     |
|    time_elapsed    | 2123    |
|    total_timesteps | 1323008 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1324000, episode_reward=1.70 +/- 2.85
Episode length: 240.00 +/- 73.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 240         |
|    mean_reward          | 1.7         |
| time/                   |             |
|    total_timesteps      | 1324000     |
| train/                  |             |
|    approx_kl            | 0.004782153 |
|    clip_fraction        | 0.0442      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.81       |
|    explained_variance   | 0.967       |
|    learning_rate        | 4.59e-05    |
|    loss                 | -0.00648    |
|    n_updates            | 6460        |
|    policy_gradient_loss | -0.00501    |
|    std                  | 0.6         |
|    value_loss           | 0.006       |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1325000, episode_reward=2.78 +/- 3.09
Episode length: 236.00 +/- 67.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 236      |
|    mean_reward     | 2.78     |
| time/              |          |
|    total_timesteps | 1325000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 623     |
|    iterations      | 647     |
|    time_elapsed    | 2126    |
|    total_timesteps | 1325056 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1326000, episode_reward=4.15 +/- 2.58
Episode length: 214.00 +/- 46.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 214         |
|    mean_reward          | 4.15        |
| time/                   |             |
|    total_timesteps      | 1326000     |
| train/                  |             |
|    approx_kl            | 0.003765466 |
|    clip_fraction        | 0.0186      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.82       |
|    explained_variance   | 0.897       |
|    learning_rate        | 4.59e-05    |
|    loss                 | 0.00391     |
|    n_updates            | 6470        |
|    policy_gradient_loss | -0.00299    |
|    std                  | 0.6         |
|    value_loss           | 0.0369      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1327000, episode_reward=1.48 +/- 3.04
Episode length: 252.20 +/- 58.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 252      |
|    mean_reward     | 1.48     |
| time/              |          |
|    total_timesteps | 1327000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 623     |
|    iterations      | 648     |
|    time_elapsed    | 2129    |
|    total_timesteps | 1327104 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1328000, episode_reward=2.18 +/- 2.41
Episode length: 261.80 +/- 47.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 262         |
|    mean_reward          | 2.18        |
| time/                   |             |
|    total_timesteps      | 1328000     |
| train/                  |             |
|    approx_kl            | 0.004664531 |
|    clip_fraction        | 0.0341      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.81       |
|    explained_variance   | 0.944       |
|    learning_rate        | 4.59e-05    |
|    loss                 | -0.0283     |
|    n_updates            | 6480        |
|    policy_gradient_loss | -0.00508    |
|    std                  | 0.599       |
|    value_loss           | 0.0154      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1329000, episode_reward=2.80 +/- 3.03
Episode length: 227.80 +/- 62.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 228      |
|    mean_reward     | 2.8      |
| time/              |          |
|    total_timesteps | 1329000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 623     |
|    iterations      | 649     |
|    time_elapsed    | 2132    |
|    total_timesteps | 1329152 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1330000, episode_reward=2.68 +/- 3.09
Episode length: 217.40 +/- 69.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 217          |
|    mean_reward          | 2.68         |
| time/                   |              |
|    total_timesteps      | 1330000      |
| train/                  |              |
|    approx_kl            | 0.0038548273 |
|    clip_fraction        | 0.0262       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.81        |
|    explained_variance   | 0.907        |
|    learning_rate        | 4.59e-05     |
|    loss                 | -0.00401     |
|    n_updates            | 6490         |
|    policy_gradient_loss | -0.00348     |
|    std                  | 0.598        |
|    value_loss           | 0.0758       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1331000, episode_reward=4.03 +/- 2.50
Episode length: 194.20 +/- 54.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 194      |
|    mean_reward     | 4.03     |
| time/              |          |
|    total_timesteps | 1331000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 622     |
|    iterations      | 650     |
|    time_elapsed    | 2136    |
|    total_timesteps | 1331200 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1332000, episode_reward=1.75 +/- 3.03
Episode length: 259.00 +/- 60.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 259          |
|    mean_reward          | 1.75         |
| time/                   |              |
|    total_timesteps      | 1332000      |
| train/                  |              |
|    approx_kl            | 0.0057173027 |
|    clip_fraction        | 0.0411       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.81        |
|    explained_variance   | 0.976        |
|    learning_rate        | 4.6e-05      |
|    loss                 | 0.0209       |
|    n_updates            | 6500         |
|    policy_gradient_loss | -0.00367     |
|    std                  | 0.596        |
|    value_loss           | 0.00412      |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1333000, episode_reward=4.05 +/- 2.53
Episode length: 197.00 +/- 54.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 197      |
|    mean_reward     | 4.05     |
| time/              |          |
|    total_timesteps | 1333000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 623     |
|    iterations      | 651     |
|    time_elapsed    | 2139    |
|    total_timesteps | 1333248 |
--------------------------------
box reached target
Eval num_timesteps=1334000, episode_reward=0.38 +/- 2.41
Episode length: 275.20 +/- 49.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 275          |
|    mean_reward          | 0.381        |
| time/                   |              |
|    total_timesteps      | 1334000      |
| train/                  |              |
|    approx_kl            | 0.0052469866 |
|    clip_fraction        | 0.0352       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.8         |
|    explained_variance   | 0.858        |
|    learning_rate        | 4.6e-05      |
|    loss                 | 0.0233       |
|    n_updates            | 6510         |
|    policy_gradient_loss | -0.00471     |
|    std                  | 0.595        |
|    value_loss           | 0.0585       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1335000, episode_reward=1.45 +/- 3.12
Episode length: 247.40 +/- 64.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 247      |
|    mean_reward     | 1.45     |
| time/              |          |
|    total_timesteps | 1335000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 623     |
|    iterations      | 652     |
|    time_elapsed    | 2142    |
|    total_timesteps | 1335296 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1336000, episode_reward=2.65 +/- 3.17
Episode length: 217.80 +/- 67.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 218         |
|    mean_reward          | 2.65        |
| time/                   |             |
|    total_timesteps      | 1336000     |
| train/                  |             |
|    approx_kl            | 0.003491045 |
|    clip_fraction        | 0.0208      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.8        |
|    explained_variance   | 0.822       |
|    learning_rate        | 4.6e-05     |
|    loss                 | -0.0108     |
|    n_updates            | 6520        |
|    policy_gradient_loss | -0.00452    |
|    std                  | 0.595       |
|    value_loss           | 0.0392      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=1337000, episode_reward=0.39 +/- 2.47
Episode length: 278.60 +/- 42.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 279      |
|    mean_reward     | 0.391    |
| time/              |          |
|    total_timesteps | 1337000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 623     |
|    iterations      | 653     |
|    time_elapsed    | 2145    |
|    total_timesteps | 1337344 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1338000, episode_reward=2.77 +/- 3.08
Episode length: 226.00 +/- 61.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 226          |
|    mean_reward          | 2.77         |
| time/                   |              |
|    total_timesteps      | 1338000      |
| train/                  |              |
|    approx_kl            | 0.0033545804 |
|    clip_fraction        | 0.0266       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.8         |
|    explained_variance   | 0.922        |
|    learning_rate        | 4.6e-05      |
|    loss                 | -0.0161      |
|    n_updates            | 6530         |
|    policy_gradient_loss | -0.00491     |
|    std                  | 0.596        |
|    value_loss           | 0.0246       |
------------------------------------------
Eval num_timesteps=1339000, episode_reward=-0.09 +/- 0.81
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.0892  |
| time/              |          |
|    total_timesteps | 1339000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 623     |
|    iterations      | 654     |
|    time_elapsed    | 2148    |
|    total_timesteps | 1339392 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1340000, episode_reward=2.91 +/- 2.81
Episode length: 230.80 +/- 57.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 231          |
|    mean_reward          | 2.91         |
| time/                   |              |
|    total_timesteps      | 1340000      |
| train/                  |              |
|    approx_kl            | 0.0053005544 |
|    clip_fraction        | 0.0436       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.8         |
|    explained_variance   | 0.865        |
|    learning_rate        | 4.61e-05     |
|    loss                 | 0.0143       |
|    n_updates            | 6540         |
|    policy_gradient_loss | -0.00655     |
|    std                  | 0.596        |
|    value_loss           | 0.0258       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1341000, episode_reward=2.70 +/- 3.02
Episode length: 214.80 +/- 69.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 215      |
|    mean_reward     | 2.7      |
| time/              |          |
|    total_timesteps | 1341000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 623     |
|    iterations      | 655     |
|    time_elapsed    | 2151    |
|    total_timesteps | 1341440 |
--------------------------------
box reached target
Eval num_timesteps=1342000, episode_reward=0.77 +/- 2.31
Episode length: 273.00 +/- 54.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 273          |
|    mean_reward          | 0.772        |
| time/                   |              |
|    total_timesteps      | 1342000      |
| train/                  |              |
|    approx_kl            | 0.0055618156 |
|    clip_fraction        | 0.0423       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.8         |
|    explained_variance   | 0.856        |
|    learning_rate        | 4.61e-05     |
|    loss                 | 0.0035       |
|    n_updates            | 6550         |
|    policy_gradient_loss | -0.00777     |
|    std                  | 0.597        |
|    value_loss           | 0.0182       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1343000, episode_reward=1.31 +/- 3.17
Episode length: 239.80 +/- 73.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 240      |
|    mean_reward     | 1.31     |
| time/              |          |
|    total_timesteps | 1343000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 623     |
|    iterations      | 656     |
|    time_elapsed    | 2154    |
|    total_timesteps | 1343488 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1344000, episode_reward=4.03 +/- 2.52
Episode length: 202.20 +/- 50.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 202          |
|    mean_reward          | 4.03         |
| time/                   |              |
|    total_timesteps      | 1344000      |
| train/                  |              |
|    approx_kl            | 0.0049894564 |
|    clip_fraction        | 0.0322       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.81        |
|    explained_variance   | 0.83         |
|    learning_rate        | 4.61e-05     |
|    loss                 | 0.00349      |
|    n_updates            | 6560         |
|    policy_gradient_loss | -0.00451     |
|    std                  | 0.599        |
|    value_loss           | 0.0475       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=1345000, episode_reward=1.77 +/- 2.96
Episode length: 260.40 +/- 50.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 260      |
|    mean_reward     | 1.77     |
| time/              |          |
|    total_timesteps | 1345000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 623     |
|    iterations      | 657     |
|    time_elapsed    | 2157    |
|    total_timesteps | 1345536 |
--------------------------------
box reached target
Eval num_timesteps=1346000, episode_reward=0.30 +/- 2.64
Episode length: 281.00 +/- 38.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 281         |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 1346000     |
| train/                  |             |
|    approx_kl            | 0.004928358 |
|    clip_fraction        | 0.0437      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.81       |
|    explained_variance   | 0.854       |
|    learning_rate        | 4.61e-05    |
|    loss                 | 0.00604     |
|    n_updates            | 6570        |
|    policy_gradient_loss | -0.00466    |
|    std                  | 0.6         |
|    value_loss           | 0.0106      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1347000, episode_reward=3.98 +/- 2.70
Episode length: 216.00 +/- 46.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 216      |
|    mean_reward     | 3.98     |
| time/              |          |
|    total_timesteps | 1347000  |
---------------------------------
box reached target
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 623     |
|    iterations      | 658     |
|    time_elapsed    | 2160    |
|    total_timesteps | 1347584 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1348000, episode_reward=0.61 +/- 2.46
Episode length: 276.60 +/- 46.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 277          |
|    mean_reward          | 0.611        |
| time/                   |              |
|    total_timesteps      | 1348000      |
| train/                  |              |
|    approx_kl            | 0.0054048575 |
|    clip_fraction        | 0.0411       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.82        |
|    explained_variance   | 0.911        |
|    learning_rate        | 4.62e-05     |
|    loss                 | -0.0182      |
|    n_updates            | 6580         |
|    policy_gradient_loss | -0.0053      |
|    std                  | 0.6          |
|    value_loss           | 0.0351       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1349000, episode_reward=2.86 +/- 2.75
Episode length: 222.80 +/- 64.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 223      |
|    mean_reward     | 2.86     |
| time/              |          |
|    total_timesteps | 1349000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 623     |
|    iterations      | 659     |
|    time_elapsed    | 2163    |
|    total_timesteps | 1349632 |
--------------------------------
box reached target
Eval num_timesteps=1350000, episode_reward=0.29 +/- 2.49
Episode length: 269.60 +/- 60.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 270          |
|    mean_reward          | 0.295        |
| time/                   |              |
|    total_timesteps      | 1350000      |
| train/                  |              |
|    approx_kl            | 0.0039553903 |
|    clip_fraction        | 0.0173       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.81        |
|    explained_variance   | 0.859        |
|    learning_rate        | 4.62e-05     |
|    loss                 | -0.00474     |
|    n_updates            | 6590         |
|    policy_gradient_loss | -0.00491     |
|    std                  | 0.599        |
|    value_loss           | 0.063        |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1351000, episode_reward=1.69 +/- 3.01
Episode length: 254.60 +/- 61.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 255      |
|    mean_reward     | 1.69     |
| time/              |          |
|    total_timesteps | 1351000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 623     |
|    iterations      | 660     |
|    time_elapsed    | 2166    |
|    total_timesteps | 1351680 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1352000, episode_reward=3.04 +/- 2.83
Episode length: 228.20 +/- 61.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 228          |
|    mean_reward          | 3.04         |
| time/                   |              |
|    total_timesteps      | 1352000      |
| train/                  |              |
|    approx_kl            | 0.0053333775 |
|    clip_fraction        | 0.0416       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.81        |
|    explained_variance   | 0.717        |
|    learning_rate        | 4.62e-05     |
|    loss                 | 0.114        |
|    n_updates            | 6600         |
|    policy_gradient_loss | -0.00543     |
|    std                  | 0.597        |
|    value_loss           | 0.062        |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1353000, episode_reward=1.73 +/- 2.84
Episode length: 246.00 +/- 66.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 246      |
|    mean_reward     | 1.73     |
| time/              |          |
|    total_timesteps | 1353000  |
---------------------------------
box reached target
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 623     |
|    iterations      | 661     |
|    time_elapsed    | 2169    |
|    total_timesteps | 1353728 |
--------------------------------
box reached target
Eval num_timesteps=1354000, episode_reward=0.56 +/- 2.47
Episode length: 281.20 +/- 37.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 281          |
|    mean_reward          | 0.557        |
| time/                   |              |
|    total_timesteps      | 1354000      |
| train/                  |              |
|    approx_kl            | 0.0037081563 |
|    clip_fraction        | 0.0243       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.8         |
|    explained_variance   | 0.769        |
|    learning_rate        | 4.62e-05     |
|    loss                 | 0.0172       |
|    n_updates            | 6610         |
|    policy_gradient_loss | -0.0042      |
|    std                  | 0.594        |
|    value_loss           | 0.122        |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=1355000, episode_reward=0.79 +/- 2.39
Episode length: 281.00 +/- 38.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 281      |
|    mean_reward     | 0.788    |
| time/              |          |
|    total_timesteps | 1355000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 623     |
|    iterations      | 662     |
|    time_elapsed    | 2173    |
|    total_timesteps | 1355776 |
--------------------------------
box reached target
Eval num_timesteps=1356000, episode_reward=1.29 +/- 1.97
Episode length: 271.20 +/- 57.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 271          |
|    mean_reward          | 1.29         |
| time/                   |              |
|    total_timesteps      | 1356000      |
| train/                  |              |
|    approx_kl            | 0.0065483386 |
|    clip_fraction        | 0.0553       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.79        |
|    explained_variance   | 0.953        |
|    learning_rate        | 4.63e-05     |
|    loss                 | 0.0179       |
|    n_updates            | 6620         |
|    policy_gradient_loss | -0.0045      |
|    std                  | 0.592        |
|    value_loss           | 0.0164       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1357000, episode_reward=2.78 +/- 3.00
Episode length: 219.80 +/- 68.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 220      |
|    mean_reward     | 2.78     |
| time/              |          |
|    total_timesteps | 1357000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 623     |
|    iterations      | 663     |
|    time_elapsed    | 2176    |
|    total_timesteps | 1357824 |
--------------------------------
Eval num_timesteps=1358000, episode_reward=-0.76 +/- 0.73
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.765       |
| time/                   |              |
|    total_timesteps      | 1358000      |
| train/                  |              |
|    approx_kl            | 0.0035687294 |
|    clip_fraction        | 0.018        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.79        |
|    explained_variance   | 0.928        |
|    learning_rate        | 4.63e-05     |
|    loss                 | 0.00198      |
|    n_updates            | 6630         |
|    policy_gradient_loss | -0.0021      |
|    std                  | 0.592        |
|    value_loss           | 0.0271       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1359000, episode_reward=1.81 +/- 2.88
Episode length: 255.20 +/- 57.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 255      |
|    mean_reward     | 1.81     |
| time/              |          |
|    total_timesteps | 1359000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 624     |
|    iterations      | 664     |
|    time_elapsed    | 2179    |
|    total_timesteps | 1359872 |
--------------------------------
Eval num_timesteps=1360000, episode_reward=-0.46 +/- 0.68
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.463      |
| time/                   |             |
|    total_timesteps      | 1360000     |
| train/                  |             |
|    approx_kl            | 0.004503673 |
|    clip_fraction        | 0.039       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.79       |
|    explained_variance   | 0.915       |
|    learning_rate        | 4.63e-05    |
|    loss                 | 0.0128      |
|    n_updates            | 6640        |
|    policy_gradient_loss | -0.00632    |
|    std                  | 0.591       |
|    value_loss           | 0.0617      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1361000, episode_reward=1.88 +/- 2.78
Episode length: 239.80 +/- 74.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 240      |
|    mean_reward     | 1.88     |
| time/              |          |
|    total_timesteps | 1361000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 624     |
|    iterations      | 665     |
|    time_elapsed    | 2182    |
|    total_timesteps | 1361920 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1362000, episode_reward=2.94 +/- 2.81
Episode length: 230.00 +/- 60.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 230         |
|    mean_reward          | 2.94        |
| time/                   |             |
|    total_timesteps      | 1362000     |
| train/                  |             |
|    approx_kl            | 0.003936059 |
|    clip_fraction        | 0.0289      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | 0.89        |
|    learning_rate        | 4.63e-05    |
|    loss                 | -0.00082    |
|    n_updates            | 6650        |
|    policy_gradient_loss | -0.00597    |
|    std                  | 0.59        |
|    value_loss           | 0.0861      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1363000, episode_reward=0.33 +/- 2.51
Episode length: 275.60 +/- 48.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 276      |
|    mean_reward     | 0.327    |
| time/              |          |
|    total_timesteps | 1363000  |
---------------------------------
box reached target
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 624     |
|    iterations      | 666     |
|    time_elapsed    | 2185    |
|    total_timesteps | 1363968 |
--------------------------------
box reached target
Eval num_timesteps=1364000, episode_reward=0.58 +/- 2.40
Episode length: 281.80 +/- 36.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 282          |
|    mean_reward          | 0.581        |
| time/                   |              |
|    total_timesteps      | 1364000      |
| train/                  |              |
|    approx_kl            | 0.0041891923 |
|    clip_fraction        | 0.0346       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.78        |
|    explained_variance   | 0.891        |
|    learning_rate        | 4.64e-05     |
|    loss                 | 0.0104       |
|    n_updates            | 6660         |
|    policy_gradient_loss | -0.00542     |
|    std                  | 0.589        |
|    value_loss           | 0.0674       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1365000, episode_reward=1.99 +/- 2.74
Episode length: 247.40 +/- 64.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 247      |
|    mean_reward     | 1.99     |
| time/              |          |
|    total_timesteps | 1365000  |
---------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1366000, episode_reward=3.97 +/- 2.49
Episode length: 183.40 +/- 59.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 183      |
|    mean_reward     | 3.97     |
| time/              |          |
|    total_timesteps | 1366000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 624     |
|    iterations      | 667     |
|    time_elapsed    | 2188    |
|    total_timesteps | 1366016 |
--------------------------------
box reached target
Eval num_timesteps=1367000, episode_reward=0.29 +/- 2.58
Episode length: 276.00 +/- 48.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 276          |
|    mean_reward          | 0.288        |
| time/                   |              |
|    total_timesteps      | 1367000      |
| train/                  |              |
|    approx_kl            | 0.0039466904 |
|    clip_fraction        | 0.0232       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.78        |
|    explained_variance   | 0.954        |
|    learning_rate        | 4.64e-05     |
|    loss                 | 0.00764      |
|    n_updates            | 6670         |
|    policy_gradient_loss | -0.00327     |
|    std                  | 0.588        |
|    value_loss           | 0.0242       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1368000, episode_reward=5.23 +/- 0.19
Episode length: 166.20 +/- 22.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 166      |
|    mean_reward     | 5.23     |
| time/              |          |
|    total_timesteps | 1368000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 624     |
|    iterations      | 668     |
|    time_elapsed    | 2191    |
|    total_timesteps | 1368064 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1369000, episode_reward=0.18 +/- 2.46
Episode length: 272.80 +/- 54.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 273          |
|    mean_reward          | 0.177        |
| time/                   |              |
|    total_timesteps      | 1369000      |
| train/                  |              |
|    approx_kl            | 0.0034418283 |
|    clip_fraction        | 0.0288       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.78        |
|    explained_variance   | 0.91         |
|    learning_rate        | 4.64e-05     |
|    loss                 | -0.00756     |
|    n_updates            | 6680         |
|    policy_gradient_loss | -0.00454     |
|    std                  | 0.59         |
|    value_loss           | 0.0275       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1370000, episode_reward=3.18 +/- 2.79
Episode length: 241.00 +/- 49.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 241      |
|    mean_reward     | 3.18     |
| time/              |          |
|    total_timesteps | 1370000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 624     |
|    iterations      | 669     |
|    time_elapsed    | 2194    |
|    total_timesteps | 1370112 |
--------------------------------
Eval num_timesteps=1371000, episode_reward=-0.68 +/- 0.57
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.678      |
| time/                   |             |
|    total_timesteps      | 1371000     |
| train/                  |             |
|    approx_kl            | 0.003867027 |
|    clip_fraction        | 0.0318      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | 0.713       |
|    learning_rate        | 4.64e-05    |
|    loss                 | 0.016       |
|    n_updates            | 6690        |
|    policy_gradient_loss | -0.00382    |
|    std                  | 0.589       |
|    value_loss           | 0.0593      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=1372000, episode_reward=0.20 +/- 2.51
Episode length: 277.20 +/- 45.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 277      |
|    mean_reward     | 0.202    |
| time/              |          |
|    total_timesteps | 1372000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 624     |
|    iterations      | 670     |
|    time_elapsed    | 2197    |
|    total_timesteps | 1372160 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1373000, episode_reward=2.04 +/- 2.73
Episode length: 243.60 +/- 69.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 244          |
|    mean_reward          | 2.04         |
| time/                   |              |
|    total_timesteps      | 1373000      |
| train/                  |              |
|    approx_kl            | 0.0050433753 |
|    clip_fraction        | 0.0428       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.78        |
|    explained_variance   | 0.747        |
|    learning_rate        | 4.65e-05     |
|    loss                 | 0.00125      |
|    n_updates            | 6700         |
|    policy_gradient_loss | -0.00643     |
|    std                  | 0.589        |
|    value_loss           | 0.0228       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1374000, episode_reward=2.78 +/- 3.08
Episode length: 238.00 +/- 55.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 238      |
|    mean_reward     | 2.78     |
| time/              |          |
|    total_timesteps | 1374000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 624     |
|    iterations      | 671     |
|    time_elapsed    | 2200    |
|    total_timesteps | 1374208 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1375000, episode_reward=1.55 +/- 3.23
Episode length: 251.40 +/- 64.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 251          |
|    mean_reward          | 1.55         |
| time/                   |              |
|    total_timesteps      | 1375000      |
| train/                  |              |
|    approx_kl            | 0.0039382167 |
|    clip_fraction        | 0.0115       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.78        |
|    explained_variance   | 0.916        |
|    learning_rate        | 4.65e-05     |
|    loss                 | 0.00162      |
|    n_updates            | 6710         |
|    policy_gradient_loss | -0.00268     |
|    std                  | 0.587        |
|    value_loss           | 0.0657       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1376000, episode_reward=0.76 +/- 2.32
Episode length: 277.00 +/- 46.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 277      |
|    mean_reward     | 0.763    |
| time/              |          |
|    total_timesteps | 1376000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 624     |
|    iterations      | 672     |
|    time_elapsed    | 2203    |
|    total_timesteps | 1376256 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1377000, episode_reward=-0.31 +/- 0.75
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.311      |
| time/                   |             |
|    total_timesteps      | 1377000     |
| train/                  |             |
|    approx_kl            | 0.004543516 |
|    clip_fraction        | 0.0473      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | 0.909       |
|    learning_rate        | 4.65e-05    |
|    loss                 | 0.0113      |
|    n_updates            | 6720        |
|    policy_gradient_loss | -0.00549    |
|    std                  | 0.588       |
|    value_loss           | 0.0335      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1378000, episode_reward=2.82 +/- 2.99
Episode length: 231.40 +/- 68.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 231      |
|    mean_reward     | 2.82     |
| time/              |          |
|    total_timesteps | 1378000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 624     |
|    iterations      | 673     |
|    time_elapsed    | 2207    |
|    total_timesteps | 1378304 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1379000, episode_reward=2.89 +/- 3.10
Episode length: 233.00 +/- 55.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 233          |
|    mean_reward          | 2.89         |
| time/                   |              |
|    total_timesteps      | 1379000      |
| train/                  |              |
|    approx_kl            | 0.0061714286 |
|    clip_fraction        | 0.0766       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.77        |
|    explained_variance   | 0.856        |
|    learning_rate        | 4.65e-05     |
|    loss                 | -0.0118      |
|    n_updates            | 6730         |
|    policy_gradient_loss | -0.0092      |
|    std                  | 0.587        |
|    value_loss           | 0.104        |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1380000, episode_reward=4.02 +/- 2.51
Episode length: 195.40 +/- 54.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 195      |
|    mean_reward     | 4.02     |
| time/              |          |
|    total_timesteps | 1380000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 624     |
|    iterations      | 674     |
|    time_elapsed    | 2209    |
|    total_timesteps | 1380352 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1381000, episode_reward=3.05 +/- 2.71
Episode length: 219.40 +/- 67.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 219          |
|    mean_reward          | 3.05         |
| time/                   |              |
|    total_timesteps      | 1381000      |
| train/                  |              |
|    approx_kl            | 0.0046181106 |
|    clip_fraction        | 0.0317       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.77        |
|    explained_variance   | 0.842        |
|    learning_rate        | 4.66e-05     |
|    loss                 | -0.00369     |
|    n_updates            | 6740         |
|    policy_gradient_loss | -0.00481     |
|    std                  | 0.587        |
|    value_loss           | 0.0571       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1382000, episode_reward=3.42 +/- 2.27
Episode length: 206.00 +/- 77.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | 3.42     |
| time/              |          |
|    total_timesteps | 1382000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 624     |
|    iterations      | 675     |
|    time_elapsed    | 2212    |
|    total_timesteps | 1382400 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1383000, episode_reward=1.38 +/- 3.05
Episode length: 253.60 +/- 57.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 254         |
|    mean_reward          | 1.38        |
| time/                   |             |
|    total_timesteps      | 1383000     |
| train/                  |             |
|    approx_kl            | 0.005410232 |
|    clip_fraction        | 0.0304      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | 0.903       |
|    learning_rate        | 4.66e-05    |
|    loss                 | 0.0374      |
|    n_updates            | 6750        |
|    policy_gradient_loss | -0.00451    |
|    std                  | 0.586       |
|    value_loss           | 0.0423      |
-----------------------------------------
box reached target
Eval num_timesteps=1384000, episode_reward=-1.06 +/- 0.22
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1.06    |
| time/              |          |
|    total_timesteps | 1384000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 624     |
|    iterations      | 676     |
|    time_elapsed    | 2215    |
|    total_timesteps | 1384448 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1385000, episode_reward=0.33 +/- 2.55
Episode length: 282.60 +/- 34.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 283          |
|    mean_reward          | 0.329        |
| time/                   |              |
|    total_timesteps      | 1385000      |
| train/                  |              |
|    approx_kl            | 0.0054469155 |
|    clip_fraction        | 0.0262       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.77        |
|    explained_variance   | 0.946        |
|    learning_rate        | 4.66e-05     |
|    loss                 | -0.00831     |
|    n_updates            | 6760         |
|    policy_gradient_loss | -0.00487     |
|    std                  | 0.586        |
|    value_loss           | 0.0216       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1386000, episode_reward=2.02 +/- 2.74
Episode length: 249.60 +/- 62.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 250      |
|    mean_reward     | 2.02     |
| time/              |          |
|    total_timesteps | 1386000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 624     |
|    iterations      | 677     |
|    time_elapsed    | 2219    |
|    total_timesteps | 1386496 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1387000, episode_reward=3.03 +/- 2.96
Episode length: 231.40 +/- 60.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 231          |
|    mean_reward          | 3.03         |
| time/                   |              |
|    total_timesteps      | 1387000      |
| train/                  |              |
|    approx_kl            | 0.0050274935 |
|    clip_fraction        | 0.0394       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.77        |
|    explained_variance   | 0.925        |
|    learning_rate        | 4.66e-05     |
|    loss                 | -0.000349    |
|    n_updates            | 6770         |
|    policy_gradient_loss | -0.00648     |
|    std                  | 0.585        |
|    value_loss           | 0.0366       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1388000, episode_reward=1.44 +/- 3.16
Episode length: 255.20 +/- 55.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 255      |
|    mean_reward     | 1.44     |
| time/              |          |
|    total_timesteps | 1388000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 624     |
|    iterations      | 678     |
|    time_elapsed    | 2222    |
|    total_timesteps | 1388544 |
--------------------------------
box reached target
Eval num_timesteps=1389000, episode_reward=0.69 +/- 2.32
Episode length: 270.80 +/- 58.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 271          |
|    mean_reward          | 0.686        |
| time/                   |              |
|    total_timesteps      | 1389000      |
| train/                  |              |
|    approx_kl            | 0.0039222892 |
|    clip_fraction        | 0.0232       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.77        |
|    explained_variance   | 0.842        |
|    learning_rate        | 4.67e-05     |
|    loss                 | -0.0224      |
|    n_updates            | 6780         |
|    policy_gradient_loss | -0.00366     |
|    std                  | 0.586        |
|    value_loss           | 0.0709       |
------------------------------------------
box reached target
Eval num_timesteps=1390000, episode_reward=0.92 +/- 2.23
Episode length: 270.80 +/- 58.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 271      |
|    mean_reward     | 0.923    |
| time/              |          |
|    total_timesteps | 1390000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 624     |
|    iterations      | 679     |
|    time_elapsed    | 2225    |
|    total_timesteps | 1390592 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1391000, episode_reward=2.16 +/- 2.66
Episode length: 260.60 +/- 48.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 261          |
|    mean_reward          | 2.16         |
| time/                   |              |
|    total_timesteps      | 1391000      |
| train/                  |              |
|    approx_kl            | 0.0047595412 |
|    clip_fraction        | 0.0391       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.77        |
|    explained_variance   | 0.844        |
|    learning_rate        | 4.67e-05     |
|    loss                 | -0.0372      |
|    n_updates            | 6790         |
|    policy_gradient_loss | -0.00429     |
|    std                  | 0.586        |
|    value_loss           | 0.0262       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1392000, episode_reward=1.67 +/- 3.09
Episode length: 250.40 +/- 61.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 250      |
|    mean_reward     | 1.67     |
| time/              |          |
|    total_timesteps | 1392000  |
---------------------------------
box reached target
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 625     |
|    iterations      | 680     |
|    time_elapsed    | 2228    |
|    total_timesteps | 1392640 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1393000, episode_reward=0.41 +/- 2.40
Episode length: 270.60 +/- 58.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 271         |
|    mean_reward          | 0.408       |
| time/                   |             |
|    total_timesteps      | 1393000     |
| train/                  |             |
|    approx_kl            | 0.006245463 |
|    clip_fraction        | 0.0412      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | 0.981       |
|    learning_rate        | 4.67e-05    |
|    loss                 | 0.0118      |
|    n_updates            | 6800        |
|    policy_gradient_loss | -0.00624    |
|    std                  | 0.586       |
|    value_loss           | 0.0158      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1394000, episode_reward=4.06 +/- 2.48
Episode length: 227.00 +/- 42.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 227      |
|    mean_reward     | 4.06     |
| time/              |          |
|    total_timesteps | 1394000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 625     |
|    iterations      | 681     |
|    time_elapsed    | 2231    |
|    total_timesteps | 1394688 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1395000, episode_reward=2.06 +/- 2.73
Episode length: 250.40 +/- 61.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 250         |
|    mean_reward          | 2.06        |
| time/                   |             |
|    total_timesteps      | 1395000     |
| train/                  |             |
|    approx_kl            | 0.004598488 |
|    clip_fraction        | 0.0643      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | 0.94        |
|    learning_rate        | 4.67e-05    |
|    loss                 | 0.0334      |
|    n_updates            | 6810        |
|    policy_gradient_loss | -0.00684    |
|    std                  | 0.585       |
|    value_loss           | 0.0419      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1396000, episode_reward=3.09 +/- 2.87
Episode length: 231.20 +/- 61.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 231      |
|    mean_reward     | 3.09     |
| time/              |          |
|    total_timesteps | 1396000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 625     |
|    iterations      | 682     |
|    time_elapsed    | 2234    |
|    total_timesteps | 1396736 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1397000, episode_reward=3.13 +/- 2.83
Episode length: 239.00 +/- 52.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 239          |
|    mean_reward          | 3.13         |
| time/                   |              |
|    total_timesteps      | 1397000      |
| train/                  |              |
|    approx_kl            | 0.0058354526 |
|    clip_fraction        | 0.054        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.77        |
|    explained_variance   | 0.926        |
|    learning_rate        | 4.68e-05     |
|    loss                 | -0.00412     |
|    n_updates            | 6820         |
|    policy_gradient_loss | -0.00483     |
|    std                  | 0.585        |
|    value_loss           | 0.0377       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1398000, episode_reward=3.01 +/- 2.75
Episode length: 230.20 +/- 61.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 230      |
|    mean_reward     | 3.01     |
| time/              |          |
|    total_timesteps | 1398000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 625     |
|    iterations      | 683     |
|    time_elapsed    | 2237    |
|    total_timesteps | 1398784 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1399000, episode_reward=1.58 +/- 3.01
Episode length: 255.60 +/- 54.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 256         |
|    mean_reward          | 1.58        |
| time/                   |             |
|    total_timesteps      | 1399000     |
| train/                  |             |
|    approx_kl            | 0.005875447 |
|    clip_fraction        | 0.0432      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | 0.951       |
|    learning_rate        | 4.68e-05    |
|    loss                 | 0.00799     |
|    n_updates            | 6830        |
|    policy_gradient_loss | -0.00683    |
|    std                  | 0.585       |
|    value_loss           | 0.0353      |
-----------------------------------------
Eval num_timesteps=1400000, episode_reward=-0.40 +/- 0.87
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.405   |
| time/              |          |
|    total_timesteps | 1400000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 625     |
|    iterations      | 684     |
|    time_elapsed    | 2240    |
|    total_timesteps | 1400832 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1401000, episode_reward=3.09 +/- 2.70
Episode length: 223.20 +/- 65.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 223          |
|    mean_reward          | 3.09         |
| time/                   |              |
|    total_timesteps      | 1401000      |
| train/                  |              |
|    approx_kl            | 0.0052979845 |
|    clip_fraction        | 0.0454       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.77        |
|    explained_variance   | 0.918        |
|    learning_rate        | 4.68e-05     |
|    loss                 | 0.0124       |
|    n_updates            | 6840         |
|    policy_gradient_loss | -0.007       |
|    std                  | 0.586        |
|    value_loss           | 0.0388       |
------------------------------------------
Eval num_timesteps=1402000, episode_reward=-0.15 +/- 0.71
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.154   |
| time/              |          |
|    total_timesteps | 1402000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 625     |
|    iterations      | 685     |
|    time_elapsed    | 2243    |
|    total_timesteps | 1402880 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1403000, episode_reward=1.49 +/- 3.00
Episode length: 241.60 +/- 71.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 242         |
|    mean_reward          | 1.49        |
| time/                   |             |
|    total_timesteps      | 1403000     |
| train/                  |             |
|    approx_kl            | 0.007060543 |
|    clip_fraction        | 0.0599      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | 0.906       |
|    learning_rate        | 4.68e-05    |
|    loss                 | 0.0397      |
|    n_updates            | 6850        |
|    policy_gradient_loss | -0.01       |
|    std                  | 0.587       |
|    value_loss           | 0.0271      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1404000, episode_reward=1.69 +/- 2.96
Episode length: 245.80 +/- 66.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 246      |
|    mean_reward     | 1.69     |
| time/              |          |
|    total_timesteps | 1404000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 625     |
|    iterations      | 686     |
|    time_elapsed    | 2246    |
|    total_timesteps | 1404928 |
--------------------------------
box reached target
Eval num_timesteps=1405000, episode_reward=0.52 +/- 2.52
Episode length: 281.20 +/- 37.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 281         |
|    mean_reward          | 0.52        |
| time/                   |             |
|    total_timesteps      | 1405000     |
| train/                  |             |
|    approx_kl            | 0.004208753 |
|    clip_fraction        | 0.0195      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | 0.903       |
|    learning_rate        | 4.69e-05    |
|    loss                 | 0.052       |
|    n_updates            | 6860        |
|    policy_gradient_loss | -0.00293    |
|    std                  | 0.586       |
|    value_loss           | 0.0476      |
-----------------------------------------
box reached target
Eval num_timesteps=1406000, episode_reward=0.52 +/- 2.31
Episode length: 271.40 +/- 57.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 271      |
|    mean_reward     | 0.524    |
| time/              |          |
|    total_timesteps | 1406000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 625     |
|    iterations      | 687     |
|    time_elapsed    | 2249    |
|    total_timesteps | 1406976 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1407000, episode_reward=3.07 +/- 2.76
Episode length: 226.40 +/- 62.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 226         |
|    mean_reward          | 3.07        |
| time/                   |             |
|    total_timesteps      | 1407000     |
| train/                  |             |
|    approx_kl            | 0.005469973 |
|    clip_fraction        | 0.0334      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | 0.884       |
|    learning_rate        | 4.69e-05    |
|    loss                 | -0.014      |
|    n_updates            | 6870        |
|    policy_gradient_loss | -0.00714    |
|    std                  | 0.587       |
|    value_loss           | 0.0277      |
-----------------------------------------
box reached target
Eval num_timesteps=1408000, episode_reward=0.82 +/- 2.22
Episode length: 270.60 +/- 58.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 271      |
|    mean_reward     | 0.819    |
| time/              |          |
|    total_timesteps | 1408000  |
---------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1409000, episode_reward=0.56 +/- 2.53
Episode length: 282.60 +/- 34.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 283      |
|    mean_reward     | 0.559    |
| time/              |          |
|    total_timesteps | 1409000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 625     |
|    iterations      | 688     |
|    time_elapsed    | 2253    |
|    total_timesteps | 1409024 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1410000, episode_reward=2.07 +/- 2.83
Episode length: 261.80 +/- 59.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 262          |
|    mean_reward          | 2.07         |
| time/                   |              |
|    total_timesteps      | 1410000      |
| train/                  |              |
|    approx_kl            | 0.0048467293 |
|    clip_fraction        | 0.0228       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.77        |
|    explained_variance   | 0.922        |
|    learning_rate        | 4.69e-05     |
|    loss                 | 0.0435       |
|    n_updates            | 6880         |
|    policy_gradient_loss | -0.00369     |
|    std                  | 0.588        |
|    value_loss           | 0.0331       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1411000, episode_reward=1.29 +/- 3.19
Episode length: 246.60 +/- 65.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 247      |
|    mean_reward     | 1.29     |
| time/              |          |
|    total_timesteps | 1411000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 625     |
|    iterations      | 689     |
|    time_elapsed    | 2256    |
|    total_timesteps | 1411072 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1412000, episode_reward=2.93 +/- 2.83
Episode length: 228.40 +/- 61.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 228         |
|    mean_reward          | 2.93        |
| time/                   |             |
|    total_timesteps      | 1412000     |
| train/                  |             |
|    approx_kl            | 0.004594451 |
|    clip_fraction        | 0.0291      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | 0.893       |
|    learning_rate        | 4.69e-05    |
|    loss                 | 0.00359     |
|    n_updates            | 6890        |
|    policy_gradient_loss | -0.00397    |
|    std                  | 0.587       |
|    value_loss           | 0.0239      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=1413000, episode_reward=0.94 +/- 2.24
Episode length: 275.60 +/- 48.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 276      |
|    mean_reward     | 0.941    |
| time/              |          |
|    total_timesteps | 1413000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 625     |
|    iterations      | 690     |
|    time_elapsed    | 2259    |
|    total_timesteps | 1413120 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1414000, episode_reward=0.31 +/- 2.46
Episode length: 276.20 +/- 47.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 276        |
|    mean_reward          | 0.306      |
| time/                   |            |
|    total_timesteps      | 1414000    |
| train/                  |            |
|    approx_kl            | 0.00522086 |
|    clip_fraction        | 0.0325     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.77      |
|    explained_variance   | 0.928      |
|    learning_rate        | 4.7e-05    |
|    loss                 | -0.00568   |
|    n_updates            | 6900       |
|    policy_gradient_loss | -0.00441   |
|    std                  | 0.586      |
|    value_loss           | 0.0241     |
----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1415000, episode_reward=0.99 +/- 2.21
Episode length: 273.40 +/- 53.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 273      |
|    mean_reward     | 0.99     |
| time/              |          |
|    total_timesteps | 1415000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 625     |
|    iterations      | 691     |
|    time_elapsed    | 2262    |
|    total_timesteps | 1415168 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1416000, episode_reward=1.89 +/- 3.07
Episode length: 265.20 +/- 42.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 265          |
|    mean_reward          | 1.89         |
| time/                   |              |
|    total_timesteps      | 1416000      |
| train/                  |              |
|    approx_kl            | 0.0024440067 |
|    clip_fraction        | 0.0117       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.77        |
|    explained_variance   | 0.837        |
|    learning_rate        | 4.7e-05      |
|    loss                 | 0.0149       |
|    n_updates            | 6910         |
|    policy_gradient_loss | -0.00153     |
|    std                  | 0.584        |
|    value_loss           | 0.132        |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1417000, episode_reward=3.18 +/- 2.45
Episode length: 221.60 +/- 64.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 222      |
|    mean_reward     | 3.18     |
| time/              |          |
|    total_timesteps | 1417000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 625     |
|    iterations      | 692     |
|    time_elapsed    | 2265    |
|    total_timesteps | 1417216 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1418000, episode_reward=1.83 +/- 2.89
Episode length: 243.00 +/- 69.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 243          |
|    mean_reward          | 1.83         |
| time/                   |              |
|    total_timesteps      | 1418000      |
| train/                  |              |
|    approx_kl            | 0.0047056945 |
|    clip_fraction        | 0.0342       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.76        |
|    explained_variance   | 0.896        |
|    learning_rate        | 4.7e-05      |
|    loss                 | -0.0164      |
|    n_updates            | 6920         |
|    policy_gradient_loss | -0.00716     |
|    std                  | 0.585        |
|    value_loss           | 0.0658       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1419000, episode_reward=0.55 +/- 2.38
Episode length: 279.20 +/- 41.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 279      |
|    mean_reward     | 0.547    |
| time/              |          |
|    total_timesteps | 1419000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 625     |
|    iterations      | 693     |
|    time_elapsed    | 2268    |
|    total_timesteps | 1419264 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1420000, episode_reward=1.39 +/- 3.06
Episode length: 242.80 +/- 70.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 243          |
|    mean_reward          | 1.39         |
| time/                   |              |
|    total_timesteps      | 1420000      |
| train/                  |              |
|    approx_kl            | 0.0048959693 |
|    clip_fraction        | 0.0426       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.77        |
|    explained_variance   | 0.93         |
|    learning_rate        | 4.7e-05      |
|    loss                 | -0.00553     |
|    n_updates            | 6930         |
|    policy_gradient_loss | -0.00395     |
|    std                  | 0.586        |
|    value_loss           | 0.0368       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1421000, episode_reward=1.65 +/- 3.19
Episode length: 259.60 +/- 51.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 260      |
|    mean_reward     | 1.65     |
| time/              |          |
|    total_timesteps | 1421000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 625     |
|    iterations      | 694     |
|    time_elapsed    | 2271    |
|    total_timesteps | 1421312 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1422000, episode_reward=1.49 +/- 3.06
Episode length: 245.20 +/- 68.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 245          |
|    mean_reward          | 1.49         |
| time/                   |              |
|    total_timesteps      | 1422000      |
| train/                  |              |
|    approx_kl            | 0.0038679116 |
|    clip_fraction        | 0.0334       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.77        |
|    explained_variance   | 0.868        |
|    learning_rate        | 4.71e-05     |
|    loss                 | -0.00576     |
|    n_updates            | 6940         |
|    policy_gradient_loss | -0.00439     |
|    std                  | 0.586        |
|    value_loss           | 0.0445       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1423000, episode_reward=3.09 +/- 2.80
Episode length: 234.80 +/- 53.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 235      |
|    mean_reward     | 3.09     |
| time/              |          |
|    total_timesteps | 1423000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 625     |
|    iterations      | 695     |
|    time_elapsed    | 2274    |
|    total_timesteps | 1423360 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1424000, episode_reward=1.45 +/- 3.02
Episode length: 255.40 +/- 54.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 255          |
|    mean_reward          | 1.45         |
| time/                   |              |
|    total_timesteps      | 1424000      |
| train/                  |              |
|    approx_kl            | 0.0057463674 |
|    clip_fraction        | 0.0395       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.77        |
|    explained_variance   | 0.933        |
|    learning_rate        | 4.71e-05     |
|    loss                 | 0.00558      |
|    n_updates            | 6950         |
|    policy_gradient_loss | -0.00385     |
|    std                  | 0.587        |
|    value_loss           | 0.023        |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1425000, episode_reward=0.52 +/- 2.33
Episode length: 270.60 +/- 58.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 271      |
|    mean_reward     | 0.517    |
| time/              |          |
|    total_timesteps | 1425000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 625     |
|    iterations      | 696     |
|    time_elapsed    | 2277    |
|    total_timesteps | 1425408 |
--------------------------------
box reached target
Eval num_timesteps=1426000, episode_reward=-0.96 +/- 0.39
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.958       |
| time/                   |              |
|    total_timesteps      | 1426000      |
| train/                  |              |
|    approx_kl            | 0.0060027037 |
|    clip_fraction        | 0.0387       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.77        |
|    explained_variance   | 0.949        |
|    learning_rate        | 4.71e-05     |
|    loss                 | -0.0275      |
|    n_updates            | 6960         |
|    policy_gradient_loss | -0.00712     |
|    std                  | 0.586        |
|    value_loss           | 0.0245       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1427000, episode_reward=0.45 +/- 2.49
Episode length: 273.00 +/- 54.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 273      |
|    mean_reward     | 0.445    |
| time/              |          |
|    total_timesteps | 1427000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 625     |
|    iterations      | 697     |
|    time_elapsed    | 2280    |
|    total_timesteps | 1427456 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1428000, episode_reward=2.77 +/- 3.09
Episode length: 219.80 +/- 66.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 220          |
|    mean_reward          | 2.77         |
| time/                   |              |
|    total_timesteps      | 1428000      |
| train/                  |              |
|    approx_kl            | 0.0033419132 |
|    clip_fraction        | 0.0212       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.77        |
|    explained_variance   | 0.936        |
|    learning_rate        | 4.71e-05     |
|    loss                 | 0.0106       |
|    n_updates            | 6970         |
|    policy_gradient_loss | -0.00359     |
|    std                  | 0.587        |
|    value_loss           | 0.0428       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1429000, episode_reward=5.24 +/- 0.08
Episode length: 172.80 +/- 14.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 173      |
|    mean_reward     | 5.24     |
| time/              |          |
|    total_timesteps | 1429000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 626     |
|    iterations      | 698     |
|    time_elapsed    | 2283    |
|    total_timesteps | 1429504 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1430000, episode_reward=1.45 +/- 3.12
Episode length: 246.40 +/- 66.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 246         |
|    mean_reward          | 1.45        |
| time/                   |             |
|    total_timesteps      | 1430000     |
| train/                  |             |
|    approx_kl            | 0.004097374 |
|    clip_fraction        | 0.0198      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | 0.911       |
|    learning_rate        | 4.72e-05    |
|    loss                 | 0.0224      |
|    n_updates            | 6980        |
|    policy_gradient_loss | -0.00674    |
|    std                  | 0.585       |
|    value_loss           | 0.0831      |
-----------------------------------------
box reached target
Eval num_timesteps=1431000, episode_reward=0.63 +/- 2.42
Episode length: 276.40 +/- 47.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 276      |
|    mean_reward     | 0.633    |
| time/              |          |
|    total_timesteps | 1431000  |
---------------------------------
box reached target
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 626     |
|    iterations      | 699     |
|    time_elapsed    | 2286    |
|    total_timesteps | 1431552 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1432000, episode_reward=3.38 +/- 2.56
Episode length: 230.80 +/- 56.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 231          |
|    mean_reward          | 3.38         |
| time/                   |              |
|    total_timesteps      | 1432000      |
| train/                  |              |
|    approx_kl            | 0.0047917105 |
|    clip_fraction        | 0.0371       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.76        |
|    explained_variance   | 0.785        |
|    learning_rate        | 4.72e-05     |
|    loss                 | -0.00129     |
|    n_updates            | 6990         |
|    policy_gradient_loss | -0.00667     |
|    std                  | 0.584        |
|    value_loss           | 0.0791       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1433000, episode_reward=4.09 +/- 2.26
Episode length: 184.20 +/- 60.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 184      |
|    mean_reward     | 4.09     |
| time/              |          |
|    total_timesteps | 1433000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 626     |
|    iterations      | 700     |
|    time_elapsed    | 2289    |
|    total_timesteps | 1433600 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1434000, episode_reward=2.69 +/- 3.18
Episode length: 233.20 +/- 59.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 233          |
|    mean_reward          | 2.69         |
| time/                   |              |
|    total_timesteps      | 1434000      |
| train/                  |              |
|    approx_kl            | 0.0061016437 |
|    clip_fraction        | 0.0331       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.76        |
|    explained_variance   | 0.967        |
|    learning_rate        | 4.72e-05     |
|    loss                 | 0.0238       |
|    n_updates            | 7000         |
|    policy_gradient_loss | -0.00648     |
|    std                  | 0.584        |
|    value_loss           | 0.0147       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=1435000, episode_reward=2.09 +/- 2.73
Episode length: 255.20 +/- 54.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 255      |
|    mean_reward     | 2.09     |
| time/              |          |
|    total_timesteps | 1435000  |
---------------------------------
box reached target
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 626     |
|    iterations      | 701     |
|    time_elapsed    | 2292    |
|    total_timesteps | 1435648 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1436000, episode_reward=3.22 +/- 2.67
Episode length: 227.80 +/- 60.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 228          |
|    mean_reward          | 3.22         |
| time/                   |              |
|    total_timesteps      | 1436000      |
| train/                  |              |
|    approx_kl            | 0.0028626572 |
|    clip_fraction        | 0.0139       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.76        |
|    explained_variance   | 0.862        |
|    learning_rate        | 4.72e-05     |
|    loss                 | 0.00254      |
|    n_updates            | 7010         |
|    policy_gradient_loss | -0.00172     |
|    std                  | 0.583        |
|    value_loss           | 0.0404       |
------------------------------------------
box reached target
Eval num_timesteps=1437000, episode_reward=0.77 +/- 2.18
Episode length: 279.20 +/- 41.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 279      |
|    mean_reward     | 0.765    |
| time/              |          |
|    total_timesteps | 1437000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 626     |
|    iterations      | 702     |
|    time_elapsed    | 2295    |
|    total_timesteps | 1437696 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1438000, episode_reward=3.35 +/- 2.72
Episode length: 235.80 +/- 59.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 236         |
|    mean_reward          | 3.35        |
| time/                   |             |
|    total_timesteps      | 1438000     |
| train/                  |             |
|    approx_kl            | 0.004916948 |
|    clip_fraction        | 0.0384      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.76       |
|    explained_variance   | 0.85        |
|    learning_rate        | 4.73e-05    |
|    loss                 | -0.00727    |
|    n_updates            | 7020        |
|    policy_gradient_loss | -0.00393    |
|    std                  | 0.583       |
|    value_loss           | 0.0516      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1439000, episode_reward=1.97 +/- 2.84
Episode length: 256.80 +/- 56.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 257      |
|    mean_reward     | 1.97     |
| time/              |          |
|    total_timesteps | 1439000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 626     |
|    iterations      | 703     |
|    time_elapsed    | 2298    |
|    total_timesteps | 1439744 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1440000, episode_reward=2.17 +/- 2.56
Episode length: 249.60 +/- 62.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 250          |
|    mean_reward          | 2.17         |
| time/                   |              |
|    total_timesteps      | 1440000      |
| train/                  |              |
|    approx_kl            | 0.0058899852 |
|    clip_fraction        | 0.0547       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.76        |
|    explained_variance   | 0.859        |
|    learning_rate        | 4.73e-05     |
|    loss                 | -0.000356    |
|    n_updates            | 7030         |
|    policy_gradient_loss | -0.00833     |
|    std                  | 0.584        |
|    value_loss           | 0.0507       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1441000, episode_reward=0.89 +/- 2.26
Episode length: 271.20 +/- 57.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 271      |
|    mean_reward     | 0.892    |
| time/              |          |
|    total_timesteps | 1441000  |
---------------------------------
box reached target
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 626     |
|    iterations      | 704     |
|    time_elapsed    | 2301    |
|    total_timesteps | 1441792 |
--------------------------------
box reached target
Eval num_timesteps=1442000, episode_reward=-0.27 +/- 0.39
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.272       |
| time/                   |              |
|    total_timesteps      | 1442000      |
| train/                  |              |
|    approx_kl            | 0.0061169746 |
|    clip_fraction        | 0.0477       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.76        |
|    explained_variance   | 0.918        |
|    learning_rate        | 4.73e-05     |
|    loss                 | -0.0191      |
|    n_updates            | 7040         |
|    policy_gradient_loss | -0.00693     |
|    std                  | 0.584        |
|    value_loss           | 0.0497       |
------------------------------------------
Eval num_timesteps=1443000, episode_reward=-0.68 +/- 0.60
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.678   |
| time/              |          |
|    total_timesteps | 1443000  |
---------------------------------
box reached target
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 626     |
|    iterations      | 705     |
|    time_elapsed    | 2304    |
|    total_timesteps | 1443840 |
--------------------------------
box reached target
Eval num_timesteps=1444000, episode_reward=0.71 +/- 2.31
Episode length: 270.20 +/- 59.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 270          |
|    mean_reward          | 0.709        |
| time/                   |              |
|    total_timesteps      | 1444000      |
| train/                  |              |
|    approx_kl            | 0.0044618817 |
|    clip_fraction        | 0.0359       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.76        |
|    explained_variance   | 0.919        |
|    learning_rate        | 4.73e-05     |
|    loss                 | 0.0144       |
|    n_updates            | 7050         |
|    policy_gradient_loss | -0.00369     |
|    std                  | 0.583        |
|    value_loss           | 0.0484       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1445000, episode_reward=2.07 +/- 2.71
Episode length: 241.40 +/- 72.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 241      |
|    mean_reward     | 2.07     |
| time/              |          |
|    total_timesteps | 1445000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 626     |
|    iterations      | 706     |
|    time_elapsed    | 2307    |
|    total_timesteps | 1445888 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1446000, episode_reward=4.03 +/- 2.52
Episode length: 211.60 +/- 47.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 212          |
|    mean_reward          | 4.03         |
| time/                   |              |
|    total_timesteps      | 1446000      |
| train/                  |              |
|    approx_kl            | 0.0050136885 |
|    clip_fraction        | 0.03         |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.76        |
|    explained_variance   | 0.96         |
|    learning_rate        | 4.74e-05     |
|    loss                 | -0.00864     |
|    n_updates            | 7060         |
|    policy_gradient_loss | -0.00293     |
|    std                  | 0.584        |
|    value_loss           | 0.00971      |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1447000, episode_reward=0.74 +/- 2.32
Episode length: 270.00 +/- 60.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 270      |
|    mean_reward     | 0.744    |
| time/              |          |
|    total_timesteps | 1447000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 626     |
|    iterations      | 707     |
|    time_elapsed    | 2310    |
|    total_timesteps | 1447936 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1448000, episode_reward=1.73 +/- 2.85
Episode length: 242.00 +/- 71.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 242          |
|    mean_reward          | 1.73         |
| time/                   |              |
|    total_timesteps      | 1448000      |
| train/                  |              |
|    approx_kl            | 0.0036209216 |
|    clip_fraction        | 0.024        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.76        |
|    explained_variance   | 0.734        |
|    learning_rate        | 4.74e-05     |
|    loss                 | 0.0345       |
|    n_updates            | 7070         |
|    policy_gradient_loss | -0.0076      |
|    std                  | 0.582        |
|    value_loss           | 0.0756       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=1449000, episode_reward=1.88 +/- 2.70
Episode length: 244.80 +/- 68.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 245      |
|    mean_reward     | 1.88     |
| time/              |          |
|    total_timesteps | 1449000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 626     |
|    iterations      | 708     |
|    time_elapsed    | 2313    |
|    total_timesteps | 1449984 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1450000, episode_reward=2.07 +/- 2.55
Episode length: 246.40 +/- 67.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 246          |
|    mean_reward          | 2.07         |
| time/                   |              |
|    total_timesteps      | 1450000      |
| train/                  |              |
|    approx_kl            | 0.0033685775 |
|    clip_fraction        | 0.0267       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.75        |
|    explained_variance   | 0.939        |
|    learning_rate        | 4.74e-05     |
|    loss                 | -0.00729     |
|    n_updates            | 7080         |
|    policy_gradient_loss | -0.00446     |
|    std                  | 0.581        |
|    value_loss           | 0.00871      |
------------------------------------------
box reached target
Eval num_timesteps=1451000, episode_reward=-0.07 +/- 0.61
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.0739  |
| time/              |          |
|    total_timesteps | 1451000  |
---------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1452000, episode_reward=3.28 +/- 2.44
Episode length: 227.20 +/- 60.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 227      |
|    mean_reward     | 3.28     |
| time/              |          |
|    total_timesteps | 1452000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 626     |
|    iterations      | 709     |
|    time_elapsed    | 2317    |
|    total_timesteps | 1452032 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1453000, episode_reward=2.77 +/- 3.22
Episode length: 236.00 +/- 54.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 236         |
|    mean_reward          | 2.77        |
| time/                   |             |
|    total_timesteps      | 1453000     |
| train/                  |             |
|    approx_kl            | 0.004143217 |
|    clip_fraction        | 0.0366      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.75       |
|    explained_variance   | 0.89        |
|    learning_rate        | 4.74e-05    |
|    loss                 | 0.0145      |
|    n_updates            | 7090        |
|    policy_gradient_loss | -0.00466    |
|    std                  | 0.58        |
|    value_loss           | 0.0409      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1454000, episode_reward=3.99 +/- 2.50
Episode length: 203.40 +/- 48.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 203      |
|    mean_reward     | 3.99     |
| time/              |          |
|    total_timesteps | 1454000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 626     |
|    iterations      | 710     |
|    time_elapsed    | 2320    |
|    total_timesteps | 1454080 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1455000, episode_reward=2.79 +/- 3.10
Episode length: 221.20 +/- 64.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 221         |
|    mean_reward          | 2.79        |
| time/                   |             |
|    total_timesteps      | 1455000     |
| train/                  |             |
|    approx_kl            | 0.005136258 |
|    clip_fraction        | 0.0382      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | 0.972       |
|    learning_rate        | 4.74e-05    |
|    loss                 | -0.0132     |
|    n_updates            | 7100        |
|    policy_gradient_loss | -0.00528    |
|    std                  | 0.578       |
|    value_loss           | 0.00642     |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1456000, episode_reward=2.83 +/- 2.90
Episode length: 224.80 +/- 63.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 225      |
|    mean_reward     | 2.83     |
| time/              |          |
|    total_timesteps | 1456000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 626     |
|    iterations      | 711     |
|    time_elapsed    | 2323    |
|    total_timesteps | 1456128 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1457000, episode_reward=1.68 +/- 2.94
Episode length: 254.60 +/- 56.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 255         |
|    mean_reward          | 1.68        |
| time/                   |             |
|    total_timesteps      | 1457000     |
| train/                  |             |
|    approx_kl            | 0.004263834 |
|    clip_fraction        | 0.0313      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | 0.769       |
|    learning_rate        | 4.75e-05    |
|    loss                 | 0.0495      |
|    n_updates            | 7110        |
|    policy_gradient_loss | -0.00493    |
|    std                  | 0.579       |
|    value_loss           | 0.0992      |
-----------------------------------------
Eval num_timesteps=1458000, episode_reward=-0.72 +/- 0.57
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.717   |
| time/              |          |
|    total_timesteps | 1458000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 626     |
|    iterations      | 712     |
|    time_elapsed    | 2326    |
|    total_timesteps | 1458176 |
--------------------------------
Eval num_timesteps=1459000, episode_reward=-0.82 +/- 0.36
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.822       |
| time/                   |              |
|    total_timesteps      | 1459000      |
| train/                  |              |
|    approx_kl            | 0.0047136513 |
|    clip_fraction        | 0.0287       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.74        |
|    explained_variance   | 0.867        |
|    learning_rate        | 4.75e-05     |
|    loss                 | 0.00482      |
|    n_updates            | 7120         |
|    policy_gradient_loss | -0.00398     |
|    std                  | 0.579        |
|    value_loss           | 0.0315       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=1460000, episode_reward=1.74 +/- 3.00
Episode length: 268.00 +/- 40.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 268      |
|    mean_reward     | 1.74     |
| time/              |          |
|    total_timesteps | 1460000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 626     |
|    iterations      | 713     |
|    time_elapsed    | 2329    |
|    total_timesteps | 1460224 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1461000, episode_reward=0.52 +/- 2.40
Episode length: 273.80 +/- 52.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 274          |
|    mean_reward          | 0.523        |
| time/                   |              |
|    total_timesteps      | 1461000      |
| train/                  |              |
|    approx_kl            | 0.0058571114 |
|    clip_fraction        | 0.0548       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.75        |
|    explained_variance   | 0.916        |
|    learning_rate        | 4.75e-05     |
|    loss                 | 0.00832      |
|    n_updates            | 7130         |
|    policy_gradient_loss | -0.00596     |
|    std                  | 0.581        |
|    value_loss           | 0.0149       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1462000, episode_reward=1.91 +/- 2.81
Episode length: 246.00 +/- 66.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 246      |
|    mean_reward     | 1.91     |
| time/              |          |
|    total_timesteps | 1462000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 626     |
|    iterations      | 714     |
|    time_elapsed    | 2332    |
|    total_timesteps | 1462272 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1463000, episode_reward=1.79 +/- 2.87
Episode length: 251.60 +/- 59.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 252         |
|    mean_reward          | 1.79        |
| time/                   |             |
|    total_timesteps      | 1463000     |
| train/                  |             |
|    approx_kl            | 0.004524661 |
|    clip_fraction        | 0.0295      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.75       |
|    explained_variance   | 0.912       |
|    learning_rate        | 4.75e-05    |
|    loss                 | -0.00331    |
|    n_updates            | 7140        |
|    policy_gradient_loss | -0.00334    |
|    std                  | 0.58        |
|    value_loss           | 0.0527      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=1464000, episode_reward=1.65 +/- 2.89
Episode length: 247.00 +/- 64.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 247      |
|    mean_reward     | 1.65     |
| time/              |          |
|    total_timesteps | 1464000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 626     |
|    iterations      | 715     |
|    time_elapsed    | 2335    |
|    total_timesteps | 1464320 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1465000, episode_reward=0.63 +/- 2.49
Episode length: 282.40 +/- 35.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 282         |
|    mean_reward          | 0.63        |
| time/                   |             |
|    total_timesteps      | 1465000     |
| train/                  |             |
|    approx_kl            | 0.005102125 |
|    clip_fraction        | 0.0389      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.75       |
|    explained_variance   | 0.91        |
|    learning_rate        | 4.76e-05    |
|    loss                 | -0.0178     |
|    n_updates            | 7150        |
|    policy_gradient_loss | -0.00548    |
|    std                  | 0.582       |
|    value_loss           | 0.0296      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1466000, episode_reward=2.17 +/- 2.59
Episode length: 244.60 +/- 68.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 245      |
|    mean_reward     | 2.17     |
| time/              |          |
|    total_timesteps | 1466000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 627     |
|    iterations      | 716     |
|    time_elapsed    | 2338    |
|    total_timesteps | 1466368 |
--------------------------------
box reached target
Eval num_timesteps=1467000, episode_reward=0.88 +/- 2.29
Episode length: 274.60 +/- 50.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 275          |
|    mean_reward          | 0.881        |
| time/                   |              |
|    total_timesteps      | 1467000      |
| train/                  |              |
|    approx_kl            | 0.0037384655 |
|    clip_fraction        | 0.0266       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.75        |
|    explained_variance   | 0.843        |
|    learning_rate        | 4.76e-05     |
|    loss                 | 0.0369       |
|    n_updates            | 7160         |
|    policy_gradient_loss | -0.00428     |
|    std                  | 0.581        |
|    value_loss           | 0.0444       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1468000, episode_reward=1.78 +/- 2.95
Episode length: 268.60 +/- 41.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 269      |
|    mean_reward     | 1.78     |
| time/              |          |
|    total_timesteps | 1468000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 627     |
|    iterations      | 717     |
|    time_elapsed    | 2341    |
|    total_timesteps | 1468416 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1469000, episode_reward=0.36 +/- 2.43
Episode length: 277.40 +/- 45.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 277         |
|    mean_reward          | 0.361       |
| time/                   |             |
|    total_timesteps      | 1469000     |
| train/                  |             |
|    approx_kl            | 0.004329475 |
|    clip_fraction        | 0.0344      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.75       |
|    explained_variance   | 0.954       |
|    learning_rate        | 4.76e-05    |
|    loss                 | -0.00446    |
|    n_updates            | 7170        |
|    policy_gradient_loss | -0.0063     |
|    std                  | 0.582       |
|    value_loss           | 0.0227      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1470000, episode_reward=0.80 +/- 2.35
Episode length: 288.40 +/- 23.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.805    |
| time/              |          |
|    total_timesteps | 1470000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 627     |
|    iterations      | 718     |
|    time_elapsed    | 2344    |
|    total_timesteps | 1470464 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1471000, episode_reward=2.94 +/- 2.81
Episode length: 231.40 +/- 59.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 231          |
|    mean_reward          | 2.94         |
| time/                   |              |
|    total_timesteps      | 1471000      |
| train/                  |              |
|    approx_kl            | 0.0049666744 |
|    clip_fraction        | 0.0335       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.76        |
|    explained_variance   | 0.96         |
|    learning_rate        | 4.76e-05     |
|    loss                 | 0.00471      |
|    n_updates            | 7180         |
|    policy_gradient_loss | -0.00404     |
|    std                  | 0.582        |
|    value_loss           | 0.0242       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1472000, episode_reward=3.22 +/- 2.59
Episode length: 244.40 +/- 55.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 244      |
|    mean_reward     | 3.22     |
| time/              |          |
|    total_timesteps | 1472000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 627     |
|    iterations      | 719     |
|    time_elapsed    | 2347    |
|    total_timesteps | 1472512 |
--------------------------------
box reached target
Eval num_timesteps=1473000, episode_reward=0.48 +/- 2.42
Episode length: 271.40 +/- 57.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 271          |
|    mean_reward          | 0.477        |
| time/                   |              |
|    total_timesteps      | 1473000      |
| train/                  |              |
|    approx_kl            | 0.0064561693 |
|    clip_fraction        | 0.0573       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.76        |
|    explained_variance   | 0.931        |
|    learning_rate        | 4.77e-05     |
|    loss                 | -0.0315      |
|    n_updates            | 7190         |
|    policy_gradient_loss | -0.00745     |
|    std                  | 0.583        |
|    value_loss           | 0.0238       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=1474000, episode_reward=1.98 +/- 2.80
Episode length: 259.40 +/- 50.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 259      |
|    mean_reward     | 1.98     |
| time/              |          |
|    total_timesteps | 1474000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 627     |
|    iterations      | 720     |
|    time_elapsed    | 2350    |
|    total_timesteps | 1474560 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1475000, episode_reward=2.89 +/- 3.22
Episode length: 242.40 +/- 53.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 242          |
|    mean_reward          | 2.89         |
| time/                   |              |
|    total_timesteps      | 1475000      |
| train/                  |              |
|    approx_kl            | 0.0049228994 |
|    clip_fraction        | 0.0338       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.76        |
|    explained_variance   | 0.7          |
|    learning_rate        | 4.77e-05     |
|    loss                 | 0.019        |
|    n_updates            | 7200         |
|    policy_gradient_loss | -0.00499     |
|    std                  | 0.583        |
|    value_loss           | 0.0614       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1476000, episode_reward=2.13 +/- 2.71
Episode length: 254.00 +/- 59.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 254      |
|    mean_reward     | 2.13     |
| time/              |          |
|    total_timesteps | 1476000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 627     |
|    iterations      | 721     |
|    time_elapsed    | 2353    |
|    total_timesteps | 1476608 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1477000, episode_reward=1.80 +/- 2.87
Episode length: 251.20 +/- 60.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 251          |
|    mean_reward          | 1.8          |
| time/                   |              |
|    total_timesteps      | 1477000      |
| train/                  |              |
|    approx_kl            | 0.0055906186 |
|    clip_fraction        | 0.0341       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.76        |
|    explained_variance   | 0.886        |
|    learning_rate        | 4.77e-05     |
|    loss                 | 0.00558      |
|    n_updates            | 7210         |
|    policy_gradient_loss | -0.00334     |
|    std                  | 0.584        |
|    value_loss           | 0.0471       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1478000, episode_reward=1.67 +/- 3.06
Episode length: 244.00 +/- 69.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 244      |
|    mean_reward     | 1.67     |
| time/              |          |
|    total_timesteps | 1478000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 627     |
|    iterations      | 722     |
|    time_elapsed    | 2356    |
|    total_timesteps | 1478656 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1479000, episode_reward=4.03 +/- 2.72
Episode length: 230.20 +/- 48.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 230          |
|    mean_reward          | 4.03         |
| time/                   |              |
|    total_timesteps      | 1479000      |
| train/                  |              |
|    approx_kl            | 0.0033313346 |
|    clip_fraction        | 0.0364       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.76        |
|    explained_variance   | 0.906        |
|    learning_rate        | 4.77e-05     |
|    loss                 | 0.00108      |
|    n_updates            | 7220         |
|    policy_gradient_loss | -0.00412     |
|    std                  | 0.583        |
|    value_loss           | 0.0345       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1480000, episode_reward=3.06 +/- 2.64
Episode length: 228.60 +/- 58.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 229      |
|    mean_reward     | 3.06     |
| time/              |          |
|    total_timesteps | 1480000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 627     |
|    iterations      | 723     |
|    time_elapsed    | 2359    |
|    total_timesteps | 1480704 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1481000, episode_reward=1.55 +/- 3.12
Episode length: 249.60 +/- 62.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 250          |
|    mean_reward          | 1.55         |
| time/                   |              |
|    total_timesteps      | 1481000      |
| train/                  |              |
|    approx_kl            | 0.0047759456 |
|    clip_fraction        | 0.0322       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.76        |
|    explained_variance   | 0.935        |
|    learning_rate        | 4.78e-05     |
|    loss                 | 0.0102       |
|    n_updates            | 7230         |
|    policy_gradient_loss | -0.00442     |
|    std                  | 0.581        |
|    value_loss           | 0.0174       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1482000, episode_reward=2.83 +/- 2.81
Episode length: 216.60 +/- 68.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 217      |
|    mean_reward     | 2.83     |
| time/              |          |
|    total_timesteps | 1482000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 627     |
|    iterations      | 724     |
|    time_elapsed    | 2362    |
|    total_timesteps | 1482752 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1483000, episode_reward=3.04 +/- 2.74
Episode length: 226.00 +/- 63.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 226          |
|    mean_reward          | 3.04         |
| time/                   |              |
|    total_timesteps      | 1483000      |
| train/                  |              |
|    approx_kl            | 0.0061402083 |
|    clip_fraction        | 0.0386       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.75        |
|    explained_variance   | 0.966        |
|    learning_rate        | 4.78e-05     |
|    loss                 | 0.0373       |
|    n_updates            | 7240         |
|    policy_gradient_loss | -0.00367     |
|    std                  | 0.58         |
|    value_loss           | 0.0255       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1484000, episode_reward=0.70 +/- 2.33
Episode length: 272.00 +/- 56.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 272      |
|    mean_reward     | 0.698    |
| time/              |          |
|    total_timesteps | 1484000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 627     |
|    iterations      | 725     |
|    time_elapsed    | 2365    |
|    total_timesteps | 1484800 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1485000, episode_reward=0.44 +/- 2.50
Episode length: 279.20 +/- 41.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 279          |
|    mean_reward          | 0.442        |
| time/                   |              |
|    total_timesteps      | 1485000      |
| train/                  |              |
|    approx_kl            | 0.0037417836 |
|    clip_fraction        | 0.0417       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.75        |
|    explained_variance   | 0.948        |
|    learning_rate        | 4.78e-05     |
|    loss                 | 0.0109       |
|    n_updates            | 7250         |
|    policy_gradient_loss | -0.00437     |
|    std                  | 0.58         |
|    value_loss           | 0.0173       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1486000, episode_reward=2.93 +/- 2.84
Episode length: 219.60 +/- 66.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 220      |
|    mean_reward     | 2.93     |
| time/              |          |
|    total_timesteps | 1486000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 627     |
|    iterations      | 726     |
|    time_elapsed    | 2368    |
|    total_timesteps | 1486848 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1487000, episode_reward=2.65 +/- 3.08
Episode length: 219.80 +/- 65.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 220          |
|    mean_reward          | 2.65         |
| time/                   |              |
|    total_timesteps      | 1487000      |
| train/                  |              |
|    approx_kl            | 0.0031276932 |
|    clip_fraction        | 0.0236       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.75        |
|    explained_variance   | 0.825        |
|    learning_rate        | 4.78e-05     |
|    loss                 | 0.0169       |
|    n_updates            | 7260         |
|    policy_gradient_loss | -0.0051      |
|    std                  | 0.579        |
|    value_loss           | 0.0749       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1488000, episode_reward=2.72 +/- 3.04
Episode length: 220.00 +/- 65.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 220      |
|    mean_reward     | 2.72     |
| time/              |          |
|    total_timesteps | 1488000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 627     |
|    iterations      | 727     |
|    time_elapsed    | 2371    |
|    total_timesteps | 1488896 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1489000, episode_reward=2.93 +/- 2.93
Episode length: 239.00 +/- 57.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 239          |
|    mean_reward          | 2.93         |
| time/                   |              |
|    total_timesteps      | 1489000      |
| train/                  |              |
|    approx_kl            | 0.0073628714 |
|    clip_fraction        | 0.0775       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.74        |
|    explained_variance   | 0.933        |
|    learning_rate        | 4.79e-05     |
|    loss                 | 0.00679      |
|    n_updates            | 7270         |
|    policy_gradient_loss | -0.0101      |
|    std                  | 0.579        |
|    value_loss           | 0.02         |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1490000, episode_reward=3.27 +/- 2.57
Episode length: 233.20 +/- 60.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 233      |
|    mean_reward     | 3.27     |
| time/              |          |
|    total_timesteps | 1490000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 627     |
|    iterations      | 728     |
|    time_elapsed    | 2374    |
|    total_timesteps | 1490944 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1491000, episode_reward=2.82 +/- 3.13
Episode length: 237.20 +/- 56.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 237          |
|    mean_reward          | 2.82         |
| time/                   |              |
|    total_timesteps      | 1491000      |
| train/                  |              |
|    approx_kl            | 0.0052543804 |
|    clip_fraction        | 0.0313       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.74        |
|    explained_variance   | 0.873        |
|    learning_rate        | 4.79e-05     |
|    loss                 | 0.0572       |
|    n_updates            | 7280         |
|    policy_gradient_loss | -0.00447     |
|    std                  | 0.578        |
|    value_loss           | 0.0687       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1492000, episode_reward=2.85 +/- 3.15
Episode length: 236.40 +/- 57.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 236      |
|    mean_reward     | 2.85     |
| time/              |          |
|    total_timesteps | 1492000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 627     |
|    iterations      | 729     |
|    time_elapsed    | 2377    |
|    total_timesteps | 1492992 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1493000, episode_reward=2.95 +/- 2.99
Episode length: 213.40 +/- 72.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 213          |
|    mean_reward          | 2.95         |
| time/                   |              |
|    total_timesteps      | 1493000      |
| train/                  |              |
|    approx_kl            | 0.0040429737 |
|    clip_fraction        | 0.0254       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.74        |
|    explained_variance   | 0.961        |
|    learning_rate        | 4.79e-05     |
|    loss                 | 0.0291       |
|    n_updates            | 7290         |
|    policy_gradient_loss | -0.00369     |
|    std                  | 0.578        |
|    value_loss           | 0.0154       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1494000, episode_reward=1.51 +/- 3.03
Episode length: 247.80 +/- 64.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 248      |
|    mean_reward     | 1.51     |
| time/              |          |
|    total_timesteps | 1494000  |
---------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1495000, episode_reward=0.58 +/- 2.40
Episode length: 272.80 +/- 54.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 273      |
|    mean_reward     | 0.578    |
| time/              |          |
|    total_timesteps | 1495000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 627     |
|    iterations      | 730     |
|    time_elapsed    | 2381    |
|    total_timesteps | 1495040 |
--------------------------------
box reached target
Eval num_timesteps=1496000, episode_reward=1.50 +/- 1.88
Episode length: 272.60 +/- 54.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 273         |
|    mean_reward          | 1.5         |
| time/                   |             |
|    total_timesteps      | 1496000     |
| train/                  |             |
|    approx_kl            | 0.004746452 |
|    clip_fraction        | 0.0376      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | 0.944       |
|    learning_rate        | 4.79e-05    |
|    loss                 | -0.00494    |
|    n_updates            | 7300        |
|    policy_gradient_loss | -0.00448    |
|    std                  | 0.579       |
|    value_loss           | 0.0273      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1497000, episode_reward=2.75 +/- 3.17
Episode length: 229.20 +/- 62.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 229      |
|    mean_reward     | 2.75     |
| time/              |          |
|    total_timesteps | 1497000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 627     |
|    iterations      | 731     |
|    time_elapsed    | 2384    |
|    total_timesteps | 1497088 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1498000, episode_reward=0.23 +/- 0.35
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | 0.233       |
| time/                   |             |
|    total_timesteps      | 1498000     |
| train/                  |             |
|    approx_kl            | 0.004948539 |
|    clip_fraction        | 0.036       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.75       |
|    explained_variance   | 0.936       |
|    learning_rate        | 4.8e-05     |
|    loss                 | -0.00736    |
|    n_updates            | 7310        |
|    policy_gradient_loss | -0.00484    |
|    std                  | 0.579       |
|    value_loss           | 0.0228      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1499000, episode_reward=2.98 +/- 2.91
Episode length: 213.20 +/- 71.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 213      |
|    mean_reward     | 2.98     |
| time/              |          |
|    total_timesteps | 1499000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 627     |
|    iterations      | 732     |
|    time_elapsed    | 2387    |
|    total_timesteps | 1499136 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1500000, episode_reward=0.92 +/- 2.28
Episode length: 272.00 +/- 56.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 272          |
|    mean_reward          | 0.919        |
| time/                   |              |
|    total_timesteps      | 1500000      |
| train/                  |              |
|    approx_kl            | 0.0057360353 |
|    clip_fraction        | 0.0356       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.75        |
|    explained_variance   | 0.936        |
|    learning_rate        | 4.8e-05      |
|    loss                 | 0.00601      |
|    n_updates            | 7320         |
|    policy_gradient_loss | -0.00569     |
|    std                  | 0.581        |
|    value_loss           | 0.0569       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1501000, episode_reward=1.55 +/- 2.96
Episode length: 254.00 +/- 61.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 254      |
|    mean_reward     | 1.55     |
| time/              |          |
|    total_timesteps | 1501000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 628     |
|    iterations      | 733     |
|    time_elapsed    | 2390    |
|    total_timesteps | 1501184 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1502000, episode_reward=1.42 +/- 3.24
Episode length: 249.20 +/- 62.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 249         |
|    mean_reward          | 1.42        |
| time/                   |             |
|    total_timesteps      | 1502000     |
| train/                  |             |
|    approx_kl            | 0.006968445 |
|    clip_fraction        | 0.0547      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.75       |
|    explained_variance   | 0.976       |
|    learning_rate        | 4.8e-05     |
|    loss                 | -0.0196     |
|    n_updates            | 7330        |
|    policy_gradient_loss | -0.0066     |
|    std                  | 0.582       |
|    value_loss           | 0.0134      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1503000, episode_reward=4.33 +/- 1.85
Episode length: 195.00 +/- 55.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 195      |
|    mean_reward     | 4.33     |
| time/              |          |
|    total_timesteps | 1503000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 628     |
|    iterations      | 734     |
|    time_elapsed    | 2393    |
|    total_timesteps | 1503232 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1504000, episode_reward=1.92 +/- 2.67
Episode length: 251.00 +/- 60.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 251          |
|    mean_reward          | 1.92         |
| time/                   |              |
|    total_timesteps      | 1504000      |
| train/                  |              |
|    approx_kl            | 0.0059789536 |
|    clip_fraction        | 0.0639       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.75        |
|    explained_variance   | 0.948        |
|    learning_rate        | 4.8e-05      |
|    loss                 | -0.0133      |
|    n_updates            | 7340         |
|    policy_gradient_loss | -0.00846     |
|    std                  | 0.582        |
|    value_loss           | 0.0145       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1505000, episode_reward=5.24 +/- 0.33
Episode length: 191.20 +/- 27.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 191      |
|    mean_reward     | 5.24     |
| time/              |          |
|    total_timesteps | 1505000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 628     |
|    iterations      | 735     |
|    time_elapsed    | 2396    |
|    total_timesteps | 1505280 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1506000, episode_reward=1.76 +/- 2.95
Episode length: 244.80 +/- 69.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 245         |
|    mean_reward          | 1.76        |
| time/                   |             |
|    total_timesteps      | 1506000     |
| train/                  |             |
|    approx_kl            | 0.005301889 |
|    clip_fraction        | 0.0368      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.76       |
|    explained_variance   | 0.955       |
|    learning_rate        | 4.81e-05    |
|    loss                 | 0.000918    |
|    n_updates            | 7350        |
|    policy_gradient_loss | -0.00633    |
|    std                  | 0.583       |
|    value_loss           | 0.0128      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=1507000, episode_reward=-0.40 +/- 0.53
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.403   |
| time/              |          |
|    total_timesteps | 1507000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 628     |
|    iterations      | 736     |
|    time_elapsed    | 2399    |
|    total_timesteps | 1507328 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1508000, episode_reward=1.67 +/- 3.02
Episode length: 251.40 +/- 59.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 251          |
|    mean_reward          | 1.67         |
| time/                   |              |
|    total_timesteps      | 1508000      |
| train/                  |              |
|    approx_kl            | 0.0053239986 |
|    clip_fraction        | 0.0336       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.76        |
|    explained_variance   | 0.88         |
|    learning_rate        | 4.81e-05     |
|    loss                 | 0.0216       |
|    n_updates            | 7360         |
|    policy_gradient_loss | -0.00437     |
|    std                  | 0.581        |
|    value_loss           | 0.0561       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1509000, episode_reward=2.73 +/- 3.05
Episode length: 219.40 +/- 65.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 219      |
|    mean_reward     | 2.73     |
| time/              |          |
|    total_timesteps | 1509000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 628     |
|    iterations      | 737     |
|    time_elapsed    | 2402    |
|    total_timesteps | 1509376 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1510000, episode_reward=2.20 +/- 2.63
Episode length: 253.60 +/- 56.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 254          |
|    mean_reward          | 2.2          |
| time/                   |              |
|    total_timesteps      | 1510000      |
| train/                  |              |
|    approx_kl            | 0.0026919385 |
|    clip_fraction        | 0.0116       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.75        |
|    explained_variance   | 0.909        |
|    learning_rate        | 4.81e-05     |
|    loss                 | -0.00725     |
|    n_updates            | 7370         |
|    policy_gradient_loss | -0.00126     |
|    std                  | 0.579        |
|    value_loss           | 0.0263       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=1511000, episode_reward=1.88 +/- 2.80
Episode length: 247.60 +/- 65.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 248      |
|    mean_reward     | 1.88     |
| time/              |          |
|    total_timesteps | 1511000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 628     |
|    iterations      | 738     |
|    time_elapsed    | 2405    |
|    total_timesteps | 1511424 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1512000, episode_reward=0.49 +/- 2.39
Episode length: 273.40 +/- 53.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 273          |
|    mean_reward          | 0.489        |
| time/                   |              |
|    total_timesteps      | 1512000      |
| train/                  |              |
|    approx_kl            | 0.0054128245 |
|    clip_fraction        | 0.025        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.74        |
|    explained_variance   | 0.846        |
|    learning_rate        | 4.81e-05     |
|    loss                 | 0.00122      |
|    n_updates            | 7380         |
|    policy_gradient_loss | -0.00375     |
|    std                  | 0.578        |
|    value_loss           | 0.0651       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1513000, episode_reward=0.56 +/- 2.46
Episode length: 266.40 +/- 67.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 266      |
|    mean_reward     | 0.558    |
| time/              |          |
|    total_timesteps | 1513000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 628     |
|    iterations      | 739     |
|    time_elapsed    | 2408    |
|    total_timesteps | 1513472 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1514000, episode_reward=2.12 +/- 2.55
Episode length: 250.40 +/- 60.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 250          |
|    mean_reward          | 2.12         |
| time/                   |              |
|    total_timesteps      | 1514000      |
| train/                  |              |
|    approx_kl            | 0.0054063406 |
|    clip_fraction        | 0.0389       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.74        |
|    explained_variance   | 0.947        |
|    learning_rate        | 4.82e-05     |
|    loss                 | 0.00977      |
|    n_updates            | 7390         |
|    policy_gradient_loss | -0.0052      |
|    std                  | 0.578        |
|    value_loss           | 0.0218       |
------------------------------------------
box reached target
Eval num_timesteps=1515000, episode_reward=-1.19 +/- 0.19
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1.19    |
| time/              |          |
|    total_timesteps | 1515000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 628     |
|    iterations      | 740     |
|    time_elapsed    | 2411    |
|    total_timesteps | 1515520 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1516000, episode_reward=1.39 +/- 3.10
Episode length: 250.40 +/- 60.87
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 250        |
|    mean_reward          | 1.39       |
| time/                   |            |
|    total_timesteps      | 1516000    |
| train/                  |            |
|    approx_kl            | 0.00450776 |
|    clip_fraction        | 0.0312     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.74      |
|    explained_variance   | 0.952      |
|    learning_rate        | 4.82e-05   |
|    loss                 | 0.00636    |
|    n_updates            | 7400       |
|    policy_gradient_loss | -0.00505   |
|    std                  | 0.575      |
|    value_loss           | 0.0337     |
----------------------------------------
box reached target
box reached target
Eval num_timesteps=1517000, episode_reward=0.44 +/- 2.50
Episode length: 271.80 +/- 56.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 272      |
|    mean_reward     | 0.443    |
| time/              |          |
|    total_timesteps | 1517000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 628     |
|    iterations      | 741     |
|    time_elapsed    | 2414    |
|    total_timesteps | 1517568 |
--------------------------------
box reached target
Eval num_timesteps=1518000, episode_reward=0.60 +/- 2.28
Episode length: 276.80 +/- 46.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 277         |
|    mean_reward          | 0.604       |
| time/                   |             |
|    total_timesteps      | 1518000     |
| train/                  |             |
|    approx_kl            | 0.006468072 |
|    clip_fraction        | 0.0491      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.73       |
|    explained_variance   | 0.945       |
|    learning_rate        | 4.82e-05    |
|    loss                 | 0.018       |
|    n_updates            | 7410        |
|    policy_gradient_loss | -0.00556    |
|    std                  | 0.574       |
|    value_loss           | 0.0335      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1519000, episode_reward=4.22 +/- 2.24
Episode length: 203.80 +/- 55.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 204      |
|    mean_reward     | 4.22     |
| time/              |          |
|    total_timesteps | 1519000  |
---------------------------------
box reached target
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 628     |
|    iterations      | 742     |
|    time_elapsed    | 2417    |
|    total_timesteps | 1519616 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1520000, episode_reward=1.46 +/- 3.21
Episode length: 263.20 +/- 45.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 263         |
|    mean_reward          | 1.46        |
| time/                   |             |
|    total_timesteps      | 1520000     |
| train/                  |             |
|    approx_kl            | 0.007143018 |
|    clip_fraction        | 0.0457      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.73       |
|    explained_variance   | 0.861       |
|    learning_rate        | 4.82e-05    |
|    loss                 | 0.0189      |
|    n_updates            | 7420        |
|    policy_gradient_loss | -0.00579    |
|    std                  | 0.573       |
|    value_loss           | 0.0301      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1521000, episode_reward=1.53 +/- 3.00
Episode length: 247.40 +/- 67.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 247      |
|    mean_reward     | 1.53     |
| time/              |          |
|    total_timesteps | 1521000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 628     |
|    iterations      | 743     |
|    time_elapsed    | 2420    |
|    total_timesteps | 1521664 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1522000, episode_reward=2.82 +/- 3.05
Episode length: 226.40 +/- 63.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 226         |
|    mean_reward          | 2.82        |
| time/                   |             |
|    total_timesteps      | 1522000     |
| train/                  |             |
|    approx_kl            | 0.004524245 |
|    clip_fraction        | 0.0247      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.72       |
|    explained_variance   | 0.967       |
|    learning_rate        | 4.83e-05    |
|    loss                 | 0.0141      |
|    n_updates            | 7430        |
|    policy_gradient_loss | -0.00402    |
|    std                  | 0.571       |
|    value_loss           | 0.0166      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1523000, episode_reward=1.52 +/- 3.21
Episode length: 267.00 +/- 41.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 267      |
|    mean_reward     | 1.52     |
| time/              |          |
|    total_timesteps | 1523000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 628     |
|    iterations      | 744     |
|    time_elapsed    | 2423    |
|    total_timesteps | 1523712 |
--------------------------------
box reached target
Eval num_timesteps=1524000, episode_reward=-0.36 +/- 0.75
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.365       |
| time/                   |              |
|    total_timesteps      | 1524000      |
| train/                  |              |
|    approx_kl            | 0.0066809217 |
|    clip_fraction        | 0.0333       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.72        |
|    explained_variance   | 0.943        |
|    learning_rate        | 4.83e-05     |
|    loss                 | 0.0268       |
|    n_updates            | 7440         |
|    policy_gradient_loss | -0.00394     |
|    std                  | 0.571        |
|    value_loss           | 0.0146       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1525000, episode_reward=4.04 +/- 2.39
Episode length: 205.20 +/- 51.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 205      |
|    mean_reward     | 4.04     |
| time/              |          |
|    total_timesteps | 1525000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 628     |
|    iterations      | 745     |
|    time_elapsed    | 2426    |
|    total_timesteps | 1525760 |
--------------------------------
box reached target
Eval num_timesteps=1526000, episode_reward=1.12 +/- 2.11
Episode length: 276.60 +/- 46.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 277          |
|    mean_reward          | 1.12         |
| time/                   |              |
|    total_timesteps      | 1526000      |
| train/                  |              |
|    approx_kl            | 0.0033682338 |
|    clip_fraction        | 0.0205       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.72        |
|    explained_variance   | 0.924        |
|    learning_rate        | 4.83e-05     |
|    loss                 | 0.0132       |
|    n_updates            | 7450         |
|    policy_gradient_loss | -0.00545     |
|    std                  | 0.571        |
|    value_loss           | 0.0272       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1527000, episode_reward=1.73 +/- 2.83
Episode length: 248.60 +/- 63.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 249      |
|    mean_reward     | 1.73     |
| time/              |          |
|    total_timesteps | 1527000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 628     |
|    iterations      | 746     |
|    time_elapsed    | 2429    |
|    total_timesteps | 1527808 |
--------------------------------
box reached target
Eval num_timesteps=1528000, episode_reward=0.32 +/- 2.54
Episode length: 278.60 +/- 42.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 279          |
|    mean_reward          | 0.32         |
| time/                   |              |
|    total_timesteps      | 1528000      |
| train/                  |              |
|    approx_kl            | 0.0044047954 |
|    clip_fraction        | 0.0469       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.71        |
|    explained_variance   | 0.95         |
|    learning_rate        | 4.83e-05     |
|    loss                 | 0.0747       |
|    n_updates            | 7460         |
|    policy_gradient_loss | -0.00461     |
|    std                  | 0.569        |
|    value_loss           | 0.029        |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1529000, episode_reward=2.86 +/- 2.90
Episode length: 224.20 +/- 63.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 224      |
|    mean_reward     | 2.86     |
| time/              |          |
|    total_timesteps | 1529000  |
---------------------------------
box reached target
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 628     |
|    iterations      | 747     |
|    time_elapsed    | 2432    |
|    total_timesteps | 1529856 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1530000, episode_reward=4.34 +/- 1.94
Episode length: 205.80 +/- 60.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | 4.34         |
| time/                   |              |
|    total_timesteps      | 1530000      |
| train/                  |              |
|    approx_kl            | 0.0043284236 |
|    clip_fraction        | 0.0393       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.71        |
|    explained_variance   | 0.831        |
|    learning_rate        | 4.84e-05     |
|    loss                 | 0.00333      |
|    n_updates            | 7470         |
|    policy_gradient_loss | -0.00658     |
|    std                  | 0.569        |
|    value_loss           | 0.0428       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1531000, episode_reward=3.17 +/- 2.93
Episode length: 240.00 +/- 50.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 240      |
|    mean_reward     | 3.17     |
| time/              |          |
|    total_timesteps | 1531000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 628     |
|    iterations      | 748     |
|    time_elapsed    | 2435    |
|    total_timesteps | 1531904 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1532000, episode_reward=2.92 +/- 2.94
Episode length: 239.40 +/- 56.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 239          |
|    mean_reward          | 2.92         |
| time/                   |              |
|    total_timesteps      | 1532000      |
| train/                  |              |
|    approx_kl            | 0.0053502815 |
|    clip_fraction        | 0.0517       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.71        |
|    explained_variance   | 0.961        |
|    learning_rate        | 4.84e-05     |
|    loss                 | -0.00386     |
|    n_updates            | 7480         |
|    policy_gradient_loss | -0.00525     |
|    std                  | 0.57         |
|    value_loss           | 0.0211       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1533000, episode_reward=2.99 +/- 2.80
Episode length: 221.80 +/- 64.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 222      |
|    mean_reward     | 2.99     |
| time/              |          |
|    total_timesteps | 1533000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 628     |
|    iterations      | 749     |
|    time_elapsed    | 2438    |
|    total_timesteps | 1533952 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1534000, episode_reward=1.95 +/- 2.66
Episode length: 259.20 +/- 56.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 259          |
|    mean_reward          | 1.95         |
| time/                   |              |
|    total_timesteps      | 1534000      |
| train/                  |              |
|    approx_kl            | 0.0044829743 |
|    clip_fraction        | 0.0277       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.71        |
|    explained_variance   | 0.822        |
|    learning_rate        | 4.84e-05     |
|    loss                 | 0.0037       |
|    n_updates            | 7490         |
|    policy_gradient_loss | -0.00406     |
|    std                  | 0.571        |
|    value_loss           | 0.108        |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1535000, episode_reward=1.43 +/- 3.14
Episode length: 241.80 +/- 71.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 242      |
|    mean_reward     | 1.43     |
| time/              |          |
|    total_timesteps | 1535000  |
---------------------------------
box reached target
Eval num_timesteps=1536000, episode_reward=1.07 +/- 2.12
Episode length: 267.80 +/- 64.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 268      |
|    mean_reward     | 1.07     |
| time/              |          |
|    total_timesteps | 1536000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 628     |
|    iterations      | 750     |
|    time_elapsed    | 2442    |
|    total_timesteps | 1536000 |
--------------------------------
Eval num_timesteps=1537000, episode_reward=-0.77 +/- 0.77
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.77        |
| time/                   |              |
|    total_timesteps      | 1537000      |
| train/                  |              |
|    approx_kl            | 0.0062980037 |
|    clip_fraction        | 0.0483       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.72        |
|    explained_variance   | 0.945        |
|    learning_rate        | 4.84e-05     |
|    loss                 | -0.0355      |
|    n_updates            | 7500         |
|    policy_gradient_loss | -0.00781     |
|    std                  | 0.57         |
|    value_loss           | 0.0156       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1538000, episode_reward=0.83 +/- 2.46
Episode length: 282.40 +/- 35.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 282      |
|    mean_reward     | 0.826    |
| time/              |          |
|    total_timesteps | 1538000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 628     |
|    iterations      | 751     |
|    time_elapsed    | 2445    |
|    total_timesteps | 1538048 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1539000, episode_reward=2.61 +/- 3.19
Episode length: 215.00 +/- 69.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 215          |
|    mean_reward          | 2.61         |
| time/                   |              |
|    total_timesteps      | 1539000      |
| train/                  |              |
|    approx_kl            | 0.0042199334 |
|    clip_fraction        | 0.0273       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.71        |
|    explained_variance   | 0.937        |
|    learning_rate        | 4.85e-05     |
|    loss                 | 0.013        |
|    n_updates            | 7510         |
|    policy_gradient_loss | -0.00348     |
|    std                  | 0.57         |
|    value_loss           | 0.0392       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1540000, episode_reward=3.92 +/- 2.46
Episode length: 185.20 +/- 57.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 185      |
|    mean_reward     | 3.92     |
| time/              |          |
|    total_timesteps | 1540000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 629     |
|    iterations      | 752     |
|    time_elapsed    | 2448    |
|    total_timesteps | 1540096 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1541000, episode_reward=2.78 +/- 2.95
Episode length: 207.80 +/- 75.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 208         |
|    mean_reward          | 2.78        |
| time/                   |             |
|    total_timesteps      | 1541000     |
| train/                  |             |
|    approx_kl            | 0.002869734 |
|    clip_fraction        | 0.0196      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | 0.846       |
|    learning_rate        | 4.85e-05    |
|    loss                 | -0.0107     |
|    n_updates            | 7520        |
|    policy_gradient_loss | -0.00406    |
|    std                  | 0.569       |
|    value_loss           | 0.0406      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=1542000, episode_reward=0.88 +/- 2.39
Episode length: 273.00 +/- 54.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 273      |
|    mean_reward     | 0.876    |
| time/              |          |
|    total_timesteps | 1542000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 629     |
|    iterations      | 753     |
|    time_elapsed    | 2451    |
|    total_timesteps | 1542144 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1543000, episode_reward=2.07 +/- 2.75
Episode length: 252.20 +/- 58.65
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 252        |
|    mean_reward          | 2.07       |
| time/                   |            |
|    total_timesteps      | 1543000    |
| train/                  |            |
|    approx_kl            | 0.00463864 |
|    clip_fraction        | 0.0329     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.71      |
|    explained_variance   | 0.95       |
|    learning_rate        | 4.85e-05   |
|    loss                 | 0.00655    |
|    n_updates            | 7530       |
|    policy_gradient_loss | -0.00586   |
|    std                  | 0.568      |
|    value_loss           | 0.0265     |
----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1544000, episode_reward=4.05 +/- 2.76
Episode length: 193.80 +/- 59.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 194      |
|    mean_reward     | 4.05     |
| time/              |          |
|    total_timesteps | 1544000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 629     |
|    iterations      | 754     |
|    time_elapsed    | 2454    |
|    total_timesteps | 1544192 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1545000, episode_reward=-0.65 +/- 0.75
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.65       |
| time/                   |             |
|    total_timesteps      | 1545000     |
| train/                  |             |
|    approx_kl            | 0.005491224 |
|    clip_fraction        | 0.0489      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | 0.932       |
|    learning_rate        | 4.85e-05    |
|    loss                 | 0.0164      |
|    n_updates            | 7540        |
|    policy_gradient_loss | -0.00679    |
|    std                  | 0.568       |
|    value_loss           | 0.0386      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=1546000, episode_reward=0.87 +/- 2.36
Episode length: 288.40 +/- 23.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 288      |
|    mean_reward     | 0.872    |
| time/              |          |
|    total_timesteps | 1546000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 629     |
|    iterations      | 755     |
|    time_elapsed    | 2457    |
|    total_timesteps | 1546240 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1547000, episode_reward=1.53 +/- 3.04
Episode length: 239.40 +/- 74.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 239          |
|    mean_reward          | 1.53         |
| time/                   |              |
|    total_timesteps      | 1547000      |
| train/                  |              |
|    approx_kl            | 0.0043287505 |
|    clip_fraction        | 0.0441       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.71        |
|    explained_variance   | 0.955        |
|    learning_rate        | 4.86e-05     |
|    loss                 | -0.0103      |
|    n_updates            | 7550         |
|    policy_gradient_loss | -0.00682     |
|    std                  | 0.57         |
|    value_loss           | 0.0199       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1548000, episode_reward=0.73 +/- 2.32
Episode length: 277.60 +/- 44.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | 0.726    |
| time/              |          |
|    total_timesteps | 1548000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 629     |
|    iterations      | 756     |
|    time_elapsed    | 2460    |
|    total_timesteps | 1548288 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1549000, episode_reward=2.99 +/- 2.81
Episode length: 227.80 +/- 61.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 228         |
|    mean_reward          | 2.99        |
| time/                   |             |
|    total_timesteps      | 1549000     |
| train/                  |             |
|    approx_kl            | 0.003911956 |
|    clip_fraction        | 0.0294      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | 0.962       |
|    learning_rate        | 4.86e-05    |
|    loss                 | -0.00588    |
|    n_updates            | 7560        |
|    policy_gradient_loss | -0.00527    |
|    std                  | 0.571       |
|    value_loss           | 0.0131      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=1550000, episode_reward=0.46 +/- 2.43
Episode length: 273.20 +/- 53.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 273      |
|    mean_reward     | 0.46     |
| time/              |          |
|    total_timesteps | 1550000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 629     |
|    iterations      | 757     |
|    time_elapsed    | 2463    |
|    total_timesteps | 1550336 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1551000, episode_reward=2.00 +/- 2.57
Episode length: 250.00 +/- 61.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 250          |
|    mean_reward          | 2            |
| time/                   |              |
|    total_timesteps      | 1551000      |
| train/                  |              |
|    approx_kl            | 0.0049907444 |
|    clip_fraction        | 0.0268       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.72        |
|    explained_variance   | 0.931        |
|    learning_rate        | 4.86e-05     |
|    loss                 | 0.00626      |
|    n_updates            | 7570         |
|    policy_gradient_loss | -0.00451     |
|    std                  | 0.572        |
|    value_loss           | 0.0186       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1552000, episode_reward=3.45 +/- 2.35
Episode length: 233.80 +/- 59.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 234      |
|    mean_reward     | 3.45     |
| time/              |          |
|    total_timesteps | 1552000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 629     |
|    iterations      | 758     |
|    time_elapsed    | 2466    |
|    total_timesteps | 1552384 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1553000, episode_reward=3.04 +/- 2.86
Episode length: 244.40 +/- 60.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 244          |
|    mean_reward          | 3.04         |
| time/                   |              |
|    total_timesteps      | 1553000      |
| train/                  |              |
|    approx_kl            | 0.0057708016 |
|    clip_fraction        | 0.0392       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.72        |
|    explained_variance   | 0.899        |
|    learning_rate        | 4.86e-05     |
|    loss                 | -0.00207     |
|    n_updates            | 7580         |
|    policy_gradient_loss | -0.00494     |
|    std                  | 0.574        |
|    value_loss           | 0.00869      |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1554000, episode_reward=3.13 +/- 2.55
Episode length: 208.20 +/- 75.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 208      |
|    mean_reward     | 3.13     |
| time/              |          |
|    total_timesteps | 1554000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 629     |
|    iterations      | 759     |
|    time_elapsed    | 2469    |
|    total_timesteps | 1554432 |
--------------------------------
box reached target
Eval num_timesteps=1555000, episode_reward=0.49 +/- 2.56
Episode length: 280.80 +/- 38.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 281         |
|    mean_reward          | 0.489       |
| time/                   |             |
|    total_timesteps      | 1555000     |
| train/                  |             |
|    approx_kl            | 0.004357799 |
|    clip_fraction        | 0.026       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.72       |
|    explained_variance   | 0.978       |
|    learning_rate        | 4.87e-05    |
|    loss                 | 0.0155      |
|    n_updates            | 7590        |
|    policy_gradient_loss | -0.00578    |
|    std                  | 0.572       |
|    value_loss           | 0.0108      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1556000, episode_reward=4.37 +/- 1.97
Episode length: 197.60 +/- 61.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 198      |
|    mean_reward     | 4.37     |
| time/              |          |
|    total_timesteps | 1556000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 629     |
|    iterations      | 760     |
|    time_elapsed    | 2472    |
|    total_timesteps | 1556480 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1557000, episode_reward=3.09 +/- 2.72
Episode length: 220.40 +/- 65.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 220          |
|    mean_reward          | 3.09         |
| time/                   |              |
|    total_timesteps      | 1557000      |
| train/                  |              |
|    approx_kl            | 0.0032498566 |
|    clip_fraction        | 0.0155       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.72        |
|    explained_variance   | 0.961        |
|    learning_rate        | 4.87e-05     |
|    loss                 | 0.0332       |
|    n_updates            | 7600         |
|    policy_gradient_loss | -0.00341     |
|    std                  | 0.571        |
|    value_loss           | 0.0295       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1558000, episode_reward=2.75 +/- 3.06
Episode length: 231.40 +/- 58.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 231      |
|    mean_reward     | 2.75     |
| time/              |          |
|    total_timesteps | 1558000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 629     |
|    iterations      | 761     |
|    time_elapsed    | 2475    |
|    total_timesteps | 1558528 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1559000, episode_reward=4.00 +/- 2.44
Episode length: 194.40 +/- 55.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 194         |
|    mean_reward          | 4           |
| time/                   |             |
|    total_timesteps      | 1559000     |
| train/                  |             |
|    approx_kl            | 0.004594057 |
|    clip_fraction        | 0.0397      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.72       |
|    explained_variance   | 0.675       |
|    learning_rate        | 4.87e-05    |
|    loss                 | 0.00169     |
|    n_updates            | 7610        |
|    policy_gradient_loss | -0.00469    |
|    std                  | 0.571       |
|    value_loss           | 0.0999      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=1560000, episode_reward=-0.63 +/- 0.67
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.627   |
| time/              |          |
|    total_timesteps | 1560000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 629     |
|    iterations      | 762     |
|    time_elapsed    | 2478    |
|    total_timesteps | 1560576 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1561000, episode_reward=3.98 +/- 2.50
Episode length: 200.80 +/- 50.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 201          |
|    mean_reward          | 3.98         |
| time/                   |              |
|    total_timesteps      | 1561000      |
| train/                  |              |
|    approx_kl            | 0.0040257154 |
|    clip_fraction        | 0.0235       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.72        |
|    explained_variance   | 0.891        |
|    learning_rate        | 4.87e-05     |
|    loss                 | -0.00178     |
|    n_updates            | 7620         |
|    policy_gradient_loss | -0.00337     |
|    std                  | 0.571        |
|    value_loss           | 0.0551       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1562000, episode_reward=0.18 +/- 2.55
Episode length: 273.00 +/- 54.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 273      |
|    mean_reward     | 0.182    |
| time/              |          |
|    total_timesteps | 1562000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 629     |
|    iterations      | 763     |
|    time_elapsed    | 2481    |
|    total_timesteps | 1562624 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1563000, episode_reward=1.55 +/- 3.04
Episode length: 248.20 +/- 63.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 248         |
|    mean_reward          | 1.55        |
| time/                   |             |
|    total_timesteps      | 1563000     |
| train/                  |             |
|    approx_kl            | 0.004932651 |
|    clip_fraction        | 0.0208      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | 0.921       |
|    learning_rate        | 4.88e-05    |
|    loss                 | 0.0198      |
|    n_updates            | 7630        |
|    policy_gradient_loss | -0.00293    |
|    std                  | 0.569       |
|    value_loss           | 0.0355      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1564000, episode_reward=3.98 +/- 2.57
Episode length: 194.40 +/- 57.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 194      |
|    mean_reward     | 3.98     |
| time/              |          |
|    total_timesteps | 1564000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 629     |
|    iterations      | 764     |
|    time_elapsed    | 2484    |
|    total_timesteps | 1564672 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1565000, episode_reward=2.78 +/- 3.10
Episode length: 220.80 +/- 65.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 221         |
|    mean_reward          | 2.78        |
| time/                   |             |
|    total_timesteps      | 1565000     |
| train/                  |             |
|    approx_kl            | 0.006661935 |
|    clip_fraction        | 0.0456      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | 0.937       |
|    learning_rate        | 4.88e-05    |
|    loss                 | 0.00438     |
|    n_updates            | 7640        |
|    policy_gradient_loss | -0.00556    |
|    std                  | 0.568       |
|    value_loss           | 0.0246      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=1566000, episode_reward=1.78 +/- 2.84
Episode length: 246.80 +/- 65.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 247      |
|    mean_reward     | 1.78     |
| time/              |          |
|    total_timesteps | 1566000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 629     |
|    iterations      | 765     |
|    time_elapsed    | 2487    |
|    total_timesteps | 1566720 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1567000, episode_reward=2.24 +/- 2.48
Episode length: 247.20 +/- 64.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 247          |
|    mean_reward          | 2.24         |
| time/                   |              |
|    total_timesteps      | 1567000      |
| train/                  |              |
|    approx_kl            | 0.0047105392 |
|    clip_fraction        | 0.0482       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.71        |
|    explained_variance   | 0.837        |
|    learning_rate        | 4.88e-05     |
|    loss                 | -0.0126      |
|    n_updates            | 7650         |
|    policy_gradient_loss | -0.00658     |
|    std                  | 0.569        |
|    value_loss           | 0.0181       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1568000, episode_reward=3.27 +/- 2.87
Episode length: 250.20 +/- 43.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 250      |
|    mean_reward     | 3.27     |
| time/              |          |
|    total_timesteps | 1568000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 629     |
|    iterations      | 766     |
|    time_elapsed    | 2490    |
|    total_timesteps | 1568768 |
--------------------------------
box reached target
Eval num_timesteps=1569000, episode_reward=0.56 +/- 2.46
Episode length: 273.00 +/- 54.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 273         |
|    mean_reward          | 0.561       |
| time/                   |             |
|    total_timesteps      | 1569000     |
| train/                  |             |
|    approx_kl            | 0.005284979 |
|    clip_fraction        | 0.0426      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | 0.915       |
|    learning_rate        | 4.88e-05    |
|    loss                 | -0.0131     |
|    n_updates            | 7660        |
|    policy_gradient_loss | -0.00412    |
|    std                  | 0.569       |
|    value_loss           | 0.0327      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1570000, episode_reward=3.31 +/- 2.32
Episode length: 230.60 +/- 58.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 231      |
|    mean_reward     | 3.31     |
| time/              |          |
|    total_timesteps | 1570000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 630     |
|    iterations      | 767     |
|    time_elapsed    | 2493    |
|    total_timesteps | 1570816 |
--------------------------------
box reached target
Eval num_timesteps=1571000, episode_reward=0.85 +/- 2.42
Episode length: 273.80 +/- 52.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 274          |
|    mean_reward          | 0.846        |
| time/                   |              |
|    total_timesteps      | 1571000      |
| train/                  |              |
|    approx_kl            | 0.0048366114 |
|    clip_fraction        | 0.036        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.71        |
|    explained_variance   | 0.918        |
|    learning_rate        | 4.88e-05     |
|    loss                 | 0.0285       |
|    n_updates            | 7670         |
|    policy_gradient_loss | -0.00445     |
|    std                  | 0.568        |
|    value_loss           | 0.0329       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1572000, episode_reward=1.57 +/- 2.93
Episode length: 251.00 +/- 63.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | 1.57     |
| time/              |          |
|    total_timesteps | 1572000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 630     |
|    iterations      | 768     |
|    time_elapsed    | 2496    |
|    total_timesteps | 1572864 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1573000, episode_reward=0.18 +/- 2.50
Episode length: 276.00 +/- 48.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 276          |
|    mean_reward          | 0.184        |
| time/                   |              |
|    total_timesteps      | 1573000      |
| train/                  |              |
|    approx_kl            | 0.0048325206 |
|    clip_fraction        | 0.0434       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.7         |
|    explained_variance   | 0.935        |
|    learning_rate        | 4.89e-05     |
|    loss                 | -0.0113      |
|    n_updates            | 7680         |
|    policy_gradient_loss | -0.00639     |
|    std                  | 0.567        |
|    value_loss           | 0.0181       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1574000, episode_reward=4.07 +/- 2.54
Episode length: 201.40 +/- 53.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 201      |
|    mean_reward     | 4.07     |
| time/              |          |
|    total_timesteps | 1574000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 630     |
|    iterations      | 769     |
|    time_elapsed    | 2499    |
|    total_timesteps | 1574912 |
--------------------------------
Eval num_timesteps=1575000, episode_reward=-0.62 +/- 0.63
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.62        |
| time/                   |              |
|    total_timesteps      | 1575000      |
| train/                  |              |
|    approx_kl            | 0.0074779945 |
|    clip_fraction        | 0.0843       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.7         |
|    explained_variance   | 0.895        |
|    learning_rate        | 4.89e-05     |
|    loss                 | -0.0164      |
|    n_updates            | 7690         |
|    policy_gradient_loss | -0.00942     |
|    std                  | 0.567        |
|    value_loss           | 0.0418       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1576000, episode_reward=2.17 +/- 2.66
Episode length: 261.80 +/- 47.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 262      |
|    mean_reward     | 2.17     |
| time/              |          |
|    total_timesteps | 1576000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 630     |
|    iterations      | 770     |
|    time_elapsed    | 2502    |
|    total_timesteps | 1576960 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1577000, episode_reward=1.79 +/- 2.85
Episode length: 250.80 +/- 60.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 251         |
|    mean_reward          | 1.79        |
| time/                   |             |
|    total_timesteps      | 1577000     |
| train/                  |             |
|    approx_kl            | 0.004213581 |
|    clip_fraction        | 0.0259      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | 0.95        |
|    learning_rate        | 4.89e-05    |
|    loss                 | -0.0082     |
|    n_updates            | 7700        |
|    policy_gradient_loss | -0.00456    |
|    std                  | 0.568       |
|    value_loss           | 0.0197      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1578000, episode_reward=3.94 +/- 2.55
Episode length: 193.40 +/- 54.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 193      |
|    mean_reward     | 3.94     |
| time/              |          |
|    total_timesteps | 1578000  |
---------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1579000, episode_reward=4.21 +/- 2.02
Episode length: 180.60 +/- 60.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 181      |
|    mean_reward     | 4.21     |
| time/              |          |
|    total_timesteps | 1579000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 630     |
|    iterations      | 771     |
|    time_elapsed    | 2505    |
|    total_timesteps | 1579008 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1580000, episode_reward=5.36 +/- 0.09
Episode length: 168.40 +/- 13.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 168         |
|    mean_reward          | 5.36        |
| time/                   |             |
|    total_timesteps      | 1580000     |
| train/                  |             |
|    approx_kl            | 0.004289115 |
|    clip_fraction        | 0.0287      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | 0.954       |
|    learning_rate        | 4.89e-05    |
|    loss                 | -0.0292     |
|    n_updates            | 7710        |
|    policy_gradient_loss | -0.00665    |
|    std                  | 0.566       |
|    value_loss           | 0.0162      |
-----------------------------------------
box reached target
Eval num_timesteps=1581000, episode_reward=0.63 +/- 2.51
Episode length: 274.20 +/- 51.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 274      |
|    mean_reward     | 0.632    |
| time/              |          |
|    total_timesteps | 1581000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 630     |
|    iterations      | 772     |
|    time_elapsed    | 2508    |
|    total_timesteps | 1581056 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1582000, episode_reward=4.23 +/- 1.98
Episode length: 194.40 +/- 54.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 194          |
|    mean_reward          | 4.23         |
| time/                   |              |
|    total_timesteps      | 1582000      |
| train/                  |              |
|    approx_kl            | 0.0042187725 |
|    clip_fraction        | 0.0292       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.7         |
|    explained_variance   | 0.87         |
|    learning_rate        | 4.9e-05      |
|    loss                 | 0.00162      |
|    n_updates            | 7720         |
|    policy_gradient_loss | -0.00547     |
|    std                  | 0.565        |
|    value_loss           | 0.0385       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1583000, episode_reward=2.92 +/- 2.80
Episode length: 226.20 +/- 60.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 226      |
|    mean_reward     | 2.92     |
| time/              |          |
|    total_timesteps | 1583000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 630     |
|    iterations      | 773     |
|    time_elapsed    | 2511    |
|    total_timesteps | 1583104 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1584000, episode_reward=4.11 +/- 2.56
Episode length: 212.60 +/- 56.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 213          |
|    mean_reward          | 4.11         |
| time/                   |              |
|    total_timesteps      | 1584000      |
| train/                  |              |
|    approx_kl            | 0.0047356654 |
|    clip_fraction        | 0.0417       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.7         |
|    explained_variance   | 0.818        |
|    learning_rate        | 4.9e-05      |
|    loss                 | -0.0129      |
|    n_updates            | 7730         |
|    policy_gradient_loss | -0.00812     |
|    std                  | 0.567        |
|    value_loss           | 0.0419       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1585000, episode_reward=2.44 +/- 2.35
Episode length: 245.20 +/- 67.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 245      |
|    mean_reward     | 2.44     |
| time/              |          |
|    total_timesteps | 1585000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 630     |
|    iterations      | 774     |
|    time_elapsed    | 2514    |
|    total_timesteps | 1585152 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1586000, episode_reward=5.26 +/- 0.21
Episode length: 173.80 +/- 18.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 174          |
|    mean_reward          | 5.26         |
| time/                   |              |
|    total_timesteps      | 1586000      |
| train/                  |              |
|    approx_kl            | 0.0064782016 |
|    clip_fraction        | 0.0536       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.7         |
|    explained_variance   | 0.911        |
|    learning_rate        | 4.9e-05      |
|    loss                 | -0.0113      |
|    n_updates            | 7740         |
|    policy_gradient_loss | -0.00661     |
|    std                  | 0.568        |
|    value_loss           | 0.0174       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1587000, episode_reward=0.95 +/- 2.27
Episode length: 278.60 +/- 42.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 279      |
|    mean_reward     | 0.946    |
| time/              |          |
|    total_timesteps | 1587000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 630     |
|    iterations      | 775     |
|    time_elapsed    | 2517    |
|    total_timesteps | 1587200 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1588000, episode_reward=1.86 +/- 2.86
Episode length: 243.40 +/- 69.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 243         |
|    mean_reward          | 1.86        |
| time/                   |             |
|    total_timesteps      | 1588000     |
| train/                  |             |
|    approx_kl            | 0.005612949 |
|    clip_fraction        | 0.0392      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | 0.919       |
|    learning_rate        | 4.9e-05     |
|    loss                 | 0.00302     |
|    n_updates            | 7750        |
|    policy_gradient_loss | -0.00723    |
|    std                  | 0.566       |
|    value_loss           | 0.0702      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1589000, episode_reward=1.65 +/- 3.09
Episode length: 249.40 +/- 63.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 249      |
|    mean_reward     | 1.65     |
| time/              |          |
|    total_timesteps | 1589000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 630     |
|    iterations      | 776     |
|    time_elapsed    | 2520    |
|    total_timesteps | 1589248 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1590000, episode_reward=3.07 +/- 2.92
Episode length: 243.80 +/- 61.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 244          |
|    mean_reward          | 3.07         |
| time/                   |              |
|    total_timesteps      | 1590000      |
| train/                  |              |
|    approx_kl            | 0.0049746074 |
|    clip_fraction        | 0.0345       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.69        |
|    explained_variance   | 0.91         |
|    learning_rate        | 4.91e-05     |
|    loss                 | 0.0415       |
|    n_updates            | 7760         |
|    policy_gradient_loss | -0.00555     |
|    std                  | 0.563        |
|    value_loss           | 0.0503       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1591000, episode_reward=4.10 +/- 2.38
Episode length: 187.60 +/- 58.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 188      |
|    mean_reward     | 4.1      |
| time/              |          |
|    total_timesteps | 1591000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 630     |
|    iterations      | 777     |
|    time_elapsed    | 2523    |
|    total_timesteps | 1591296 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1592000, episode_reward=3.07 +/- 2.79
Episode length: 238.20 +/- 52.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 238         |
|    mean_reward          | 3.07        |
| time/                   |             |
|    total_timesteps      | 1592000     |
| train/                  |             |
|    approx_kl            | 0.004800603 |
|    clip_fraction        | 0.0264      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.69       |
|    explained_variance   | 0.925       |
|    learning_rate        | 4.91e-05    |
|    loss                 | -0.00696    |
|    n_updates            | 7770        |
|    policy_gradient_loss | -0.00409    |
|    std                  | 0.562       |
|    value_loss           | 0.0349      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1593000, episode_reward=0.55 +/- 2.40
Episode length: 269.80 +/- 60.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 270      |
|    mean_reward     | 0.551    |
| time/              |          |
|    total_timesteps | 1593000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 630     |
|    iterations      | 778     |
|    time_elapsed    | 2526    |
|    total_timesteps | 1593344 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1594000, episode_reward=1.92 +/- 2.63
Episode length: 247.60 +/- 64.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 248         |
|    mean_reward          | 1.92        |
| time/                   |             |
|    total_timesteps      | 1594000     |
| train/                  |             |
|    approx_kl            | 0.005359113 |
|    clip_fraction        | 0.0465      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.68       |
|    explained_variance   | 0.922       |
|    learning_rate        | 4.91e-05    |
|    loss                 | 0.0543      |
|    n_updates            | 7780        |
|    policy_gradient_loss | -0.00375    |
|    std                  | 0.561       |
|    value_loss           | 0.0493      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1595000, episode_reward=0.48 +/- 2.49
Episode length: 273.40 +/- 53.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 273      |
|    mean_reward     | 0.482    |
| time/              |          |
|    total_timesteps | 1595000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 630     |
|    iterations      | 779     |
|    time_elapsed    | 2529    |
|    total_timesteps | 1595392 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1596000, episode_reward=1.90 +/- 2.75
Episode length: 245.60 +/- 66.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 246         |
|    mean_reward          | 1.9         |
| time/                   |             |
|    total_timesteps      | 1596000     |
| train/                  |             |
|    approx_kl            | 0.004235653 |
|    clip_fraction        | 0.031       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.68       |
|    explained_variance   | 0.964       |
|    learning_rate        | 4.91e-05    |
|    loss                 | 0.0228      |
|    n_updates            | 7790        |
|    policy_gradient_loss | -0.00352    |
|    std                  | 0.561       |
|    value_loss           | 0.0277      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1597000, episode_reward=1.71 +/- 3.01
Episode length: 249.00 +/- 63.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 249      |
|    mean_reward     | 1.71     |
| time/              |          |
|    total_timesteps | 1597000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 630     |
|    iterations      | 780     |
|    time_elapsed    | 2532    |
|    total_timesteps | 1597440 |
--------------------------------
box reached target
Eval num_timesteps=1598000, episode_reward=-0.19 +/- 0.44
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.189       |
| time/                   |              |
|    total_timesteps      | 1598000      |
| train/                  |              |
|    approx_kl            | 0.0040177736 |
|    clip_fraction        | 0.0236       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.68        |
|    explained_variance   | 0.867        |
|    learning_rate        | 4.92e-05     |
|    loss                 | 0.00992      |
|    n_updates            | 7800         |
|    policy_gradient_loss | -0.00317     |
|    std                  | 0.561        |
|    value_loss           | 0.0544       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1599000, episode_reward=2.76 +/- 2.99
Episode length: 224.20 +/- 65.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 224      |
|    mean_reward     | 2.76     |
| time/              |          |
|    total_timesteps | 1599000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 630     |
|    iterations      | 781     |
|    time_elapsed    | 2535    |
|    total_timesteps | 1599488 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1600000, episode_reward=2.76 +/- 3.07
Episode length: 222.60 +/- 65.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 223          |
|    mean_reward          | 2.76         |
| time/                   |              |
|    total_timesteps      | 1600000      |
| train/                  |              |
|    approx_kl            | 0.0048802514 |
|    clip_fraction        | 0.0355       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.68        |
|    explained_variance   | 0.901        |
|    learning_rate        | 4.92e-05     |
|    loss                 | -0.00997     |
|    n_updates            | 7810         |
|    policy_gradient_loss | -0.0044      |
|    std                  | 0.561        |
|    value_loss           | 0.0475       |
------------------------------------------
Eval num_timesteps=1601000, episode_reward=-0.48 +/- 0.65
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.476   |
| time/              |          |
|    total_timesteps | 1601000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 630     |
|    iterations      | 782     |
|    time_elapsed    | 2538    |
|    total_timesteps | 1601536 |
--------------------------------
box reached target
Eval num_timesteps=1602000, episode_reward=0.68 +/- 2.33
Episode length: 269.20 +/- 61.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 269         |
|    mean_reward          | 0.676       |
| time/                   |             |
|    total_timesteps      | 1602000     |
| train/                  |             |
|    approx_kl            | 0.006621847 |
|    clip_fraction        | 0.0719      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.68       |
|    explained_variance   | 0.914       |
|    learning_rate        | 4.92e-05    |
|    loss                 | 0.0189      |
|    n_updates            | 7820        |
|    policy_gradient_loss | -0.00797    |
|    std                  | 0.563       |
|    value_loss           | 0.0323      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1603000, episode_reward=3.23 +/- 2.56
Episode length: 224.40 +/- 62.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 224      |
|    mean_reward     | 3.23     |
| time/              |          |
|    total_timesteps | 1603000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 630     |
|    iterations      | 783     |
|    time_elapsed    | 2541    |
|    total_timesteps | 1603584 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1604000, episode_reward=3.09 +/- 2.71
Episode length: 234.40 +/- 55.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 234          |
|    mean_reward          | 3.09         |
| time/                   |              |
|    total_timesteps      | 1604000      |
| train/                  |              |
|    approx_kl            | 0.0057983426 |
|    clip_fraction        | 0.05         |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.68        |
|    explained_variance   | 0.925        |
|    learning_rate        | 4.92e-05     |
|    loss                 | -0.0131      |
|    n_updates            | 7830         |
|    policy_gradient_loss | -0.00621     |
|    std                  | 0.561        |
|    value_loss           | 0.0358       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1605000, episode_reward=0.67 +/- 2.30
Episode length: 278.80 +/- 42.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 279      |
|    mean_reward     | 0.669    |
| time/              |          |
|    total_timesteps | 1605000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 631     |
|    iterations      | 784     |
|    time_elapsed    | 2544    |
|    total_timesteps | 1605632 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1606000, episode_reward=0.75 +/- 2.36
Episode length: 274.00 +/- 52.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 274         |
|    mean_reward          | 0.747       |
| time/                   |             |
|    total_timesteps      | 1606000     |
| train/                  |             |
|    approx_kl            | 0.005230912 |
|    clip_fraction        | 0.0439      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.68       |
|    explained_variance   | 0.937       |
|    learning_rate        | 4.93e-05    |
|    loss                 | -0.00132    |
|    n_updates            | 7840        |
|    policy_gradient_loss | -0.00635    |
|    std                  | 0.561       |
|    value_loss           | 0.0387      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1607000, episode_reward=3.94 +/- 2.55
Episode length: 187.40 +/- 57.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 187      |
|    mean_reward     | 3.94     |
| time/              |          |
|    total_timesteps | 1607000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 631     |
|    iterations      | 785     |
|    time_elapsed    | 2547    |
|    total_timesteps | 1607680 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1608000, episode_reward=3.18 +/- 2.80
Episode length: 237.00 +/- 57.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 237         |
|    mean_reward          | 3.18        |
| time/                   |             |
|    total_timesteps      | 1608000     |
| train/                  |             |
|    approx_kl            | 0.003799154 |
|    clip_fraction        | 0.0265      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.68       |
|    explained_variance   | 0.879       |
|    learning_rate        | 4.93e-05    |
|    loss                 | 0.00756     |
|    n_updates            | 7850        |
|    policy_gradient_loss | -0.00316    |
|    std                  | 0.562       |
|    value_loss           | 0.0186      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1609000, episode_reward=0.45 +/- 2.42
Episode length: 275.00 +/- 50.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 275      |
|    mean_reward     | 0.454    |
| time/              |          |
|    total_timesteps | 1609000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 631     |
|    iterations      | 786     |
|    time_elapsed    | 2550    |
|    total_timesteps | 1609728 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1610000, episode_reward=1.82 +/- 2.81
Episode length: 244.20 +/- 68.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 244          |
|    mean_reward          | 1.82         |
| time/                   |              |
|    total_timesteps      | 1610000      |
| train/                  |              |
|    approx_kl            | 0.0065519167 |
|    clip_fraction        | 0.0548       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.68        |
|    explained_variance   | 0.945        |
|    learning_rate        | 4.93e-05     |
|    loss                 | 0.0234       |
|    n_updates            | 7860         |
|    policy_gradient_loss | -0.00792     |
|    std                  | 0.562        |
|    value_loss           | 0.0351       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1611000, episode_reward=2.20 +/- 2.53
Episode length: 262.80 +/- 46.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 263      |
|    mean_reward     | 2.2      |
| time/              |          |
|    total_timesteps | 1611000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 631     |
|    iterations      | 787     |
|    time_elapsed    | 2553    |
|    total_timesteps | 1611776 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1612000, episode_reward=2.08 +/- 2.84
Episode length: 284.60 +/- 22.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 285          |
|    mean_reward          | 2.08         |
| time/                   |              |
|    total_timesteps      | 1612000      |
| train/                  |              |
|    approx_kl            | 0.0049384325 |
|    clip_fraction        | 0.0398       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.68        |
|    explained_variance   | 0.941        |
|    learning_rate        | 4.93e-05     |
|    loss                 | 0.0374       |
|    n_updates            | 7870         |
|    policy_gradient_loss | -0.00486     |
|    std                  | 0.563        |
|    value_loss           | 0.017        |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1613000, episode_reward=1.43 +/- 3.17
Episode length: 244.80 +/- 68.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 245      |
|    mean_reward     | 1.43     |
| time/              |          |
|    total_timesteps | 1613000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 631     |
|    iterations      | 788     |
|    time_elapsed    | 2556    |
|    total_timesteps | 1613824 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1614000, episode_reward=0.91 +/- 2.25
Episode length: 268.40 +/- 63.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 268         |
|    mean_reward          | 0.914       |
| time/                   |             |
|    total_timesteps      | 1614000     |
| train/                  |             |
|    approx_kl            | 0.006413252 |
|    clip_fraction        | 0.0444      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.69       |
|    explained_variance   | 0.959       |
|    learning_rate        | 4.94e-05    |
|    loss                 | 0.0235      |
|    n_updates            | 7880        |
|    policy_gradient_loss | -0.00336    |
|    std                  | 0.563       |
|    value_loss           | 0.0244      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1615000, episode_reward=0.92 +/- 2.37
Episode length: 281.80 +/- 36.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 282      |
|    mean_reward     | 0.925    |
| time/              |          |
|    total_timesteps | 1615000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 631     |
|    iterations      | 789     |
|    time_elapsed    | 2559    |
|    total_timesteps | 1615872 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1616000, episode_reward=4.07 +/- 2.55
Episode length: 209.40 +/- 54.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 209         |
|    mean_reward          | 4.07        |
| time/                   |             |
|    total_timesteps      | 1616000     |
| train/                  |             |
|    approx_kl            | 0.007643353 |
|    clip_fraction        | 0.0609      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.69       |
|    explained_variance   | 0.885       |
|    learning_rate        | 4.94e-05    |
|    loss                 | 0.0241      |
|    n_updates            | 7890        |
|    policy_gradient_loss | -0.0047     |
|    std                  | 0.563       |
|    value_loss           | 0.0599      |
-----------------------------------------
box reached target
Eval num_timesteps=1617000, episode_reward=0.64 +/- 2.35
Episode length: 277.60 +/- 44.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | 0.643    |
| time/              |          |
|    total_timesteps | 1617000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 631     |
|    iterations      | 790     |
|    time_elapsed    | 2562    |
|    total_timesteps | 1617920 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1618000, episode_reward=1.45 +/- 3.00
Episode length: 245.20 +/- 68.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 245         |
|    mean_reward          | 1.45        |
| time/                   |             |
|    total_timesteps      | 1618000     |
| train/                  |             |
|    approx_kl            | 0.005004067 |
|    clip_fraction        | 0.0405      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.69       |
|    explained_variance   | 0.955       |
|    learning_rate        | 4.94e-05    |
|    loss                 | 0.0129      |
|    n_updates            | 7900        |
|    policy_gradient_loss | -0.00637    |
|    std                  | 0.563       |
|    value_loss           | 0.0258      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1619000, episode_reward=5.36 +/- 0.16
Episode length: 188.00 +/- 24.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 188      |
|    mean_reward     | 5.36     |
| time/              |          |
|    total_timesteps | 1619000  |
---------------------------------
box reached target
box reached target
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 631     |
|    iterations      | 791     |
|    time_elapsed    | 2565    |
|    total_timesteps | 1619968 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1620000, episode_reward=1.85 +/- 2.97
Episode length: 258.00 +/- 55.28
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 258        |
|    mean_reward          | 1.85       |
| time/                   |            |
|    total_timesteps      | 1620000    |
| train/                  |            |
|    approx_kl            | 0.00433436 |
|    clip_fraction        | 0.046      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.69      |
|    explained_variance   | 0.936      |
|    learning_rate        | 4.94e-05   |
|    loss                 | 0.0359     |
|    n_updates            | 7910       |
|    policy_gradient_loss | -0.00643   |
|    std                  | 0.563      |
|    value_loss           | 0.0547     |
----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1621000, episode_reward=1.52 +/- 3.09
Episode length: 252.60 +/- 58.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 253      |
|    mean_reward     | 1.52     |
| time/              |          |
|    total_timesteps | 1621000  |
---------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1622000, episode_reward=1.59 +/- 2.98
Episode length: 250.20 +/- 64.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 250      |
|    mean_reward     | 1.59     |
| time/              |          |
|    total_timesteps | 1622000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 631     |
|    iterations      | 792     |
|    time_elapsed    | 2569    |
|    total_timesteps | 1622016 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1623000, episode_reward=3.02 +/- 2.96
Episode length: 231.00 +/- 60.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 231          |
|    mean_reward          | 3.02         |
| time/                   |              |
|    total_timesteps      | 1623000      |
| train/                  |              |
|    approx_kl            | 0.0046283426 |
|    clip_fraction        | 0.0393       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.69        |
|    explained_variance   | 0.857        |
|    learning_rate        | 4.95e-05     |
|    loss                 | -0.00643     |
|    n_updates            | 7920         |
|    policy_gradient_loss | -0.00446     |
|    std                  | 0.563        |
|    value_loss           | 0.0501       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1624000, episode_reward=4.08 +/- 2.54
Episode length: 206.20 +/- 53.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | 4.08     |
| time/              |          |
|    total_timesteps | 1624000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 631     |
|    iterations      | 793     |
|    time_elapsed    | 2571    |
|    total_timesteps | 1624064 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1625000, episode_reward=1.75 +/- 2.81
Episode length: 244.60 +/- 67.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 245          |
|    mean_reward          | 1.75         |
| time/                   |              |
|    total_timesteps      | 1625000      |
| train/                  |              |
|    approx_kl            | 0.0029646284 |
|    clip_fraction        | 0.0236       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.69        |
|    explained_variance   | 0.831        |
|    learning_rate        | 4.95e-05     |
|    loss                 | 0.0109       |
|    n_updates            | 7930         |
|    policy_gradient_loss | -0.00242     |
|    std                  | 0.563        |
|    value_loss           | 0.0395       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1626000, episode_reward=3.04 +/- 2.79
Episode length: 238.00 +/- 58.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 238      |
|    mean_reward     | 3.04     |
| time/              |          |
|    total_timesteps | 1626000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 631     |
|    iterations      | 794     |
|    time_elapsed    | 2574    |
|    total_timesteps | 1626112 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1627000, episode_reward=3.13 +/- 2.52
Episode length: 210.80 +/- 73.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 211          |
|    mean_reward          | 3.13         |
| time/                   |              |
|    total_timesteps      | 1627000      |
| train/                  |              |
|    approx_kl            | 0.0057768943 |
|    clip_fraction        | 0.0425       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.68        |
|    explained_variance   | 0.939        |
|    learning_rate        | 4.95e-05     |
|    loss                 | -0.015       |
|    n_updates            | 7940         |
|    policy_gradient_loss | -0.00507     |
|    std                  | 0.561        |
|    value_loss           | 0.0286       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1628000, episode_reward=0.40 +/- 2.42
Episode length: 271.40 +/- 57.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 271      |
|    mean_reward     | 0.405    |
| time/              |          |
|    total_timesteps | 1628000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 631     |
|    iterations      | 795     |
|    time_elapsed    | 2577    |
|    total_timesteps | 1628160 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1629000, episode_reward=2.59 +/- 3.23
Episode length: 215.40 +/- 70.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 215         |
|    mean_reward          | 2.59        |
| time/                   |             |
|    total_timesteps      | 1629000     |
| train/                  |             |
|    approx_kl            | 0.007311205 |
|    clip_fraction        | 0.0647      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.68       |
|    explained_variance   | 0.903       |
|    learning_rate        | 4.95e-05    |
|    loss                 | 0.0124      |
|    n_updates            | 7950        |
|    policy_gradient_loss | -0.00617    |
|    std                  | 0.56        |
|    value_loss           | 0.0658      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1630000, episode_reward=4.31 +/- 1.94
Episode length: 218.20 +/- 46.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 218      |
|    mean_reward     | 4.31     |
| time/              |          |
|    total_timesteps | 1630000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 631     |
|    iterations      | 796     |
|    time_elapsed    | 2580    |
|    total_timesteps | 1630208 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1631000, episode_reward=2.78 +/- 3.07
Episode length: 224.80 +/- 62.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 225         |
|    mean_reward          | 2.78        |
| time/                   |             |
|    total_timesteps      | 1631000     |
| train/                  |             |
|    approx_kl            | 0.004782454 |
|    clip_fraction        | 0.031       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.68       |
|    explained_variance   | 0.889       |
|    learning_rate        | 4.96e-05    |
|    loss                 | 0.019       |
|    n_updates            | 7960        |
|    policy_gradient_loss | -0.00467    |
|    std                  | 0.561       |
|    value_loss           | 0.0345      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1632000, episode_reward=1.90 +/- 2.82
Episode length: 249.00 +/- 62.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 249      |
|    mean_reward     | 1.9      |
| time/              |          |
|    total_timesteps | 1632000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 631     |
|    iterations      | 797     |
|    time_elapsed    | 2583    |
|    total_timesteps | 1632256 |
--------------------------------
box reached target
Eval num_timesteps=1633000, episode_reward=0.36 +/- 2.41
Episode length: 273.80 +/- 52.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 274          |
|    mean_reward          | 0.359        |
| time/                   |              |
|    total_timesteps      | 1633000      |
| train/                  |              |
|    approx_kl            | 0.0042642513 |
|    clip_fraction        | 0.024        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.68        |
|    explained_variance   | 0.929        |
|    learning_rate        | 4.96e-05     |
|    loss                 | -0.0177      |
|    n_updates            | 7970         |
|    policy_gradient_loss | -0.00439     |
|    std                  | 0.561        |
|    value_loss           | 0.0399       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1634000, episode_reward=2.82 +/- 2.88
Episode length: 229.20 +/- 59.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 229      |
|    mean_reward     | 2.82     |
| time/              |          |
|    total_timesteps | 1634000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 631     |
|    iterations      | 798     |
|    time_elapsed    | 2586    |
|    total_timesteps | 1634304 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1635000, episode_reward=1.89 +/- 2.80
Episode length: 243.80 +/- 69.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 244         |
|    mean_reward          | 1.89        |
| time/                   |             |
|    total_timesteps      | 1635000     |
| train/                  |             |
|    approx_kl            | 0.006120745 |
|    clip_fraction        | 0.0441      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.68       |
|    explained_variance   | 0.922       |
|    learning_rate        | 4.96e-05    |
|    loss                 | 0.0275      |
|    n_updates            | 7980        |
|    policy_gradient_loss | -0.00718    |
|    std                  | 0.561       |
|    value_loss           | 0.0321      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1636000, episode_reward=2.12 +/- 2.63
Episode length: 252.80 +/- 58.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 253      |
|    mean_reward     | 2.12     |
| time/              |          |
|    total_timesteps | 1636000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 631     |
|    iterations      | 799     |
|    time_elapsed    | 2589    |
|    total_timesteps | 1636352 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1637000, episode_reward=1.97 +/- 2.66
Episode length: 252.20 +/- 59.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 252          |
|    mean_reward          | 1.97         |
| time/                   |              |
|    total_timesteps      | 1637000      |
| train/                  |              |
|    approx_kl            | 0.0043779467 |
|    clip_fraction        | 0.023        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.68        |
|    explained_variance   | 0.945        |
|    learning_rate        | 4.96e-05     |
|    loss                 | 0.0108       |
|    n_updates            | 7990         |
|    policy_gradient_loss | -0.00263     |
|    std                  | 0.56         |
|    value_loss           | 0.0217       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1638000, episode_reward=2.11 +/- 2.66
Episode length: 253.80 +/- 62.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 254      |
|    mean_reward     | 2.11     |
| time/              |          |
|    total_timesteps | 1638000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 631     |
|    iterations      | 800     |
|    time_elapsed    | 2592    |
|    total_timesteps | 1638400 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1639000, episode_reward=0.41 +/- 2.45
Episode length: 275.80 +/- 48.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 276         |
|    mean_reward          | 0.414       |
| time/                   |             |
|    total_timesteps      | 1639000     |
| train/                  |             |
|    approx_kl            | 0.004433105 |
|    clip_fraction        | 0.0247      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.68       |
|    explained_variance   | 0.954       |
|    learning_rate        | 4.97e-05    |
|    loss                 | 0.0081      |
|    n_updates            | 8000        |
|    policy_gradient_loss | -0.00449    |
|    std                  | 0.562       |
|    value_loss           | 0.0339      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1640000, episode_reward=2.73 +/- 3.16
Episode length: 243.40 +/- 61.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 243      |
|    mean_reward     | 2.73     |
| time/              |          |
|    total_timesteps | 1640000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 631     |
|    iterations      | 801     |
|    time_elapsed    | 2595    |
|    total_timesteps | 1640448 |
--------------------------------
box reached target
Eval num_timesteps=1641000, episode_reward=0.59 +/- 2.58
Episode length: 288.60 +/- 22.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 289          |
|    mean_reward          | 0.593        |
| time/                   |              |
|    total_timesteps      | 1641000      |
| train/                  |              |
|    approx_kl            | 0.0040956913 |
|    clip_fraction        | 0.0372       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.69        |
|    explained_variance   | 0.829        |
|    learning_rate        | 4.97e-05     |
|    loss                 | -0.00442     |
|    n_updates            | 8010         |
|    policy_gradient_loss | -0.00403     |
|    std                  | 0.564        |
|    value_loss           | 0.0336       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1642000, episode_reward=1.54 +/- 3.02
Episode length: 258.60 +/- 52.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 259      |
|    mean_reward     | 1.54     |
| time/              |          |
|    total_timesteps | 1642000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 631     |
|    iterations      | 802     |
|    time_elapsed    | 2599    |
|    total_timesteps | 1642496 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1643000, episode_reward=2.74 +/- 3.12
Episode length: 241.40 +/- 54.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 241         |
|    mean_reward          | 2.74        |
| time/                   |             |
|    total_timesteps      | 1643000     |
| train/                  |             |
|    approx_kl            | 0.006209145 |
|    clip_fraction        | 0.069       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.69       |
|    explained_variance   | 0.945       |
|    learning_rate        | 4.97e-05    |
|    loss                 | -0.00949    |
|    n_updates            | 8020        |
|    policy_gradient_loss | -0.00919    |
|    std                  | 0.565       |
|    value_loss           | 0.0125      |
-----------------------------------------
box reached target
Eval num_timesteps=1644000, episode_reward=1.12 +/- 2.25
Episode length: 284.40 +/- 31.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 284      |
|    mean_reward     | 1.12     |
| time/              |          |
|    total_timesteps | 1644000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 632     |
|    iterations      | 803     |
|    time_elapsed    | 2602    |
|    total_timesteps | 1644544 |
--------------------------------
box reached target
Eval num_timesteps=1645000, episode_reward=0.68 +/- 2.53
Episode length: 282.40 +/- 35.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 282          |
|    mean_reward          | 0.685        |
| time/                   |              |
|    total_timesteps      | 1645000      |
| train/                  |              |
|    approx_kl            | 0.0041249814 |
|    clip_fraction        | 0.0222       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.7         |
|    explained_variance   | 0.875        |
|    learning_rate        | 4.97e-05     |
|    loss                 | 0.0114       |
|    n_updates            | 8030         |
|    policy_gradient_loss | -0.00309     |
|    std                  | 0.566        |
|    value_loss           | 0.0366       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=1646000, episode_reward=2.13 +/- 2.57
Episode length: 240.80 +/- 72.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 241      |
|    mean_reward     | 2.13     |
| time/              |          |
|    total_timesteps | 1646000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 632     |
|    iterations      | 804     |
|    time_elapsed    | 2605    |
|    total_timesteps | 1646592 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1647000, episode_reward=5.22 +/- 0.13
Episode length: 174.60 +/- 7.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 175         |
|    mean_reward          | 5.22        |
| time/                   |             |
|    total_timesteps      | 1647000     |
| train/                  |             |
|    approx_kl            | 0.005200779 |
|    clip_fraction        | 0.0404      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | 0.889       |
|    learning_rate        | 4.98e-05    |
|    loss                 | 0.00198     |
|    n_updates            | 8040        |
|    policy_gradient_loss | -0.00521    |
|    std                  | 0.566       |
|    value_loss           | 0.014       |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1648000, episode_reward=0.24 +/- 2.47
Episode length: 273.80 +/- 52.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 274      |
|    mean_reward     | 0.245    |
| time/              |          |
|    total_timesteps | 1648000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 632     |
|    iterations      | 805     |
|    time_elapsed    | 2608    |
|    total_timesteps | 1648640 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1649000, episode_reward=1.86 +/- 2.89
Episode length: 259.40 +/- 49.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 259          |
|    mean_reward          | 1.86         |
| time/                   |              |
|    total_timesteps      | 1649000      |
| train/                  |              |
|    approx_kl            | 0.0033883592 |
|    clip_fraction        | 0.0189       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.69        |
|    explained_variance   | 0.918        |
|    learning_rate        | 4.98e-05     |
|    loss                 | 0.0156       |
|    n_updates            | 8050         |
|    policy_gradient_loss | -0.00328     |
|    std                  | 0.564        |
|    value_loss           | 0.0466       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1650000, episode_reward=3.07 +/- 2.73
Episode length: 225.60 +/- 61.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 226      |
|    mean_reward     | 3.07     |
| time/              |          |
|    total_timesteps | 1650000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 632     |
|    iterations      | 806     |
|    time_elapsed    | 2611    |
|    total_timesteps | 1650688 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1651000, episode_reward=3.03 +/- 2.80
Episode length: 216.80 +/- 68.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 217          |
|    mean_reward          | 3.03         |
| time/                   |              |
|    total_timesteps      | 1651000      |
| train/                  |              |
|    approx_kl            | 0.0054492364 |
|    clip_fraction        | 0.0357       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.69        |
|    explained_variance   | 0.967        |
|    learning_rate        | 4.98e-05     |
|    loss                 | -0.0189      |
|    n_updates            | 8060         |
|    policy_gradient_loss | -0.00701     |
|    std                  | 0.564        |
|    value_loss           | 0.0141       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1652000, episode_reward=2.75 +/- 3.10
Episode length: 237.20 +/- 51.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 237      |
|    mean_reward     | 2.75     |
| time/              |          |
|    total_timesteps | 1652000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 632     |
|    iterations      | 807     |
|    time_elapsed    | 2613    |
|    total_timesteps | 1652736 |
--------------------------------
box reached target
Eval num_timesteps=1653000, episode_reward=0.90 +/- 2.26
Episode length: 283.80 +/- 32.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 284          |
|    mean_reward          | 0.896        |
| time/                   |              |
|    total_timesteps      | 1653000      |
| train/                  |              |
|    approx_kl            | 0.0043622246 |
|    clip_fraction        | 0.0192       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.69        |
|    explained_variance   | 0.946        |
|    learning_rate        | 4.98e-05     |
|    loss                 | -0.0163      |
|    n_updates            | 8070         |
|    policy_gradient_loss | -0.00321     |
|    std                  | 0.564        |
|    value_loss           | 0.0239       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=1654000, episode_reward=1.09 +/- 2.09
Episode length: 278.00 +/- 44.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | 1.09     |
| time/              |          |
|    total_timesteps | 1654000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 632     |
|    iterations      | 808     |
|    time_elapsed    | 2617    |
|    total_timesteps | 1654784 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1655000, episode_reward=0.43 +/- 2.39
Episode length: 276.00 +/- 48.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 276          |
|    mean_reward          | 0.427        |
| time/                   |              |
|    total_timesteps      | 1655000      |
| train/                  |              |
|    approx_kl            | 0.0049892063 |
|    clip_fraction        | 0.0352       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.69        |
|    explained_variance   | 0.851        |
|    learning_rate        | 4.99e-05     |
|    loss                 | 0.0216       |
|    n_updates            | 8080         |
|    policy_gradient_loss | -0.00436     |
|    std                  | 0.564        |
|    value_loss           | 0.0563       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1656000, episode_reward=2.87 +/- 2.91
Episode length: 217.80 +/- 67.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 218      |
|    mean_reward     | 2.87     |
| time/              |          |
|    total_timesteps | 1656000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 632     |
|    iterations      | 809     |
|    time_elapsed    | 2620    |
|    total_timesteps | 1656832 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1657000, episode_reward=4.01 +/- 2.69
Episode length: 211.20 +/- 64.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 211         |
|    mean_reward          | 4.01        |
| time/                   |             |
|    total_timesteps      | 1657000     |
| train/                  |             |
|    approx_kl            | 0.005390712 |
|    clip_fraction        | 0.0517      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.69       |
|    explained_variance   | 0.935       |
|    learning_rate        | 4.99e-05    |
|    loss                 | -0.0147     |
|    n_updates            | 8090        |
|    policy_gradient_loss | -0.00992    |
|    std                  | 0.564       |
|    value_loss           | 0.033       |
-----------------------------------------
box reached target
Eval num_timesteps=1658000, episode_reward=0.10 +/- 2.51
Episode length: 279.00 +/- 42.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 279      |
|    mean_reward     | 0.104    |
| time/              |          |
|    total_timesteps | 1658000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 632     |
|    iterations      | 810     |
|    time_elapsed    | 2623    |
|    total_timesteps | 1658880 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1659000, episode_reward=1.49 +/- 3.09
Episode length: 244.80 +/- 67.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 245          |
|    mean_reward          | 1.49         |
| time/                   |              |
|    total_timesteps      | 1659000      |
| train/                  |              |
|    approx_kl            | 0.0067756916 |
|    clip_fraction        | 0.042        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.69        |
|    explained_variance   | 0.923        |
|    learning_rate        | 4.99e-05     |
|    loss                 | -0.0114      |
|    n_updates            | 8100         |
|    policy_gradient_loss | -0.00556     |
|    std                  | 0.563        |
|    value_loss           | 0.0181       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=1660000, episode_reward=1.73 +/- 2.85
Episode length: 244.00 +/- 69.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 244      |
|    mean_reward     | 1.73     |
| time/              |          |
|    total_timesteps | 1660000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 632     |
|    iterations      | 811     |
|    time_elapsed    | 2626    |
|    total_timesteps | 1660928 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1661000, episode_reward=1.57 +/- 2.98
Episode length: 251.60 +/- 63.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 252          |
|    mean_reward          | 1.57         |
| time/                   |              |
|    total_timesteps      | 1661000      |
| train/                  |              |
|    approx_kl            | 0.0057953456 |
|    clip_fraction        | 0.037        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.69        |
|    explained_variance   | 0.889        |
|    learning_rate        | 4.99e-05     |
|    loss                 | 0.052        |
|    n_updates            | 8110         |
|    policy_gradient_loss | -0.00562     |
|    std                  | 0.563        |
|    value_loss           | 0.0441       |
------------------------------------------
box reached target
Eval num_timesteps=1662000, episode_reward=0.43 +/- 2.47
Episode length: 268.80 +/- 62.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 269      |
|    mean_reward     | 0.434    |
| time/              |          |
|    total_timesteps | 1662000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 632     |
|    iterations      | 812     |
|    time_elapsed    | 2629    |
|    total_timesteps | 1662976 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1663000, episode_reward=3.02 +/- 2.77
Episode length: 208.80 +/- 74.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 209          |
|    mean_reward          | 3.02         |
| time/                   |              |
|    total_timesteps      | 1663000      |
| train/                  |              |
|    approx_kl            | 0.0060271192 |
|    clip_fraction        | 0.0525       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.68        |
|    explained_variance   | 0.95         |
|    learning_rate        | 5e-05        |
|    loss                 | -0.0148      |
|    n_updates            | 8120         |
|    policy_gradient_loss | -0.00682     |
|    std                  | 0.562        |
|    value_loss           | 0.0124       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1664000, episode_reward=1.47 +/- 3.12
Episode length: 263.00 +/- 48.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 263      |
|    mean_reward     | 1.47     |
| time/              |          |
|    total_timesteps | 1664000  |
---------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1665000, episode_reward=2.95 +/- 2.92
Episode length: 214.80 +/- 69.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 215      |
|    mean_reward     | 2.95     |
| time/              |          |
|    total_timesteps | 1665000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 632     |
|    iterations      | 813     |
|    time_elapsed    | 2632    |
|    total_timesteps | 1665024 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1666000, episode_reward=0.55 +/- 2.40
Episode length: 269.40 +/- 61.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 269          |
|    mean_reward          | 0.55         |
| time/                   |              |
|    total_timesteps      | 1666000      |
| train/                  |              |
|    approx_kl            | 0.0025995728 |
|    clip_fraction        | 0.0212       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.68        |
|    explained_variance   | 0.915        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.0158       |
|    n_updates            | 8130         |
|    policy_gradient_loss | -0.00267     |
|    std                  | 0.562        |
|    value_loss           | 0.0388       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1667000, episode_reward=2.24 +/- 2.72
Episode length: 254.40 +/- 57.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 254      |
|    mean_reward     | 2.24     |
| time/              |          |
|    total_timesteps | 1667000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 632     |
|    iterations      | 814     |
|    time_elapsed    | 2635    |
|    total_timesteps | 1667072 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1668000, episode_reward=1.50 +/- 3.06
Episode length: 243.60 +/- 69.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 244         |
|    mean_reward          | 1.5         |
| time/                   |             |
|    total_timesteps      | 1668000     |
| train/                  |             |
|    approx_kl            | 0.005300107 |
|    clip_fraction        | 0.0352      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.68       |
|    explained_variance   | 0.8         |
|    learning_rate        | 5e-05       |
|    loss                 | -0.0107     |
|    n_updates            | 8140        |
|    policy_gradient_loss | -0.00447    |
|    std                  | 0.563       |
|    value_loss           | 0.0706      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1669000, episode_reward=4.04 +/- 2.52
Episode length: 178.40 +/- 61.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 178      |
|    mean_reward     | 4.04     |
| time/              |          |
|    total_timesteps | 1669000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 632     |
|    iterations      | 815     |
|    time_elapsed    | 2638    |
|    total_timesteps | 1669120 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1670000, episode_reward=2.91 +/- 3.24
Episode length: 264.00 +/- 32.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 264          |
|    mean_reward          | 2.91         |
| time/                   |              |
|    total_timesteps      | 1670000      |
| train/                  |              |
|    approx_kl            | 0.0067790286 |
|    clip_fraction        | 0.0663       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.68        |
|    explained_variance   | 0.953        |
|    learning_rate        | 5e-05        |
|    loss                 | -0.00473     |
|    n_updates            | 8150         |
|    policy_gradient_loss | -0.00916     |
|    std                  | 0.56         |
|    value_loss           | 0.0194       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1671000, episode_reward=3.11 +/- 2.85
Episode length: 236.00 +/- 60.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 236      |
|    mean_reward     | 3.11     |
| time/              |          |
|    total_timesteps | 1671000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 632     |
|    iterations      | 816     |
|    time_elapsed    | 2641    |
|    total_timesteps | 1671168 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1672000, episode_reward=0.91 +/- 2.16
Episode length: 277.20 +/- 45.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 277          |
|    mean_reward          | 0.907        |
| time/                   |              |
|    total_timesteps      | 1672000      |
| train/                  |              |
|    approx_kl            | 0.0028728307 |
|    clip_fraction        | 0.0199       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.68        |
|    explained_variance   | 0.923        |
|    learning_rate        | 5.01e-05     |
|    loss                 | 0.000318     |
|    n_updates            | 8160         |
|    policy_gradient_loss | -0.00297     |
|    std                  | 0.562        |
|    value_loss           | 0.0279       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1673000, episode_reward=4.11 +/- 2.25
Episode length: 206.20 +/- 47.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | 4.11     |
| time/              |          |
|    total_timesteps | 1673000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 632     |
|    iterations      | 817     |
|    time_elapsed    | 2644    |
|    total_timesteps | 1673216 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1674000, episode_reward=1.99 +/- 2.88
Episode length: 259.80 +/- 57.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 260          |
|    mean_reward          | 1.99         |
| time/                   |              |
|    total_timesteps      | 1674000      |
| train/                  |              |
|    approx_kl            | 0.0027707322 |
|    clip_fraction        | 0.0146       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.68        |
|    explained_variance   | 0.908        |
|    learning_rate        | 5.01e-05     |
|    loss                 | 0.00523      |
|    n_updates            | 8170         |
|    policy_gradient_loss | -0.00242     |
|    std                  | 0.563        |
|    value_loss           | 0.0666       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=1675000, episode_reward=0.42 +/- 2.40
Episode length: 274.40 +/- 51.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 274      |
|    mean_reward     | 0.421    |
| time/              |          |
|    total_timesteps | 1675000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 632     |
|    iterations      | 818     |
|    time_elapsed    | 2647    |
|    total_timesteps | 1675264 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1676000, episode_reward=1.86 +/- 2.82
Episode length: 249.40 +/- 64.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 249         |
|    mean_reward          | 1.86        |
| time/                   |             |
|    total_timesteps      | 1676000     |
| train/                  |             |
|    approx_kl            | 0.007044318 |
|    clip_fraction        | 0.0532      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.68       |
|    explained_variance   | 0.949       |
|    learning_rate        | 5.01e-05    |
|    loss                 | 0.00133     |
|    n_updates            | 8180        |
|    policy_gradient_loss | -0.00562    |
|    std                  | 0.562       |
|    value_loss           | 0.0229      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1677000, episode_reward=3.32 +/- 2.42
Episode length: 231.80 +/- 60.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 232      |
|    mean_reward     | 3.32     |
| time/              |          |
|    total_timesteps | 1677000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 632     |
|    iterations      | 819     |
|    time_elapsed    | 2650    |
|    total_timesteps | 1677312 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1678000, episode_reward=1.46 +/- 3.01
Episode length: 245.60 +/- 66.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 246          |
|    mean_reward          | 1.46         |
| time/                   |              |
|    total_timesteps      | 1678000      |
| train/                  |              |
|    approx_kl            | 0.0052953647 |
|    clip_fraction        | 0.0408       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.68        |
|    explained_variance   | 0.848        |
|    learning_rate        | 5.01e-05     |
|    loss                 | 0.0131       |
|    n_updates            | 8190         |
|    policy_gradient_loss | -0.00663     |
|    std                  | 0.563        |
|    value_loss           | 0.0743       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=1679000, episode_reward=0.26 +/- 2.66
Episode length: 285.60 +/- 28.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 286      |
|    mean_reward     | 0.259    |
| time/              |          |
|    total_timesteps | 1679000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 632     |
|    iterations      | 820     |
|    time_elapsed    | 2653    |
|    total_timesteps | 1679360 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1680000, episode_reward=0.91 +/- 2.26
Episode length: 269.80 +/- 60.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 270         |
|    mean_reward          | 0.914       |
| time/                   |             |
|    total_timesteps      | 1680000     |
| train/                  |             |
|    approx_kl            | 0.005637842 |
|    clip_fraction        | 0.0531      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.69       |
|    explained_variance   | 0.884       |
|    learning_rate        | 5.02e-05    |
|    loss                 | -0.0174     |
|    n_updates            | 8200        |
|    policy_gradient_loss | -0.0088     |
|    std                  | 0.565       |
|    value_loss           | 0.036       |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1681000, episode_reward=2.23 +/- 2.52
Episode length: 246.00 +/- 66.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 246      |
|    mean_reward     | 2.23     |
| time/              |          |
|    total_timesteps | 1681000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 632     |
|    iterations      | 821     |
|    time_elapsed    | 2656    |
|    total_timesteps | 1681408 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1682000, episode_reward=2.15 +/- 2.91
Episode length: 252.00 +/- 67.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 252          |
|    mean_reward          | 2.15         |
| time/                   |              |
|    total_timesteps      | 1682000      |
| train/                  |              |
|    approx_kl            | 0.0043296763 |
|    clip_fraction        | 0.0375       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.69        |
|    explained_variance   | 0.954        |
|    learning_rate        | 5.02e-05     |
|    loss                 | 0.0052       |
|    n_updates            | 8210         |
|    policy_gradient_loss | -0.0048      |
|    std                  | 0.563        |
|    value_loss           | 0.0193       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1683000, episode_reward=-0.53 +/- 0.75
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.534   |
| time/              |          |
|    total_timesteps | 1683000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 632     |
|    iterations      | 822     |
|    time_elapsed    | 2659    |
|    total_timesteps | 1683456 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1684000, episode_reward=3.28 +/- 2.55
Episode length: 225.40 +/- 66.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 225          |
|    mean_reward          | 3.28         |
| time/                   |              |
|    total_timesteps      | 1684000      |
| train/                  |              |
|    approx_kl            | 0.0054150443 |
|    clip_fraction        | 0.0522       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.69        |
|    explained_variance   | 0.962        |
|    learning_rate        | 5.02e-05     |
|    loss                 | -0.0043      |
|    n_updates            | 8220         |
|    policy_gradient_loss | -0.00704     |
|    std                  | 0.563        |
|    value_loss           | 0.0147       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1685000, episode_reward=2.04 +/- 2.70
Episode length: 250.80 +/- 60.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | 2.04     |
| time/              |          |
|    total_timesteps | 1685000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 632     |
|    iterations      | 823     |
|    time_elapsed    | 2662    |
|    total_timesteps | 1685504 |
--------------------------------
box reached target
Eval num_timesteps=1686000, episode_reward=0.31 +/- 2.56
Episode length: 286.40 +/- 27.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 286         |
|    mean_reward          | 0.308       |
| time/                   |             |
|    total_timesteps      | 1686000     |
| train/                  |             |
|    approx_kl            | 0.004977298 |
|    clip_fraction        | 0.0366      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.68       |
|    explained_variance   | 0.953       |
|    learning_rate        | 5.02e-05    |
|    loss                 | 0.0104      |
|    n_updates            | 8230        |
|    policy_gradient_loss | -0.00535    |
|    std                  | 0.562       |
|    value_loss           | 0.0133      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1687000, episode_reward=1.76 +/- 2.95
Episode length: 252.20 +/- 58.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 252      |
|    mean_reward     | 1.76     |
| time/              |          |
|    total_timesteps | 1687000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 633     |
|    iterations      | 824     |
|    time_elapsed    | 2665    |
|    total_timesteps | 1687552 |
--------------------------------
box reached target
Eval num_timesteps=1688000, episode_reward=0.85 +/- 2.27
Episode length: 271.60 +/- 56.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 272         |
|    mean_reward          | 0.852       |
| time/                   |             |
|    total_timesteps      | 1688000     |
| train/                  |             |
|    approx_kl            | 0.005691091 |
|    clip_fraction        | 0.046       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.68       |
|    explained_variance   | 0.942       |
|    learning_rate        | 5.03e-05    |
|    loss                 | 0.0126      |
|    n_updates            | 8240        |
|    policy_gradient_loss | -0.00636    |
|    std                  | 0.564       |
|    value_loss           | 0.0151      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1689000, episode_reward=4.40 +/- 1.89
Episode length: 200.80 +/- 52.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 201      |
|    mean_reward     | 4.4      |
| time/              |          |
|    total_timesteps | 1689000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 633     |
|    iterations      | 825     |
|    time_elapsed    | 2668    |
|    total_timesteps | 1689600 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1690000, episode_reward=1.46 +/- 3.24
Episode length: 266.80 +/- 41.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 267         |
|    mean_reward          | 1.46        |
| time/                   |             |
|    total_timesteps      | 1690000     |
| train/                  |             |
|    approx_kl            | 0.006493587 |
|    clip_fraction        | 0.0455      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.69       |
|    explained_variance   | 0.93        |
|    learning_rate        | 5.03e-05    |
|    loss                 | 0.0323      |
|    n_updates            | 8250        |
|    policy_gradient_loss | -0.00617    |
|    std                  | 0.565       |
|    value_loss           | 0.0195      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1691000, episode_reward=1.50 +/- 3.06
Episode length: 250.20 +/- 61.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 250      |
|    mean_reward     | 1.5      |
| time/              |          |
|    total_timesteps | 1691000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 633     |
|    iterations      | 826     |
|    time_elapsed    | 2671    |
|    total_timesteps | 1691648 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1692000, episode_reward=1.38 +/- 3.12
Episode length: 262.00 +/- 47.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 262          |
|    mean_reward          | 1.38         |
| time/                   |              |
|    total_timesteps      | 1692000      |
| train/                  |              |
|    approx_kl            | 0.0044429116 |
|    clip_fraction        | 0.0433       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.7         |
|    explained_variance   | 0.888        |
|    learning_rate        | 5.03e-05     |
|    loss                 | -0.0145      |
|    n_updates            | 8260         |
|    policy_gradient_loss | -0.0063      |
|    std                  | 0.566        |
|    value_loss           | 0.012        |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1693000, episode_reward=2.97 +/- 2.75
Episode length: 219.80 +/- 65.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 220      |
|    mean_reward     | 2.97     |
| time/              |          |
|    total_timesteps | 1693000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 633     |
|    iterations      | 827     |
|    time_elapsed    | 2674    |
|    total_timesteps | 1693696 |
--------------------------------
box reached target
Eval num_timesteps=1694000, episode_reward=0.59 +/- 2.32
Episode length: 277.00 +/- 46.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 277         |
|    mean_reward          | 0.586       |
| time/                   |             |
|    total_timesteps      | 1694000     |
| train/                  |             |
|    approx_kl            | 0.004822731 |
|    clip_fraction        | 0.031       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | 0.966       |
|    learning_rate        | 5.03e-05    |
|    loss                 | -0.019      |
|    n_updates            | 8270        |
|    policy_gradient_loss | -0.00765    |
|    std                  | 0.567       |
|    value_loss           | 0.0169      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1695000, episode_reward=2.91 +/- 2.88
Episode length: 221.00 +/- 65.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 221      |
|    mean_reward     | 2.91     |
| time/              |          |
|    total_timesteps | 1695000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 633     |
|    iterations      | 828     |
|    time_elapsed    | 2677    |
|    total_timesteps | 1695744 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1696000, episode_reward=4.06 +/- 2.54
Episode length: 187.00 +/- 58.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 187          |
|    mean_reward          | 4.06         |
| time/                   |              |
|    total_timesteps      | 1696000      |
| train/                  |              |
|    approx_kl            | 0.0041874647 |
|    clip_fraction        | 0.0474       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.7         |
|    explained_variance   | 0.947        |
|    learning_rate        | 5.03e-05     |
|    loss                 | 0.00505      |
|    n_updates            | 8280         |
|    policy_gradient_loss | -0.00548     |
|    std                  | 0.568        |
|    value_loss           | 0.0245       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1697000, episode_reward=2.91 +/- 2.89
Episode length: 222.20 +/- 66.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 222      |
|    mean_reward     | 2.91     |
| time/              |          |
|    total_timesteps | 1697000  |
---------------------------------
box reached target
box reached target
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 633     |
|    iterations      | 829     |
|    time_elapsed    | 2680    |
|    total_timesteps | 1697792 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1698000, episode_reward=1.58 +/- 3.04
Episode length: 250.60 +/- 60.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 251          |
|    mean_reward          | 1.58         |
| time/                   |              |
|    total_timesteps      | 1698000      |
| train/                  |              |
|    approx_kl            | 0.0037467906 |
|    clip_fraction        | 0.0198       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.7         |
|    explained_variance   | 0.913        |
|    learning_rate        | 5.04e-05     |
|    loss                 | 0.0213       |
|    n_updates            | 8290         |
|    policy_gradient_loss | -0.00448     |
|    std                  | 0.567        |
|    value_loss           | 0.0521       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1699000, episode_reward=0.72 +/- 2.31
Episode length: 268.80 +/- 62.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 269      |
|    mean_reward     | 0.717    |
| time/              |          |
|    total_timesteps | 1699000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 633     |
|    iterations      | 830     |
|    time_elapsed    | 2683    |
|    total_timesteps | 1699840 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1700000, episode_reward=2.78 +/- 3.09
Episode length: 221.80 +/- 64.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 222          |
|    mean_reward          | 2.78         |
| time/                   |              |
|    total_timesteps      | 1700000      |
| train/                  |              |
|    approx_kl            | 0.0035866518 |
|    clip_fraction        | 0.0387       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.7         |
|    explained_variance   | 0.888        |
|    learning_rate        | 5.04e-05     |
|    loss                 | 0.0704       |
|    n_updates            | 8300         |
|    policy_gradient_loss | -0.00392     |
|    std                  | 0.565        |
|    value_loss           | 0.0803       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1701000, episode_reward=4.22 +/- 2.34
Episode length: 213.00 +/- 51.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 213      |
|    mean_reward     | 4.22     |
| time/              |          |
|    total_timesteps | 1701000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 633     |
|    iterations      | 831     |
|    time_elapsed    | 2686    |
|    total_timesteps | 1701888 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1702000, episode_reward=3.12 +/- 2.81
Episode length: 238.40 +/- 57.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 238         |
|    mean_reward          | 3.12        |
| time/                   |             |
|    total_timesteps      | 1702000     |
| train/                  |             |
|    approx_kl            | 0.006061256 |
|    clip_fraction        | 0.0603      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.69       |
|    explained_variance   | 0.959       |
|    learning_rate        | 5.04e-05    |
|    loss                 | -0.0169     |
|    n_updates            | 8310        |
|    policy_gradient_loss | -0.00744    |
|    std                  | 0.565       |
|    value_loss           | 0.0184      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1703000, episode_reward=1.99 +/- 2.79
Episode length: 269.60 +/- 55.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 270      |
|    mean_reward     | 1.99     |
| time/              |          |
|    total_timesteps | 1703000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 633     |
|    iterations      | 832     |
|    time_elapsed    | 2689    |
|    total_timesteps | 1703936 |
--------------------------------
box reached target
Eval num_timesteps=1704000, episode_reward=0.86 +/- 2.38
Episode length: 277.40 +/- 45.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 277         |
|    mean_reward          | 0.862       |
| time/                   |             |
|    total_timesteps      | 1704000     |
| train/                  |             |
|    approx_kl            | 0.005352091 |
|    clip_fraction        | 0.0369      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | 0.916       |
|    learning_rate        | 5.04e-05    |
|    loss                 | 0.0112      |
|    n_updates            | 8320        |
|    policy_gradient_loss | -0.00431    |
|    std                  | 0.566       |
|    value_loss           | 0.0433      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1705000, episode_reward=2.89 +/- 2.98
Episode length: 241.40 +/- 47.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 241      |
|    mean_reward     | 2.89     |
| time/              |          |
|    total_timesteps | 1705000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 633     |
|    iterations      | 833     |
|    time_elapsed    | 2692    |
|    total_timesteps | 1705984 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1706000, episode_reward=1.78 +/- 3.06
Episode length: 243.60 +/- 69.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 244          |
|    mean_reward          | 1.78         |
| time/                   |              |
|    total_timesteps      | 1706000      |
| train/                  |              |
|    approx_kl            | 0.0038033854 |
|    clip_fraction        | 0.0368       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.7         |
|    explained_variance   | 0.924        |
|    learning_rate        | 5.05e-05     |
|    loss                 | -0.0186      |
|    n_updates            | 8330         |
|    policy_gradient_loss | -0.00826     |
|    std                  | 0.566        |
|    value_loss           | 0.0332       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1707000, episode_reward=1.83 +/- 2.88
Episode length: 252.00 +/- 59.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 252      |
|    mean_reward     | 1.83     |
| time/              |          |
|    total_timesteps | 1707000  |
---------------------------------
box reached target
box reached target
Eval num_timesteps=1708000, episode_reward=0.35 +/- 2.42
Episode length: 271.80 +/- 56.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 272      |
|    mean_reward     | 0.347    |
| time/              |          |
|    total_timesteps | 1708000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 633     |
|    iterations      | 834     |
|    time_elapsed    | 2696    |
|    total_timesteps | 1708032 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1709000, episode_reward=2.36 +/- 2.56
Episode length: 262.40 +/- 46.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 262         |
|    mean_reward          | 2.36        |
| time/                   |             |
|    total_timesteps      | 1709000     |
| train/                  |             |
|    approx_kl            | 0.006091606 |
|    clip_fraction        | 0.0463      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | 0.892       |
|    learning_rate        | 5.05e-05    |
|    loss                 | -0.0316     |
|    n_updates            | 8340        |
|    policy_gradient_loss | -0.00857    |
|    std                  | 0.566       |
|    value_loss           | 0.0213      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1710000, episode_reward=1.82 +/- 2.83
Episode length: 242.40 +/- 70.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 242      |
|    mean_reward     | 1.82     |
| time/              |          |
|    total_timesteps | 1710000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 633     |
|    iterations      | 835     |
|    time_elapsed    | 2699    |
|    total_timesteps | 1710080 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1711000, episode_reward=2.27 +/- 2.41
Episode length: 250.40 +/- 60.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 250         |
|    mean_reward          | 2.27        |
| time/                   |             |
|    total_timesteps      | 1711000     |
| train/                  |             |
|    approx_kl            | 0.004461786 |
|    clip_fraction        | 0.0473      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | 0.906       |
|    learning_rate        | 5.05e-05    |
|    loss                 | 0.0371      |
|    n_updates            | 8350        |
|    policy_gradient_loss | -0.00719    |
|    std                  | 0.566       |
|    value_loss           | 0.0357      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1712000, episode_reward=4.06 +/- 2.70
Episode length: 217.00 +/- 57.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 217      |
|    mean_reward     | 4.06     |
| time/              |          |
|    total_timesteps | 1712000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 633     |
|    iterations      | 836     |
|    time_elapsed    | 2702    |
|    total_timesteps | 1712128 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1713000, episode_reward=1.76 +/- 2.98
Episode length: 266.00 +/- 46.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 266          |
|    mean_reward          | 1.76         |
| time/                   |              |
|    total_timesteps      | 1713000      |
| train/                  |              |
|    approx_kl            | 0.0037385817 |
|    clip_fraction        | 0.0199       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.7         |
|    explained_variance   | 0.94         |
|    learning_rate        | 5.05e-05     |
|    loss                 | 0.0198       |
|    n_updates            | 8360         |
|    policy_gradient_loss | -0.00114     |
|    std                  | 0.566        |
|    value_loss           | 0.0187       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1714000, episode_reward=3.89 +/- 2.67
Episode length: 193.20 +/- 56.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 193      |
|    mean_reward     | 3.89     |
| time/              |          |
|    total_timesteps | 1714000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 633     |
|    iterations      | 837     |
|    time_elapsed    | 2705    |
|    total_timesteps | 1714176 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1715000, episode_reward=3.02 +/- 2.80
Episode length: 241.60 +/- 49.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 242         |
|    mean_reward          | 3.02        |
| time/                   |             |
|    total_timesteps      | 1715000     |
| train/                  |             |
|    approx_kl            | 0.003704257 |
|    clip_fraction        | 0.0253      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | 0.95        |
|    learning_rate        | 5.06e-05    |
|    loss                 | -0.0192     |
|    n_updates            | 8370        |
|    policy_gradient_loss | -0.00402    |
|    std                  | 0.569       |
|    value_loss           | 0.0167      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1716000, episode_reward=2.90 +/- 2.89
Episode length: 205.20 +/- 77.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 205      |
|    mean_reward     | 2.9      |
| time/              |          |
|    total_timesteps | 1716000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 633     |
|    iterations      | 838     |
|    time_elapsed    | 2708    |
|    total_timesteps | 1716224 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1717000, episode_reward=3.01 +/- 2.89
Episode length: 227.60 +/- 62.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 228          |
|    mean_reward          | 3.01         |
| time/                   |              |
|    total_timesteps      | 1717000      |
| train/                  |              |
|    approx_kl            | 0.0038420963 |
|    clip_fraction        | 0.0275       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.71        |
|    explained_variance   | 0.898        |
|    learning_rate        | 5.06e-05     |
|    loss                 | 0.0222       |
|    n_updates            | 8380         |
|    policy_gradient_loss | -0.00285     |
|    std                  | 0.569        |
|    value_loss           | 0.0733       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=1718000, episode_reward=0.86 +/- 2.34
Episode length: 276.60 +/- 46.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 277      |
|    mean_reward     | 0.861    |
| time/              |          |
|    total_timesteps | 1718000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 633     |
|    iterations      | 839     |
|    time_elapsed    | 2711    |
|    total_timesteps | 1718272 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1719000, episode_reward=2.73 +/- 2.98
Episode length: 239.60 +/- 50.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 240         |
|    mean_reward          | 2.73        |
| time/                   |             |
|    total_timesteps      | 1719000     |
| train/                  |             |
|    approx_kl            | 0.003906547 |
|    clip_fraction        | 0.0269      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | 0.958       |
|    learning_rate        | 5.06e-05    |
|    loss                 | -0.0105     |
|    n_updates            | 8390        |
|    policy_gradient_loss | -0.00447    |
|    std                  | 0.568       |
|    value_loss           | 0.0262      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1720000, episode_reward=4.19 +/- 2.60
Episode length: 225.80 +/- 39.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 226      |
|    mean_reward     | 4.19     |
| time/              |          |
|    total_timesteps | 1720000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 633     |
|    iterations      | 840     |
|    time_elapsed    | 2714    |
|    total_timesteps | 1720320 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1721000, episode_reward=3.14 +/- 2.81
Episode length: 238.80 +/- 56.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 239          |
|    mean_reward          | 3.14         |
| time/                   |              |
|    total_timesteps      | 1721000      |
| train/                  |              |
|    approx_kl            | 0.0039292127 |
|    clip_fraction        | 0.0387       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.7         |
|    explained_variance   | 0.927        |
|    learning_rate        | 5.06e-05     |
|    loss                 | -0.00518     |
|    n_updates            | 8400         |
|    policy_gradient_loss | -0.00458     |
|    std                  | 0.567        |
|    value_loss           | 0.0442       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1722000, episode_reward=4.26 +/- 2.04
Episode length: 198.80 +/- 55.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 199      |
|    mean_reward     | 4.26     |
| time/              |          |
|    total_timesteps | 1722000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 633     |
|    iterations      | 841     |
|    time_elapsed    | 2717    |
|    total_timesteps | 1722368 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1723000, episode_reward=3.90 +/- 2.72
Episode length: 204.40 +/- 50.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 204         |
|    mean_reward          | 3.9         |
| time/                   |             |
|    total_timesteps      | 1723000     |
| train/                  |             |
|    approx_kl            | 0.004757664 |
|    clip_fraction        | 0.0412      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | 0.814       |
|    learning_rate        | 5.07e-05    |
|    loss                 | -0.00352    |
|    n_updates            | 8410        |
|    policy_gradient_loss | -0.00675    |
|    std                  | 0.566       |
|    value_loss           | 0.0943      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1724000, episode_reward=2.74 +/- 3.05
Episode length: 217.00 +/- 67.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 217      |
|    mean_reward     | 2.74     |
| time/              |          |
|    total_timesteps | 1724000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 633     |
|    iterations      | 842     |
|    time_elapsed    | 2719    |
|    total_timesteps | 1724416 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1725000, episode_reward=5.21 +/- 0.07
Episode length: 151.80 +/- 11.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 152        |
|    mean_reward          | 5.21       |
| time/                   |            |
|    total_timesteps      | 1725000    |
| train/                  |            |
|    approx_kl            | 0.00613133 |
|    clip_fraction        | 0.0548     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.7       |
|    explained_variance   | 0.945      |
|    learning_rate        | 5.07e-05   |
|    loss                 | 0.00416    |
|    n_updates            | 8420       |
|    policy_gradient_loss | -0.00711   |
|    std                  | 0.566      |
|    value_loss           | 0.0208     |
----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1726000, episode_reward=2.82 +/- 3.00
Episode length: 219.00 +/- 67.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 219      |
|    mean_reward     | 2.82     |
| time/              |          |
|    total_timesteps | 1726000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 634     |
|    iterations      | 843     |
|    time_elapsed    | 2722    |
|    total_timesteps | 1726464 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1727000, episode_reward=1.46 +/- 3.04
Episode length: 242.60 +/- 70.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 243         |
|    mean_reward          | 1.46        |
| time/                   |             |
|    total_timesteps      | 1727000     |
| train/                  |             |
|    approx_kl            | 0.005770848 |
|    clip_fraction        | 0.0505      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | 0.939       |
|    learning_rate        | 5.07e-05    |
|    loss                 | 0.0105      |
|    n_updates            | 8430        |
|    policy_gradient_loss | -0.00678    |
|    std                  | 0.565       |
|    value_loss           | 0.0263      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=1728000, episode_reward=0.90 +/- 2.31
Episode length: 272.80 +/- 54.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 273      |
|    mean_reward     | 0.895    |
| time/              |          |
|    total_timesteps | 1728000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 634     |
|    iterations      | 844     |
|    time_elapsed    | 2725    |
|    total_timesteps | 1728512 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1729000, episode_reward=2.74 +/- 3.04
Episode length: 217.60 +/- 72.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 218          |
|    mean_reward          | 2.74         |
| time/                   |              |
|    total_timesteps      | 1729000      |
| train/                  |              |
|    approx_kl            | 0.0059570363 |
|    clip_fraction        | 0.0688       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.69        |
|    explained_variance   | 0.954        |
|    learning_rate        | 5.07e-05     |
|    loss                 | -0.0139      |
|    n_updates            | 8440         |
|    policy_gradient_loss | -0.0104      |
|    std                  | 0.565        |
|    value_loss           | 0.0236       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1730000, episode_reward=2.58 +/- 3.23
Episode length: 224.20 +/- 62.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 224      |
|    mean_reward     | 2.58     |
| time/              |          |
|    total_timesteps | 1730000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 634     |
|    iterations      | 845     |
|    time_elapsed    | 2728    |
|    total_timesteps | 1730560 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1731000, episode_reward=2.29 +/- 2.60
Episode length: 254.80 +/- 58.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 255         |
|    mean_reward          | 2.29        |
| time/                   |             |
|    total_timesteps      | 1731000     |
| train/                  |             |
|    approx_kl            | 0.004672729 |
|    clip_fraction        | 0.027       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.69       |
|    explained_variance   | 0.954       |
|    learning_rate        | 5.08e-05    |
|    loss                 | 0.00536     |
|    n_updates            | 8450        |
|    policy_gradient_loss | -0.0056     |
|    std                  | 0.563       |
|    value_loss           | 0.0267      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1732000, episode_reward=4.29 +/- 2.05
Episode length: 195.40 +/- 53.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 195      |
|    mean_reward     | 4.29     |
| time/              |          |
|    total_timesteps | 1732000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 634     |
|    iterations      | 846     |
|    time_elapsed    | 2731    |
|    total_timesteps | 1732608 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1733000, episode_reward=2.19 +/- 2.50
Episode length: 251.00 +/- 60.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 251          |
|    mean_reward          | 2.19         |
| time/                   |              |
|    total_timesteps      | 1733000      |
| train/                  |              |
|    approx_kl            | 0.0048758797 |
|    clip_fraction        | 0.0303       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.69        |
|    explained_variance   | 0.792        |
|    learning_rate        | 5.08e-05     |
|    loss                 | 0.019        |
|    n_updates            | 8460         |
|    policy_gradient_loss | -0.00514     |
|    std                  | 0.564        |
|    value_loss           | 0.0733       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1734000, episode_reward=1.81 +/- 2.83
Episode length: 247.20 +/- 66.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 247      |
|    mean_reward     | 1.81     |
| time/              |          |
|    total_timesteps | 1734000  |
---------------------------------
box reached target
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 634     |
|    iterations      | 847     |
|    time_elapsed    | 2735    |
|    total_timesteps | 1734656 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1735000, episode_reward=2.75 +/- 3.06
Episode length: 217.80 +/- 67.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 218          |
|    mean_reward          | 2.75         |
| time/                   |              |
|    total_timesteps      | 1735000      |
| train/                  |              |
|    approx_kl            | 0.0045681857 |
|    clip_fraction        | 0.0226       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.69        |
|    explained_variance   | 0.911        |
|    learning_rate        | 5.08e-05     |
|    loss                 | 0.0338       |
|    n_updates            | 8470         |
|    policy_gradient_loss | -0.00432     |
|    std                  | 0.562        |
|    value_loss           | 0.0736       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1736000, episode_reward=4.32 +/- 1.84
Episode length: 203.80 +/- 53.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 204      |
|    mean_reward     | 4.32     |
| time/              |          |
|    total_timesteps | 1736000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 634     |
|    iterations      | 848     |
|    time_elapsed    | 2738    |
|    total_timesteps | 1736704 |
--------------------------------
box reached target
Eval num_timesteps=1737000, episode_reward=0.79 +/- 2.32
Episode length: 276.80 +/- 46.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 277         |
|    mean_reward          | 0.794       |
| time/                   |             |
|    total_timesteps      | 1737000     |
| train/                  |             |
|    approx_kl            | 0.007434834 |
|    clip_fraction        | 0.0675      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.68       |
|    explained_variance   | 0.92        |
|    learning_rate        | 5.08e-05    |
|    loss                 | -0.00303    |
|    n_updates            | 8480        |
|    policy_gradient_loss | -0.00914    |
|    std                  | 0.563       |
|    value_loss           | 0.0506      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1738000, episode_reward=0.80 +/- 2.51
Episode length: 279.60 +/- 40.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 280      |
|    mean_reward     | 0.797    |
| time/              |          |
|    total_timesteps | 1738000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 634     |
|    iterations      | 849     |
|    time_elapsed    | 2741    |
|    total_timesteps | 1738752 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1739000, episode_reward=1.67 +/- 2.93
Episode length: 258.80 +/- 50.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 259          |
|    mean_reward          | 1.67         |
| time/                   |              |
|    total_timesteps      | 1739000      |
| train/                  |              |
|    approx_kl            | 0.0062505854 |
|    clip_fraction        | 0.07         |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.69        |
|    explained_variance   | 0.891        |
|    learning_rate        | 5.09e-05     |
|    loss                 | 0.0281       |
|    n_updates            | 8490         |
|    policy_gradient_loss | -0.0101      |
|    std                  | 0.566        |
|    value_loss           | 0.0517       |
------------------------------------------
box reached target
Eval num_timesteps=1740000, episode_reward=0.53 +/- 2.31
Episode length: 281.20 +/- 37.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 281      |
|    mean_reward     | 0.535    |
| time/              |          |
|    total_timesteps | 1740000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 634     |
|    iterations      | 850     |
|    time_elapsed    | 2745    |
|    total_timesteps | 1740800 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1741000, episode_reward=1.83 +/- 3.13
Episode length: 257.40 +/- 52.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 257          |
|    mean_reward          | 1.83         |
| time/                   |              |
|    total_timesteps      | 1741000      |
| train/                  |              |
|    approx_kl            | 0.0063621732 |
|    clip_fraction        | 0.0496       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.7         |
|    explained_variance   | 0.871        |
|    learning_rate        | 5.09e-05     |
|    loss                 | -0.00878     |
|    n_updates            | 8500         |
|    policy_gradient_loss | -0.00793     |
|    std                  | 0.568        |
|    value_loss           | 0.0293       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1742000, episode_reward=3.97 +/- 2.63
Episode length: 193.60 +/- 54.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 194      |
|    mean_reward     | 3.97     |
| time/              |          |
|    total_timesteps | 1742000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 634     |
|    iterations      | 851     |
|    time_elapsed    | 2747    |
|    total_timesteps | 1742848 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1743000, episode_reward=2.34 +/- 2.52
Episode length: 256.80 +/- 55.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 257          |
|    mean_reward          | 2.34         |
| time/                   |              |
|    total_timesteps      | 1743000      |
| train/                  |              |
|    approx_kl            | 0.0051282686 |
|    clip_fraction        | 0.0563       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.71        |
|    explained_variance   | 0.908        |
|    learning_rate        | 5.09e-05     |
|    loss                 | 0.0389       |
|    n_updates            | 8510         |
|    policy_gradient_loss | -0.00722     |
|    std                  | 0.57         |
|    value_loss           | 0.0483       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1744000, episode_reward=2.63 +/- 3.15
Episode length: 212.40 +/- 72.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 212      |
|    mean_reward     | 2.63     |
| time/              |          |
|    total_timesteps | 1744000  |
---------------------------------
box reached target
box reached target
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 634     |
|    iterations      | 852     |
|    time_elapsed    | 2750    |
|    total_timesteps | 1744896 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1745000, episode_reward=2.21 +/- 2.48
Episode length: 244.40 +/- 69.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 244          |
|    mean_reward          | 2.21         |
| time/                   |              |
|    total_timesteps      | 1745000      |
| train/                  |              |
|    approx_kl            | 0.0052014226 |
|    clip_fraction        | 0.0312       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.71        |
|    explained_variance   | 0.876        |
|    learning_rate        | 5.09e-05     |
|    loss                 | 0.0196       |
|    n_updates            | 8520         |
|    policy_gradient_loss | -0.00678     |
|    std                  | 0.57         |
|    value_loss           | 0.081        |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1746000, episode_reward=4.03 +/- 2.52
Episode length: 216.60 +/- 65.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 217      |
|    mean_reward     | 4.03     |
| time/              |          |
|    total_timesteps | 1746000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 634     |
|    iterations      | 853     |
|    time_elapsed    | 2753    |
|    total_timesteps | 1746944 |
--------------------------------
Eval num_timesteps=1747000, episode_reward=-0.79 +/- 0.63
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.79        |
| time/                   |              |
|    total_timesteps      | 1747000      |
| train/                  |              |
|    approx_kl            | 0.0061689676 |
|    clip_fraction        | 0.052        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.71        |
|    explained_variance   | 0.91         |
|    learning_rate        | 5.1e-05      |
|    loss                 | 0.00395      |
|    n_updates            | 8530         |
|    policy_gradient_loss | -0.00646     |
|    std                  | 0.571        |
|    value_loss           | 0.0457       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=1748000, episode_reward=0.47 +/- 2.49
Episode length: 268.40 +/- 63.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 268      |
|    mean_reward     | 0.471    |
| time/              |          |
|    total_timesteps | 1748000  |
---------------------------------
box reached target
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 634     |
|    iterations      | 854     |
|    time_elapsed    | 2757    |
|    total_timesteps | 1748992 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1749000, episode_reward=4.07 +/- 2.20
Episode length: 189.00 +/- 59.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 189         |
|    mean_reward          | 4.07        |
| time/                   |             |
|    total_timesteps      | 1749000     |
| train/                  |             |
|    approx_kl            | 0.005241739 |
|    clip_fraction        | 0.042       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | 0.943       |
|    learning_rate        | 5.1e-05     |
|    loss                 | 0.0307      |
|    n_updates            | 8540        |
|    policy_gradient_loss | -0.00703    |
|    std                  | 0.569       |
|    value_loss           | 0.0295      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1750000, episode_reward=3.28 +/- 2.34
Episode length: 223.00 +/- 64.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 223      |
|    mean_reward     | 3.28     |
| time/              |          |
|    total_timesteps | 1750000  |
---------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1751000, episode_reward=2.72 +/- 3.03
Episode length: 220.80 +/- 65.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 221      |
|    mean_reward     | 2.72     |
| time/              |          |
|    total_timesteps | 1751000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 634     |
|    iterations      | 855     |
|    time_elapsed    | 2760    |
|    total_timesteps | 1751040 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1752000, episode_reward=-0.76 +/- 0.67
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.765      |
| time/                   |             |
|    total_timesteps      | 1752000     |
| train/                  |             |
|    approx_kl            | 0.006792912 |
|    clip_fraction        | 0.0543      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | 0.948       |
|    learning_rate        | 5.1e-05     |
|    loss                 | 0.00253     |
|    n_updates            | 8550        |
|    policy_gradient_loss | -0.00633    |
|    std                  | 0.569       |
|    value_loss           | 0.0272      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1753000, episode_reward=0.93 +/- 2.23
Episode length: 280.00 +/- 40.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 280      |
|    mean_reward     | 0.925    |
| time/              |          |
|    total_timesteps | 1753000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 634     |
|    iterations      | 856     |
|    time_elapsed    | 2763    |
|    total_timesteps | 1753088 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1754000, episode_reward=3.34 +/- 2.25
Episode length: 220.60 +/- 68.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 221         |
|    mean_reward          | 3.34        |
| time/                   |             |
|    total_timesteps      | 1754000     |
| train/                  |             |
|    approx_kl            | 0.003088075 |
|    clip_fraction        | 0.00903     |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | 0.876       |
|    learning_rate        | 5.1e-05     |
|    loss                 | 0.00626     |
|    n_updates            | 8560        |
|    policy_gradient_loss | -0.00217    |
|    std                  | 0.571       |
|    value_loss           | 0.05        |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1755000, episode_reward=1.57 +/- 2.96
Episode length: 256.80 +/- 52.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 257      |
|    mean_reward     | 1.57     |
| time/              |          |
|    total_timesteps | 1755000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 634     |
|    iterations      | 857     |
|    time_elapsed    | 2766    |
|    total_timesteps | 1755136 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1756000, episode_reward=3.43 +/- 2.34
Episode length: 231.80 +/- 60.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 232         |
|    mean_reward          | 3.43        |
| time/                   |             |
|    total_timesteps      | 1756000     |
| train/                  |             |
|    approx_kl            | 0.008460436 |
|    clip_fraction        | 0.074       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | 0.919       |
|    learning_rate        | 5.11e-05    |
|    loss                 | 0.0153      |
|    n_updates            | 8570        |
|    policy_gradient_loss | -0.00611    |
|    std                  | 0.569       |
|    value_loss           | 0.0392      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1757000, episode_reward=3.33 +/- 2.44
Episode length: 219.00 +/- 68.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 219      |
|    mean_reward     | 3.33     |
| time/              |          |
|    total_timesteps | 1757000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 634     |
|    iterations      | 858     |
|    time_elapsed    | 2769    |
|    total_timesteps | 1757184 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1758000, episode_reward=5.30 +/- 0.20
Episode length: 175.00 +/- 27.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 175          |
|    mean_reward          | 5.3          |
| time/                   |              |
|    total_timesteps      | 1758000      |
| train/                  |              |
|    approx_kl            | 0.0049288026 |
|    clip_fraction        | 0.0324       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.7         |
|    explained_variance   | 0.873        |
|    learning_rate        | 5.11e-05     |
|    loss                 | 0.00231      |
|    n_updates            | 8580         |
|    policy_gradient_loss | -0.00543     |
|    std                  | 0.567        |
|    value_loss           | 0.0323       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1759000, episode_reward=2.59 +/- 3.12
Episode length: 217.60 +/- 68.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 218      |
|    mean_reward     | 2.59     |
| time/              |          |
|    total_timesteps | 1759000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 634     |
|    iterations      | 859     |
|    time_elapsed    | 2772    |
|    total_timesteps | 1759232 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1760000, episode_reward=4.05 +/- 2.53
Episode length: 207.00 +/- 51.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 207          |
|    mean_reward          | 4.05         |
| time/                   |              |
|    total_timesteps      | 1760000      |
| train/                  |              |
|    approx_kl            | 0.0055179712 |
|    clip_fraction        | 0.0471       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.7         |
|    explained_variance   | 0.823        |
|    learning_rate        | 5.11e-05     |
|    loss                 | 0.000881     |
|    n_updates            | 8590         |
|    policy_gradient_loss | -0.0061      |
|    std                  | 0.567        |
|    value_loss           | 0.0591       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1761000, episode_reward=1.79 +/- 2.86
Episode length: 254.80 +/- 55.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 255      |
|    mean_reward     | 1.79     |
| time/              |          |
|    total_timesteps | 1761000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 634     |
|    iterations      | 860     |
|    time_elapsed    | 2775    |
|    total_timesteps | 1761280 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1762000, episode_reward=1.64 +/- 2.91
Episode length: 244.40 +/- 68.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 244         |
|    mean_reward          | 1.64        |
| time/                   |             |
|    total_timesteps      | 1762000     |
| train/                  |             |
|    approx_kl            | 0.003228819 |
|    clip_fraction        | 0.0248      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | 0.927       |
|    learning_rate        | 5.11e-05    |
|    loss                 | 0.0242      |
|    n_updates            | 8600        |
|    policy_gradient_loss | -0.00279    |
|    std                  | 0.565       |
|    value_loss           | 0.0236      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=1763000, episode_reward=1.67 +/- 2.85
Episode length: 245.40 +/- 67.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 245      |
|    mean_reward     | 1.67     |
| time/              |          |
|    total_timesteps | 1763000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 634     |
|    iterations      | 861     |
|    time_elapsed    | 2778    |
|    total_timesteps | 1763328 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1764000, episode_reward=0.15 +/- 2.49
Episode length: 287.40 +/- 25.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 287         |
|    mean_reward          | 0.148       |
| time/                   |             |
|    total_timesteps      | 1764000     |
| train/                  |             |
|    approx_kl            | 0.006094465 |
|    clip_fraction        | 0.0422      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.69       |
|    explained_variance   | 0.951       |
|    learning_rate        | 5.12e-05    |
|    loss                 | -0.00569    |
|    n_updates            | 8610        |
|    policy_gradient_loss | -0.00394    |
|    std                  | 0.565       |
|    value_loss           | 0.00838     |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1765000, episode_reward=2.22 +/- 2.51
Episode length: 254.80 +/- 57.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 255      |
|    mean_reward     | 2.22     |
| time/              |          |
|    total_timesteps | 1765000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 634     |
|    iterations      | 862     |
|    time_elapsed    | 2781    |
|    total_timesteps | 1765376 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1766000, episode_reward=0.67 +/- 2.35
Episode length: 268.00 +/- 64.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 268          |
|    mean_reward          | 0.673        |
| time/                   |              |
|    total_timesteps      | 1766000      |
| train/                  |              |
|    approx_kl            | 0.0053255786 |
|    clip_fraction        | 0.0405       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.69        |
|    explained_variance   | 0.937        |
|    learning_rate        | 5.12e-05     |
|    loss                 | -0.0214      |
|    n_updates            | 8620         |
|    policy_gradient_loss | -0.00641     |
|    std                  | 0.562        |
|    value_loss           | 0.0286       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1767000, episode_reward=0.68 +/- 2.36
Episode length: 273.20 +/- 53.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 273      |
|    mean_reward     | 0.68     |
| time/              |          |
|    total_timesteps | 1767000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 634     |
|    iterations      | 863     |
|    time_elapsed    | 2784    |
|    total_timesteps | 1767424 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1768000, episode_reward=0.26 +/- 2.38
Episode length: 270.80 +/- 58.40
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 271        |
|    mean_reward          | 0.262      |
| time/                   |            |
|    total_timesteps      | 1768000    |
| train/                  |            |
|    approx_kl            | 0.00441006 |
|    clip_fraction        | 0.0354     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.68      |
|    explained_variance   | 0.938      |
|    learning_rate        | 5.12e-05   |
|    loss                 | 0.00505    |
|    n_updates            | 8630       |
|    policy_gradient_loss | -0.00793   |
|    std                  | 0.561      |
|    value_loss           | 0.0389     |
----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1769000, episode_reward=2.00 +/- 2.77
Episode length: 254.40 +/- 58.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 254      |
|    mean_reward     | 2        |
| time/              |          |
|    total_timesteps | 1769000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 634     |
|    iterations      | 864     |
|    time_elapsed    | 2787    |
|    total_timesteps | 1769472 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1770000, episode_reward=1.96 +/- 2.81
Episode length: 248.60 +/- 62.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 249          |
|    mean_reward          | 1.96         |
| time/                   |              |
|    total_timesteps      | 1770000      |
| train/                  |              |
|    approx_kl            | 0.0077617983 |
|    clip_fraction        | 0.0731       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.68        |
|    explained_variance   | 0.976        |
|    learning_rate        | 5.12e-05     |
|    loss                 | 0.000215     |
|    n_updates            | 8640         |
|    policy_gradient_loss | -0.0104      |
|    std                  | 0.56         |
|    value_loss           | 0.0137       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1771000, episode_reward=2.63 +/- 3.12
Episode length: 221.60 +/- 64.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 222      |
|    mean_reward     | 2.63     |
| time/              |          |
|    total_timesteps | 1771000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 634     |
|    iterations      | 865     |
|    time_elapsed    | 2790    |
|    total_timesteps | 1771520 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1772000, episode_reward=1.85 +/- 2.90
Episode length: 252.40 +/- 59.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 252          |
|    mean_reward          | 1.85         |
| time/                   |              |
|    total_timesteps      | 1772000      |
| train/                  |              |
|    approx_kl            | 0.0062181996 |
|    clip_fraction        | 0.056        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.68        |
|    explained_variance   | 0.951        |
|    learning_rate        | 5.13e-05     |
|    loss                 | -0.000852    |
|    n_updates            | 8650         |
|    policy_gradient_loss | -0.00627     |
|    std                  | 0.56         |
|    value_loss           | 0.0161       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1773000, episode_reward=1.36 +/- 3.13
Episode length: 246.40 +/- 65.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 246      |
|    mean_reward     | 1.36     |
| time/              |          |
|    total_timesteps | 1773000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 634     |
|    iterations      | 866     |
|    time_elapsed    | 2793    |
|    total_timesteps | 1773568 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1774000, episode_reward=4.00 +/- 2.66
Episode length: 210.40 +/- 52.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 210         |
|    mean_reward          | 4           |
| time/                   |             |
|    total_timesteps      | 1774000     |
| train/                  |             |
|    approx_kl            | 0.006877986 |
|    clip_fraction        | 0.0802      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.68       |
|    explained_variance   | 0.918       |
|    learning_rate        | 5.13e-05    |
|    loss                 | -0.00591    |
|    n_updates            | 8660        |
|    policy_gradient_loss | -0.0103     |
|    std                  | 0.561       |
|    value_loss           | 0.0237      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1775000, episode_reward=5.24 +/- 0.12
Episode length: 177.80 +/- 7.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 178      |
|    mean_reward     | 5.24     |
| time/              |          |
|    total_timesteps | 1775000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 635     |
|    iterations      | 867     |
|    time_elapsed    | 2796    |
|    total_timesteps | 1775616 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1776000, episode_reward=2.99 +/- 2.86
Episode length: 224.00 +/- 65.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 224         |
|    mean_reward          | 2.99        |
| time/                   |             |
|    total_timesteps      | 1776000     |
| train/                  |             |
|    approx_kl            | 0.004674535 |
|    clip_fraction        | 0.0455      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.68       |
|    explained_variance   | 0.796       |
|    learning_rate        | 5.13e-05    |
|    loss                 | 0.0781      |
|    n_updates            | 8670        |
|    policy_gradient_loss | -0.0057     |
|    std                  | 0.561       |
|    value_loss           | 0.0687      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=1777000, episode_reward=-1.02 +/- 0.16
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -1.02    |
| time/              |          |
|    total_timesteps | 1777000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 635     |
|    iterations      | 868     |
|    time_elapsed    | 2799    |
|    total_timesteps | 1777664 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1778000, episode_reward=0.96 +/- 2.21
Episode length: 274.80 +/- 50.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 275         |
|    mean_reward          | 0.958       |
| time/                   |             |
|    total_timesteps      | 1778000     |
| train/                  |             |
|    approx_kl            | 0.004292558 |
|    clip_fraction        | 0.0297      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.68       |
|    explained_variance   | 0.917       |
|    learning_rate        | 5.13e-05    |
|    loss                 | 0.0082      |
|    n_updates            | 8680        |
|    policy_gradient_loss | -0.00492    |
|    std                  | 0.56        |
|    value_loss           | 0.015       |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1779000, episode_reward=3.11 +/- 2.97
Episode length: 242.20 +/- 49.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 242      |
|    mean_reward     | 3.11     |
| time/              |          |
|    total_timesteps | 1779000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 635     |
|    iterations      | 869     |
|    time_elapsed    | 2802    |
|    total_timesteps | 1779712 |
--------------------------------
box reached target
Eval num_timesteps=1780000, episode_reward=0.75 +/- 2.27
Episode length: 279.40 +/- 41.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 279          |
|    mean_reward          | 0.752        |
| time/                   |              |
|    total_timesteps      | 1780000      |
| train/                  |              |
|    approx_kl            | 0.0041357614 |
|    clip_fraction        | 0.0346       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.68        |
|    explained_variance   | 0.942        |
|    learning_rate        | 5.14e-05     |
|    loss                 | 0.00784      |
|    n_updates            | 8690         |
|    policy_gradient_loss | -0.00477     |
|    std                  | 0.559        |
|    value_loss           | 0.0396       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1781000, episode_reward=0.20 +/- 2.48
Episode length: 274.40 +/- 51.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 274      |
|    mean_reward     | 0.197    |
| time/              |          |
|    total_timesteps | 1781000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 635     |
|    iterations      | 870     |
|    time_elapsed    | 2805    |
|    total_timesteps | 1781760 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1782000, episode_reward=1.79 +/- 2.88
Episode length: 259.40 +/- 50.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 259         |
|    mean_reward          | 1.79        |
| time/                   |             |
|    total_timesteps      | 1782000     |
| train/                  |             |
|    approx_kl            | 0.005596567 |
|    clip_fraction        | 0.0432      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.67       |
|    explained_variance   | 0.907       |
|    learning_rate        | 5.14e-05    |
|    loss                 | 0.000466    |
|    n_updates            | 8700        |
|    policy_gradient_loss | -0.00569    |
|    std                  | 0.559       |
|    value_loss           | 0.0243      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1783000, episode_reward=0.02 +/- 2.62
Episode length: 270.80 +/- 58.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 271      |
|    mean_reward     | 0.0161   |
| time/              |          |
|    total_timesteps | 1783000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 635     |
|    iterations      | 871     |
|    time_elapsed    | 2808    |
|    total_timesteps | 1783808 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1784000, episode_reward=3.96 +/- 2.69
Episode length: 197.80 +/- 54.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 198          |
|    mean_reward          | 3.96         |
| time/                   |              |
|    total_timesteps      | 1784000      |
| train/                  |              |
|    approx_kl            | 0.0036150902 |
|    clip_fraction        | 0.0218       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.67        |
|    explained_variance   | 0.869        |
|    learning_rate        | 5.14e-05     |
|    loss                 | 0.0165       |
|    n_updates            | 8710         |
|    policy_gradient_loss | -0.00322     |
|    std                  | 0.558        |
|    value_loss           | 0.0611       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1785000, episode_reward=2.88 +/- 3.17
Episode length: 254.40 +/- 60.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 254      |
|    mean_reward     | 2.88     |
| time/              |          |
|    total_timesteps | 1785000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 635     |
|    iterations      | 872     |
|    time_elapsed    | 2811    |
|    total_timesteps | 1785856 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1786000, episode_reward=2.05 +/- 2.65
Episode length: 253.20 +/- 59.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 253          |
|    mean_reward          | 2.05         |
| time/                   |              |
|    total_timesteps      | 1786000      |
| train/                  |              |
|    approx_kl            | 0.0063733673 |
|    clip_fraction        | 0.0463       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.67        |
|    explained_variance   | 0.948        |
|    learning_rate        | 5.14e-05     |
|    loss                 | -0.00019     |
|    n_updates            | 8720         |
|    policy_gradient_loss | -0.0062      |
|    std                  | 0.557        |
|    value_loss           | 0.011        |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1787000, episode_reward=3.13 +/- 2.76
Episode length: 219.40 +/- 67.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 219      |
|    mean_reward     | 3.13     |
| time/              |          |
|    total_timesteps | 1787000  |
---------------------------------
box reached target
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 635     |
|    iterations      | 873     |
|    time_elapsed    | 2814    |
|    total_timesteps | 1787904 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1788000, episode_reward=1.47 +/- 3.20
Episode length: 263.60 +/- 48.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 264         |
|    mean_reward          | 1.47        |
| time/                   |             |
|    total_timesteps      | 1788000     |
| train/                  |             |
|    approx_kl            | 0.005251851 |
|    clip_fraction        | 0.0354      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.66       |
|    explained_variance   | 0.919       |
|    learning_rate        | 5.15e-05    |
|    loss                 | 0.0477      |
|    n_updates            | 8730        |
|    policy_gradient_loss | -0.00411    |
|    std                  | 0.556       |
|    value_loss           | 0.0384      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=1789000, episode_reward=0.09 +/- 2.52
Episode length: 270.60 +/- 58.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 271      |
|    mean_reward     | 0.0916   |
| time/              |          |
|    total_timesteps | 1789000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 635     |
|    iterations      | 874     |
|    time_elapsed    | 2817    |
|    total_timesteps | 1789952 |
--------------------------------
box reached target
Eval num_timesteps=1790000, episode_reward=0.54 +/- 2.43
Episode length: 280.60 +/- 38.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 281          |
|    mean_reward          | 0.544        |
| time/                   |              |
|    total_timesteps      | 1790000      |
| train/                  |              |
|    approx_kl            | 0.0055065695 |
|    clip_fraction        | 0.0385       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.67        |
|    explained_variance   | 0.956        |
|    learning_rate        | 5.15e-05     |
|    loss                 | -0.0302      |
|    n_updates            | 8740         |
|    policy_gradient_loss | -0.00779     |
|    std                  | 0.557        |
|    value_loss           | 0.00989      |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1791000, episode_reward=3.37 +/- 2.39
Episode length: 233.00 +/- 56.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 233      |
|    mean_reward     | 3.37     |
| time/              |          |
|    total_timesteps | 1791000  |
---------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1792000, episode_reward=1.53 +/- 3.04
Episode length: 250.20 +/- 63.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 250      |
|    mean_reward     | 1.53     |
| time/              |          |
|    total_timesteps | 1792000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 635     |
|    iterations      | 875     |
|    time_elapsed    | 2820    |
|    total_timesteps | 1792000 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1793000, episode_reward=1.54 +/- 2.97
Episode length: 245.80 +/- 66.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 246         |
|    mean_reward          | 1.54        |
| time/                   |             |
|    total_timesteps      | 1793000     |
| train/                  |             |
|    approx_kl            | 0.003658898 |
|    clip_fraction        | 0.032       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.67       |
|    explained_variance   | 0.858       |
|    learning_rate        | 5.15e-05    |
|    loss                 | -0.0157     |
|    n_updates            | 8750        |
|    policy_gradient_loss | -0.00635    |
|    std                  | 0.557       |
|    value_loss           | 0.0564      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1794000, episode_reward=5.26 +/- 0.10
Episode length: 184.40 +/- 5.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 184      |
|    mean_reward     | 5.26     |
| time/              |          |
|    total_timesteps | 1794000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 635     |
|    iterations      | 876     |
|    time_elapsed    | 2823    |
|    total_timesteps | 1794048 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1795000, episode_reward=0.70 +/- 2.34
Episode length: 268.00 +/- 64.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 268          |
|    mean_reward          | 0.695        |
| time/                   |              |
|    total_timesteps      | 1795000      |
| train/                  |              |
|    approx_kl            | 0.0051512807 |
|    clip_fraction        | 0.0397       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.66        |
|    explained_variance   | 0.86         |
|    learning_rate        | 5.15e-05     |
|    loss                 | 0.0151       |
|    n_updates            | 8760         |
|    policy_gradient_loss | -0.00513     |
|    std                  | 0.556        |
|    value_loss           | 0.0423       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1796000, episode_reward=1.82 +/- 3.01
Episode length: 252.40 +/- 58.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 252      |
|    mean_reward     | 1.82     |
| time/              |          |
|    total_timesteps | 1796000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 635     |
|    iterations      | 877     |
|    time_elapsed    | 2826    |
|    total_timesteps | 1796096 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1797000, episode_reward=2.65 +/- 3.12
Episode length: 227.80 +/- 59.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 228          |
|    mean_reward          | 2.65         |
| time/                   |              |
|    total_timesteps      | 1797000      |
| train/                  |              |
|    approx_kl            | 0.0033844463 |
|    clip_fraction        | 0.0218       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.66        |
|    explained_variance   | 0.892        |
|    learning_rate        | 5.16e-05     |
|    loss                 | 0.0156       |
|    n_updates            | 8770         |
|    policy_gradient_loss | -0.0032      |
|    std                  | 0.556        |
|    value_loss           | 0.0395       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1798000, episode_reward=0.33 +/- 2.56
Episode length: 282.20 +/- 35.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 282      |
|    mean_reward     | 0.329    |
| time/              |          |
|    total_timesteps | 1798000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 635     |
|    iterations      | 878     |
|    time_elapsed    | 2829    |
|    total_timesteps | 1798144 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1799000, episode_reward=0.53 +/- 2.39
Episode length: 274.20 +/- 51.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 274          |
|    mean_reward          | 0.531        |
| time/                   |              |
|    total_timesteps      | 1799000      |
| train/                  |              |
|    approx_kl            | 0.0054215062 |
|    clip_fraction        | 0.0416       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.66        |
|    explained_variance   | 0.88         |
|    learning_rate        | 5.16e-05     |
|    loss                 | 0.00682      |
|    n_updates            | 8780         |
|    policy_gradient_loss | -0.00647     |
|    std                  | 0.556        |
|    value_loss           | 0.0342       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1800000, episode_reward=1.74 +/- 2.91
Episode length: 251.20 +/- 60.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | 1.74     |
| time/              |          |
|    total_timesteps | 1800000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 635     |
|    iterations      | 879     |
|    time_elapsed    | 2833    |
|    total_timesteps | 1800192 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1801000, episode_reward=4.02 +/- 2.51
Episode length: 183.40 +/- 58.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 183         |
|    mean_reward          | 4.02        |
| time/                   |             |
|    total_timesteps      | 1801000     |
| train/                  |             |
|    approx_kl            | 0.004985097 |
|    clip_fraction        | 0.0345      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.66       |
|    explained_variance   | 0.964       |
|    learning_rate        | 5.16e-05    |
|    loss                 | -0.0181     |
|    n_updates            | 8790        |
|    policy_gradient_loss | -0.00594    |
|    std                  | 0.555       |
|    value_loss           | 0.0224      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=1802000, episode_reward=0.40 +/- 2.45
Episode length: 276.60 +/- 46.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 277      |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 1802000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 635     |
|    iterations      | 880     |
|    time_elapsed    | 2835    |
|    total_timesteps | 1802240 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1803000, episode_reward=3.28 +/- 2.39
Episode length: 222.80 +/- 64.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 223          |
|    mean_reward          | 3.28         |
| time/                   |              |
|    total_timesteps      | 1803000      |
| train/                  |              |
|    approx_kl            | 0.0073624845 |
|    clip_fraction        | 0.0509       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.66        |
|    explained_variance   | 0.678        |
|    learning_rate        | 5.16e-05     |
|    loss                 | 0.0835       |
|    n_updates            | 8800         |
|    policy_gradient_loss | -0.00588     |
|    std                  | 0.556        |
|    value_loss           | 0.0944       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1804000, episode_reward=0.47 +/- 2.40
Episode length: 274.80 +/- 50.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 275      |
|    mean_reward     | 0.469    |
| time/              |          |
|    total_timesteps | 1804000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 635     |
|    iterations      | 881     |
|    time_elapsed    | 2838    |
|    total_timesteps | 1804288 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1805000, episode_reward=1.82 +/- 2.80
Episode length: 236.40 +/- 77.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 236          |
|    mean_reward          | 1.82         |
| time/                   |              |
|    total_timesteps      | 1805000      |
| train/                  |              |
|    approx_kl            | 0.0067577534 |
|    clip_fraction        | 0.0503       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.66        |
|    explained_variance   | 0.93         |
|    learning_rate        | 5.17e-05     |
|    loss                 | -0.00706     |
|    n_updates            | 8810         |
|    policy_gradient_loss | -0.00608     |
|    std                  | 0.556        |
|    value_loss           | 0.0221       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1806000, episode_reward=3.95 +/- 2.48
Episode length: 192.00 +/- 56.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 192      |
|    mean_reward     | 3.95     |
| time/              |          |
|    total_timesteps | 1806000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 635     |
|    iterations      | 882     |
|    time_elapsed    | 2841    |
|    total_timesteps | 1806336 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1807000, episode_reward=0.81 +/- 2.31
Episode length: 271.60 +/- 56.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 272          |
|    mean_reward          | 0.807        |
| time/                   |              |
|    total_timesteps      | 1807000      |
| train/                  |              |
|    approx_kl            | 0.0056837318 |
|    clip_fraction        | 0.0497       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.66        |
|    explained_variance   | 0.883        |
|    learning_rate        | 5.17e-05     |
|    loss                 | 0.0515       |
|    n_updates            | 8820         |
|    policy_gradient_loss | -0.00555     |
|    std                  | 0.555        |
|    value_loss           | 0.06         |
------------------------------------------
box reached target
Eval num_timesteps=1808000, episode_reward=0.25 +/- 2.50
Episode length: 270.20 +/- 59.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 270      |
|    mean_reward     | 0.251    |
| time/              |          |
|    total_timesteps | 1808000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 635     |
|    iterations      | 883     |
|    time_elapsed    | 2844    |
|    total_timesteps | 1808384 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1809000, episode_reward=0.79 +/- 2.32
Episode length: 282.80 +/- 34.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 283         |
|    mean_reward          | 0.787       |
| time/                   |             |
|    total_timesteps      | 1809000     |
| train/                  |             |
|    approx_kl            | 0.005528982 |
|    clip_fraction        | 0.0467      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.66       |
|    explained_variance   | 0.919       |
|    learning_rate        | 5.17e-05    |
|    loss                 | -0.00461    |
|    n_updates            | 8830        |
|    policy_gradient_loss | -0.00642    |
|    std                  | 0.554       |
|    value_loss           | 0.0462      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1810000, episode_reward=1.93 +/- 2.71
Episode length: 253.60 +/- 62.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 254      |
|    mean_reward     | 1.93     |
| time/              |          |
|    total_timesteps | 1810000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 635     |
|    iterations      | 884     |
|    time_elapsed    | 2848    |
|    total_timesteps | 1810432 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1811000, episode_reward=1.74 +/- 2.85
Episode length: 242.40 +/- 70.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 242         |
|    mean_reward          | 1.74        |
| time/                   |             |
|    total_timesteps      | 1811000     |
| train/                  |             |
|    approx_kl            | 0.004663457 |
|    clip_fraction        | 0.0449      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.65       |
|    explained_variance   | 0.927       |
|    learning_rate        | 5.17e-05    |
|    loss                 | 0.0126      |
|    n_updates            | 8840        |
|    policy_gradient_loss | -0.00522    |
|    std                  | 0.554       |
|    value_loss           | 0.0297      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1812000, episode_reward=0.75 +/- 2.47
Episode length: 284.60 +/- 30.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 285      |
|    mean_reward     | 0.746    |
| time/              |          |
|    total_timesteps | 1812000  |
---------------------------------
box reached target
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 635     |
|    iterations      | 885     |
|    time_elapsed    | 2851    |
|    total_timesteps | 1812480 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1813000, episode_reward=3.08 +/- 2.76
Episode length: 218.60 +/- 67.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 219         |
|    mean_reward          | 3.08        |
| time/                   |             |
|    total_timesteps      | 1813000     |
| train/                  |             |
|    approx_kl            | 0.004572125 |
|    clip_fraction        | 0.0347      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.65       |
|    explained_variance   | 0.897       |
|    learning_rate        | 5.17e-05    |
|    loss                 | 0.0107      |
|    n_updates            | 8850        |
|    policy_gradient_loss | -0.00532    |
|    std                  | 0.552       |
|    value_loss           | 0.0812      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1814000, episode_reward=2.22 +/- 2.50
Episode length: 237.80 +/- 76.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 238      |
|    mean_reward     | 2.22     |
| time/              |          |
|    total_timesteps | 1814000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 635     |
|    iterations      | 886     |
|    time_elapsed    | 2854    |
|    total_timesteps | 1814528 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1815000, episode_reward=1.90 +/- 2.81
Episode length: 258.40 +/- 55.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 258          |
|    mean_reward          | 1.9          |
| time/                   |              |
|    total_timesteps      | 1815000      |
| train/                  |              |
|    approx_kl            | 0.0047035916 |
|    clip_fraction        | 0.0451       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.64        |
|    explained_variance   | 0.89         |
|    learning_rate        | 5.18e-05     |
|    loss                 | 0.00635      |
|    n_updates            | 8860         |
|    policy_gradient_loss | -0.0067      |
|    std                  | 0.55         |
|    value_loss           | 0.0484       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1816000, episode_reward=4.23 +/- 1.93
Episode length: 193.20 +/- 57.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 193      |
|    mean_reward     | 4.23     |
| time/              |          |
|    total_timesteps | 1816000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 635     |
|    iterations      | 887     |
|    time_elapsed    | 2856    |
|    total_timesteps | 1816576 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1817000, episode_reward=3.99 +/- 2.50
Episode length: 191.40 +/- 58.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 191         |
|    mean_reward          | 3.99        |
| time/                   |             |
|    total_timesteps      | 1817000     |
| train/                  |             |
|    approx_kl            | 0.003686339 |
|    clip_fraction        | 0.028       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.64       |
|    explained_variance   | 0.764       |
|    learning_rate        | 5.18e-05    |
|    loss                 | 0.0389      |
|    n_updates            | 8870        |
|    policy_gradient_loss | -0.00319    |
|    std                  | 0.549       |
|    value_loss           | 0.047       |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1818000, episode_reward=2.71 +/- 3.07
Episode length: 210.00 +/- 73.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | 2.71     |
| time/              |          |
|    total_timesteps | 1818000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 635     |
|    iterations      | 888     |
|    time_elapsed    | 2859    |
|    total_timesteps | 1818624 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1819000, episode_reward=1.03 +/- 2.18
Episode length: 272.60 +/- 54.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 273          |
|    mean_reward          | 1.03         |
| time/                   |              |
|    total_timesteps      | 1819000      |
| train/                  |              |
|    approx_kl            | 0.0052523315 |
|    clip_fraction        | 0.0548       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.64        |
|    explained_variance   | 0.871        |
|    learning_rate        | 5.18e-05     |
|    loss                 | 0.0293       |
|    n_updates            | 8880         |
|    policy_gradient_loss | -0.00642     |
|    std                  | 0.551        |
|    value_loss           | 0.0624       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1820000, episode_reward=0.83 +/- 2.41
Episode length: 276.80 +/- 46.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 277      |
|    mean_reward     | 0.834    |
| time/              |          |
|    total_timesteps | 1820000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 635     |
|    iterations      | 889     |
|    time_elapsed    | 2862    |
|    total_timesteps | 1820672 |
--------------------------------
Eval num_timesteps=1821000, episode_reward=-0.48 +/- 0.66
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.478      |
| time/                   |             |
|    total_timesteps      | 1821000     |
| train/                  |             |
|    approx_kl            | 0.004682163 |
|    clip_fraction        | 0.0424      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.64       |
|    explained_variance   | 0.874       |
|    learning_rate        | 5.18e-05    |
|    loss                 | 0.00797     |
|    n_updates            | 8890        |
|    policy_gradient_loss | -0.0042     |
|    std                  | 0.55        |
|    value_loss           | 0.0868      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1822000, episode_reward=2.75 +/- 3.18
Episode length: 237.20 +/- 62.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 237      |
|    mean_reward     | 2.75     |
| time/              |          |
|    total_timesteps | 1822000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 635     |
|    iterations      | 890     |
|    time_elapsed    | 2866    |
|    total_timesteps | 1822720 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1823000, episode_reward=2.08 +/- 2.78
Episode length: 267.40 +/- 41.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 267         |
|    mean_reward          | 2.08        |
| time/                   |             |
|    total_timesteps      | 1823000     |
| train/                  |             |
|    approx_kl            | 0.006071396 |
|    clip_fraction        | 0.0461      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.64       |
|    explained_variance   | 0.918       |
|    learning_rate        | 5.19e-05    |
|    loss                 | 0.0269      |
|    n_updates            | 8900        |
|    policy_gradient_loss | -0.00521    |
|    std                  | 0.549       |
|    value_loss           | 0.0455      |
-----------------------------------------
box reached target
Eval num_timesteps=1824000, episode_reward=0.78 +/- 2.41
Episode length: 277.00 +/- 46.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 277      |
|    mean_reward     | 0.776    |
| time/              |          |
|    total_timesteps | 1824000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 635     |
|    iterations      | 891     |
|    time_elapsed    | 2869    |
|    total_timesteps | 1824768 |
--------------------------------
box reached target
Eval num_timesteps=1825000, episode_reward=0.31 +/- 2.59
Episode length: 287.40 +/- 25.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 287          |
|    mean_reward          | 0.307        |
| time/                   |              |
|    total_timesteps      | 1825000      |
| train/                  |              |
|    approx_kl            | 0.0059112348 |
|    clip_fraction        | 0.0514       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.64        |
|    explained_variance   | 0.909        |
|    learning_rate        | 5.19e-05     |
|    loss                 | 0.00256      |
|    n_updates            | 8910         |
|    policy_gradient_loss | -0.00862     |
|    std                  | 0.55         |
|    value_loss           | 0.0172       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1826000, episode_reward=0.49 +/- 2.43
Episode length: 271.20 +/- 57.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 271      |
|    mean_reward     | 0.493    |
| time/              |          |
|    total_timesteps | 1826000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 636     |
|    iterations      | 892     |
|    time_elapsed    | 2872    |
|    total_timesteps | 1826816 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1827000, episode_reward=0.63 +/- 2.41
Episode length: 274.80 +/- 50.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 275         |
|    mean_reward          | 0.63        |
| time/                   |             |
|    total_timesteps      | 1827000     |
| train/                  |             |
|    approx_kl            | 0.006753608 |
|    clip_fraction        | 0.0511      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.64       |
|    explained_variance   | 0.963       |
|    learning_rate        | 5.19e-05    |
|    loss                 | 0.00261     |
|    n_updates            | 8920        |
|    policy_gradient_loss | -0.00574    |
|    std                  | 0.549       |
|    value_loss           | 0.0212      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1828000, episode_reward=3.09 +/- 2.79
Episode length: 225.20 +/- 61.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 225      |
|    mean_reward     | 3.09     |
| time/              |          |
|    total_timesteps | 1828000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 636     |
|    iterations      | 893     |
|    time_elapsed    | 2875    |
|    total_timesteps | 1828864 |
--------------------------------
Eval num_timesteps=1829000, episode_reward=-0.80 +/- 0.57
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.803       |
| time/                   |              |
|    total_timesteps      | 1829000      |
| train/                  |              |
|    approx_kl            | 0.0050385147 |
|    clip_fraction        | 0.0412       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.64        |
|    explained_variance   | 0.954        |
|    learning_rate        | 5.19e-05     |
|    loss                 | -0.0363      |
|    n_updates            | 8930         |
|    policy_gradient_loss | -0.00581     |
|    std                  | 0.55         |
|    value_loss           | 0.0148       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1830000, episode_reward=1.24 +/- 2.03
Episode length: 270.60 +/- 58.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 271      |
|    mean_reward     | 1.24     |
| time/              |          |
|    total_timesteps | 1830000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 636     |
|    iterations      | 894     |
|    time_elapsed    | 2878    |
|    total_timesteps | 1830912 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1831000, episode_reward=3.02 +/- 2.66
Episode length: 223.40 +/- 64.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 223          |
|    mean_reward          | 3.02         |
| time/                   |              |
|    total_timesteps      | 1831000      |
| train/                  |              |
|    approx_kl            | 0.0045832787 |
|    clip_fraction        | 0.0305       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.64        |
|    explained_variance   | 0.953        |
|    learning_rate        | 5.2e-05      |
|    loss                 | 0.000523     |
|    n_updates            | 8940         |
|    policy_gradient_loss | -0.00489     |
|    std                  | 0.55         |
|    value_loss           | 0.0131       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1832000, episode_reward=3.14 +/- 2.45
Episode length: 212.20 +/- 71.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 212      |
|    mean_reward     | 3.14     |
| time/              |          |
|    total_timesteps | 1832000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 636     |
|    iterations      | 895     |
|    time_elapsed    | 2881    |
|    total_timesteps | 1832960 |
--------------------------------
box reached target
Eval num_timesteps=1833000, episode_reward=0.91 +/- 2.16
Episode length: 270.20 +/- 59.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 270         |
|    mean_reward          | 0.913       |
| time/                   |             |
|    total_timesteps      | 1833000     |
| train/                  |             |
|    approx_kl            | 0.004274166 |
|    clip_fraction        | 0.0247      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.64       |
|    explained_variance   | 0.907       |
|    learning_rate        | 5.2e-05     |
|    loss                 | 0.0871      |
|    n_updates            | 8950        |
|    policy_gradient_loss | -0.005      |
|    std                  | 0.549       |
|    value_loss           | 0.0428      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1834000, episode_reward=1.84 +/- 2.86
Episode length: 240.80 +/- 72.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 241      |
|    mean_reward     | 1.84     |
| time/              |          |
|    total_timesteps | 1834000  |
---------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1835000, episode_reward=-0.61 +/- 0.67
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.614   |
| time/              |          |
|    total_timesteps | 1835000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 636     |
|    iterations      | 896     |
|    time_elapsed    | 2885    |
|    total_timesteps | 1835008 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1836000, episode_reward=4.29 +/- 1.90
Episode length: 208.20 +/- 47.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 208          |
|    mean_reward          | 4.29         |
| time/                   |              |
|    total_timesteps      | 1836000      |
| train/                  |              |
|    approx_kl            | 0.0066752983 |
|    clip_fraction        | 0.0457       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.64        |
|    explained_variance   | 0.965        |
|    learning_rate        | 5.2e-05      |
|    loss                 | -0.0201      |
|    n_updates            | 8960         |
|    policy_gradient_loss | -0.00636     |
|    std                  | 0.548        |
|    value_loss           | 0.0281       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1837000, episode_reward=3.07 +/- 2.55
Episode length: 224.80 +/- 62.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 225      |
|    mean_reward     | 3.07     |
| time/              |          |
|    total_timesteps | 1837000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 636     |
|    iterations      | 897     |
|    time_elapsed    | 2887    |
|    total_timesteps | 1837056 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1838000, episode_reward=1.66 +/- 2.89
Episode length: 246.20 +/- 66.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 246         |
|    mean_reward          | 1.66        |
| time/                   |             |
|    total_timesteps      | 1838000     |
| train/                  |             |
|    approx_kl            | 0.003566768 |
|    clip_fraction        | 0.0302      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.63       |
|    explained_variance   | 0.882       |
|    learning_rate        | 5.2e-05     |
|    loss                 | 0.00942     |
|    n_updates            | 8970        |
|    policy_gradient_loss | -0.00596    |
|    std                  | 0.548       |
|    value_loss           | 0.022       |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1839000, episode_reward=4.01 +/- 2.50
Episode length: 190.80 +/- 57.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 191      |
|    mean_reward     | 4.01     |
| time/              |          |
|    total_timesteps | 1839000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 636     |
|    iterations      | 898     |
|    time_elapsed    | 2890    |
|    total_timesteps | 1839104 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1840000, episode_reward=1.70 +/- 2.91
Episode length: 249.40 +/- 65.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 249         |
|    mean_reward          | 1.7         |
| time/                   |             |
|    total_timesteps      | 1840000     |
| train/                  |             |
|    approx_kl            | 0.004585717 |
|    clip_fraction        | 0.0369      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.63       |
|    explained_variance   | 0.92        |
|    learning_rate        | 5.21e-05    |
|    loss                 | 0.0257      |
|    n_updates            | 8980        |
|    policy_gradient_loss | -0.00467    |
|    std                  | 0.546       |
|    value_loss           | 0.0323      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1841000, episode_reward=4.09 +/- 2.55
Episode length: 206.60 +/- 49.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 207      |
|    mean_reward     | 4.09     |
| time/              |          |
|    total_timesteps | 1841000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 636     |
|    iterations      | 899     |
|    time_elapsed    | 2893    |
|    total_timesteps | 1841152 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1842000, episode_reward=3.22 +/- 2.33
Episode length: 222.80 +/- 64.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 223          |
|    mean_reward          | 3.22         |
| time/                   |              |
|    total_timesteps      | 1842000      |
| train/                  |              |
|    approx_kl            | 0.0042686034 |
|    clip_fraction        | 0.03         |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.63        |
|    explained_variance   | 0.899        |
|    learning_rate        | 5.21e-05     |
|    loss                 | -0.00647     |
|    n_updates            | 8990         |
|    policy_gradient_loss | -0.00485     |
|    std                  | 0.548        |
|    value_loss           | 0.024        |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1843000, episode_reward=2.08 +/- 2.77
Episode length: 247.80 +/- 65.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 248      |
|    mean_reward     | 2.08     |
| time/              |          |
|    total_timesteps | 1843000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 636     |
|    iterations      | 900     |
|    time_elapsed    | 2896    |
|    total_timesteps | 1843200 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1844000, episode_reward=2.67 +/- 3.00
Episode length: 214.20 +/- 70.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 214          |
|    mean_reward          | 2.67         |
| time/                   |              |
|    total_timesteps      | 1844000      |
| train/                  |              |
|    approx_kl            | 0.0044507007 |
|    clip_fraction        | 0.022        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.63        |
|    explained_variance   | 0.863        |
|    learning_rate        | 5.21e-05     |
|    loss                 | 0.0271       |
|    n_updates            | 9000         |
|    policy_gradient_loss | -0.00261     |
|    std                  | 0.547        |
|    value_loss           | 0.0513       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1845000, episode_reward=1.55 +/- 3.13
Episode length: 255.80 +/- 54.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 256      |
|    mean_reward     | 1.55     |
| time/              |          |
|    total_timesteps | 1845000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 636     |
|    iterations      | 901     |
|    time_elapsed    | 2899    |
|    total_timesteps | 1845248 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1846000, episode_reward=1.47 +/- 3.14
Episode length: 251.80 +/- 61.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 252          |
|    mean_reward          | 1.47         |
| time/                   |              |
|    total_timesteps      | 1846000      |
| train/                  |              |
|    approx_kl            | 0.0055886107 |
|    clip_fraction        | 0.0366       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.63        |
|    explained_variance   | 0.71         |
|    learning_rate        | 5.21e-05     |
|    loss                 | 0.0171       |
|    n_updates            | 9010         |
|    policy_gradient_loss | -0.00523     |
|    std                  | 0.548        |
|    value_loss           | 0.0867       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1847000, episode_reward=0.86 +/- 2.35
Episode length: 274.40 +/- 51.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 274      |
|    mean_reward     | 0.861    |
| time/              |          |
|    total_timesteps | 1847000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 636     |
|    iterations      | 902     |
|    time_elapsed    | 2902    |
|    total_timesteps | 1847296 |
--------------------------------
box reached target
Eval num_timesteps=1848000, episode_reward=0.50 +/- 2.32
Episode length: 270.80 +/- 58.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 271          |
|    mean_reward          | 0.496        |
| time/                   |              |
|    total_timesteps      | 1848000      |
| train/                  |              |
|    approx_kl            | 0.0063672764 |
|    clip_fraction        | 0.0542       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.63        |
|    explained_variance   | 0.937        |
|    learning_rate        | 5.22e-05     |
|    loss                 | 0.00714      |
|    n_updates            | 9020         |
|    policy_gradient_loss | -0.00569     |
|    std                  | 0.548        |
|    value_loss           | 0.0254       |
------------------------------------------
Eval num_timesteps=1849000, episode_reward=-0.11 +/- 0.67
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.113   |
| time/              |          |
|    total_timesteps | 1849000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 636     |
|    iterations      | 903     |
|    time_elapsed    | 2905    |
|    total_timesteps | 1849344 |
--------------------------------
box reached target
Eval num_timesteps=1850000, episode_reward=1.01 +/- 2.28
Episode length: 276.00 +/- 48.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 276         |
|    mean_reward          | 1.01        |
| time/                   |             |
|    total_timesteps      | 1850000     |
| train/                  |             |
|    approx_kl            | 0.006867827 |
|    clip_fraction        | 0.0542      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.63       |
|    explained_variance   | 0.775       |
|    learning_rate        | 5.22e-05    |
|    loss                 | 0.0461      |
|    n_updates            | 9030        |
|    policy_gradient_loss | -0.00679    |
|    std                  | 0.547       |
|    value_loss           | 0.0484      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1851000, episode_reward=1.84 +/- 2.97
Episode length: 267.00 +/- 44.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 267      |
|    mean_reward     | 1.84     |
| time/              |          |
|    total_timesteps | 1851000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 636     |
|    iterations      | 904     |
|    time_elapsed    | 2908    |
|    total_timesteps | 1851392 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1852000, episode_reward=4.07 +/- 2.74
Episode length: 221.40 +/- 63.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 221          |
|    mean_reward          | 4.07         |
| time/                   |              |
|    total_timesteps      | 1852000      |
| train/                  |              |
|    approx_kl            | 0.0053960653 |
|    clip_fraction        | 0.0434       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.63        |
|    explained_variance   | 0.926        |
|    learning_rate        | 5.22e-05     |
|    loss                 | -0.0224      |
|    n_updates            | 9040         |
|    policy_gradient_loss | -0.00653     |
|    std                  | 0.548        |
|    value_loss           | 0.0211       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1853000, episode_reward=2.72 +/- 3.17
Episode length: 223.20 +/- 63.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 223      |
|    mean_reward     | 2.72     |
| time/              |          |
|    total_timesteps | 1853000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 636     |
|    iterations      | 905     |
|    time_elapsed    | 2911    |
|    total_timesteps | 1853440 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1854000, episode_reward=2.81 +/- 3.04
Episode length: 220.20 +/- 65.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 220         |
|    mean_reward          | 2.81        |
| time/                   |             |
|    total_timesteps      | 1854000     |
| train/                  |             |
|    approx_kl            | 0.006610735 |
|    clip_fraction        | 0.0684      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.63       |
|    explained_variance   | 0.882       |
|    learning_rate        | 5.22e-05    |
|    loss                 | 0.0231      |
|    n_updates            | 9050        |
|    policy_gradient_loss | -0.00797    |
|    std                  | 0.547       |
|    value_loss           | 0.0339      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1855000, episode_reward=3.29 +/- 2.64
Episode length: 235.00 +/- 54.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 235      |
|    mean_reward     | 3.29     |
| time/              |          |
|    total_timesteps | 1855000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 636     |
|    iterations      | 906     |
|    time_elapsed    | 2914    |
|    total_timesteps | 1855488 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1856000, episode_reward=3.29 +/- 2.58
Episode length: 228.20 +/- 59.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 228          |
|    mean_reward          | 3.29         |
| time/                   |              |
|    total_timesteps      | 1856000      |
| train/                  |              |
|    approx_kl            | 0.0044491123 |
|    clip_fraction        | 0.0282       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.63        |
|    explained_variance   | 0.884        |
|    learning_rate        | 5.23e-05     |
|    loss                 | 0.0346       |
|    n_updates            | 9060         |
|    policy_gradient_loss | -0.00477     |
|    std                  | 0.547        |
|    value_loss           | 0.0583       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1857000, episode_reward=3.97 +/- 2.64
Episode length: 218.80 +/- 44.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 219      |
|    mean_reward     | 3.97     |
| time/              |          |
|    total_timesteps | 1857000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 636     |
|    iterations      | 907     |
|    time_elapsed    | 2917    |
|    total_timesteps | 1857536 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1858000, episode_reward=2.93 +/- 2.81
Episode length: 214.00 +/- 70.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 214          |
|    mean_reward          | 2.93         |
| time/                   |              |
|    total_timesteps      | 1858000      |
| train/                  |              |
|    approx_kl            | 0.0044015083 |
|    clip_fraction        | 0.0321       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.63        |
|    explained_variance   | 0.834        |
|    learning_rate        | 5.23e-05     |
|    loss                 | -0.00553     |
|    n_updates            | 9070         |
|    policy_gradient_loss | -0.00285     |
|    std                  | 0.549        |
|    value_loss           | 0.0214       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1859000, episode_reward=3.98 +/- 2.49
Episode length: 189.40 +/- 56.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 189      |
|    mean_reward     | 3.98     |
| time/              |          |
|    total_timesteps | 1859000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 636     |
|    iterations      | 908     |
|    time_elapsed    | 2920    |
|    total_timesteps | 1859584 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1860000, episode_reward=3.08 +/- 2.71
Episode length: 240.00 +/- 51.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 240          |
|    mean_reward          | 3.08         |
| time/                   |              |
|    total_timesteps      | 1860000      |
| train/                  |              |
|    approx_kl            | 0.0043102195 |
|    clip_fraction        | 0.034        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.64        |
|    explained_variance   | 0.888        |
|    learning_rate        | 5.23e-05     |
|    loss                 | 0.0075       |
|    n_updates            | 9080         |
|    policy_gradient_loss | -0.00285     |
|    std                  | 0.551        |
|    value_loss           | 0.0187       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=1861000, episode_reward=-0.21 +/- 0.47
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.209   |
| time/              |          |
|    total_timesteps | 1861000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 636     |
|    iterations      | 909     |
|    time_elapsed    | 2923    |
|    total_timesteps | 1861632 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1862000, episode_reward=4.18 +/- 2.21
Episode length: 211.80 +/- 46.30
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 212        |
|    mean_reward          | 4.18       |
| time/                   |            |
|    total_timesteps      | 1862000    |
| train/                  |            |
|    approx_kl            | 0.00468342 |
|    clip_fraction        | 0.0419     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.65      |
|    explained_variance   | 0.969      |
|    learning_rate        | 5.23e-05   |
|    loss                 | -0.0083    |
|    n_updates            | 9090       |
|    policy_gradient_loss | -0.00476   |
|    std                  | 0.551      |
|    value_loss           | 0.0148     |
----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1863000, episode_reward=2.54 +/- 3.18
Episode length: 213.00 +/- 71.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 213      |
|    mean_reward     | 2.54     |
| time/              |          |
|    total_timesteps | 1863000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 636     |
|    iterations      | 910     |
|    time_elapsed    | 2926    |
|    total_timesteps | 1863680 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1864000, episode_reward=2.70 +/- 2.94
Episode length: 215.80 +/- 68.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 216         |
|    mean_reward          | 2.7         |
| time/                   |             |
|    total_timesteps      | 1864000     |
| train/                  |             |
|    approx_kl            | 0.006706739 |
|    clip_fraction        | 0.0409      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.64       |
|    explained_variance   | 0.847       |
|    learning_rate        | 5.24e-05    |
|    loss                 | 0.0773      |
|    n_updates            | 9100        |
|    policy_gradient_loss | -0.00553    |
|    std                  | 0.551       |
|    value_loss           | 0.0938      |
-----------------------------------------
box reached target
Eval num_timesteps=1865000, episode_reward=0.89 +/- 2.28
Episode length: 268.20 +/- 63.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 268      |
|    mean_reward     | 0.885    |
| time/              |          |
|    total_timesteps | 1865000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 636     |
|    iterations      | 911     |
|    time_elapsed    | 2929    |
|    total_timesteps | 1865728 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1866000, episode_reward=1.63 +/- 2.96
Episode length: 240.60 +/- 73.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 241          |
|    mean_reward          | 1.63         |
| time/                   |              |
|    total_timesteps      | 1866000      |
| train/                  |              |
|    approx_kl            | 0.0075319884 |
|    clip_fraction        | 0.0728       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.65        |
|    explained_variance   | 0.934        |
|    learning_rate        | 5.24e-05     |
|    loss                 | 0.0265       |
|    n_updates            | 9110         |
|    policy_gradient_loss | -0.0087      |
|    std                  | 0.553        |
|    value_loss           | 0.0147       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1867000, episode_reward=2.97 +/- 3.01
Episode length: 244.00 +/- 56.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 244      |
|    mean_reward     | 2.97     |
| time/              |          |
|    total_timesteps | 1867000  |
---------------------------------
box reached target
box reached target
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 636     |
|    iterations      | 912     |
|    time_elapsed    | 2932    |
|    total_timesteps | 1867776 |
--------------------------------
box reached target
Eval num_timesteps=1868000, episode_reward=0.25 +/- 2.48
Episode length: 269.20 +/- 61.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 269          |
|    mean_reward          | 0.245        |
| time/                   |              |
|    total_timesteps      | 1868000      |
| train/                  |              |
|    approx_kl            | 0.0045250426 |
|    clip_fraction        | 0.0347       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.65        |
|    explained_variance   | 0.965        |
|    learning_rate        | 5.24e-05     |
|    loss                 | -0.00947     |
|    n_updates            | 9120         |
|    policy_gradient_loss | -0.00555     |
|    std                  | 0.552        |
|    value_loss           | 0.0231       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1869000, episode_reward=1.85 +/- 2.84
Episode length: 247.80 +/- 63.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 248      |
|    mean_reward     | 1.85     |
| time/              |          |
|    total_timesteps | 1869000  |
---------------------------------
box reached target
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 636     |
|    iterations      | 913     |
|    time_elapsed    | 2935    |
|    total_timesteps | 1869824 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1870000, episode_reward=2.70 +/- 3.02
Episode length: 225.40 +/- 64.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 225          |
|    mean_reward          | 2.7          |
| time/                   |              |
|    total_timesteps      | 1870000      |
| train/                  |              |
|    approx_kl            | 0.0056161247 |
|    clip_fraction        | 0.0388       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.65        |
|    explained_variance   | 0.953        |
|    learning_rate        | 5.24e-05     |
|    loss                 | 0.00771      |
|    n_updates            | 9130         |
|    policy_gradient_loss | -0.00651     |
|    std                  | 0.552        |
|    value_loss           | 0.0225       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1871000, episode_reward=2.84 +/- 3.15
Episode length: 243.00 +/- 47.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 243      |
|    mean_reward     | 2.84     |
| time/              |          |
|    total_timesteps | 1871000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 637     |
|    iterations      | 914     |
|    time_elapsed    | 2938    |
|    total_timesteps | 1871872 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1872000, episode_reward=3.10 +/- 2.85
Episode length: 230.00 +/- 67.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 230         |
|    mean_reward          | 3.1         |
| time/                   |             |
|    total_timesteps      | 1872000     |
| train/                  |             |
|    approx_kl            | 0.007839622 |
|    clip_fraction        | 0.0634      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.65       |
|    explained_variance   | 0.976       |
|    learning_rate        | 5.25e-05    |
|    loss                 | 0.0301      |
|    n_updates            | 9140        |
|    policy_gradient_loss | -0.00855    |
|    std                  | 0.552       |
|    value_loss           | 0.0143      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1873000, episode_reward=2.91 +/- 2.95
Episode length: 218.60 +/- 67.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 219      |
|    mean_reward     | 2.91     |
| time/              |          |
|    total_timesteps | 1873000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 637     |
|    iterations      | 915     |
|    time_elapsed    | 2941    |
|    total_timesteps | 1873920 |
--------------------------------
box reached target
Eval num_timesteps=1874000, episode_reward=0.50 +/- 2.42
Episode length: 274.60 +/- 50.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 275          |
|    mean_reward          | 0.5          |
| time/                   |              |
|    total_timesteps      | 1874000      |
| train/                  |              |
|    approx_kl            | 0.0052410085 |
|    clip_fraction        | 0.0437       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.64        |
|    explained_variance   | 0.908        |
|    learning_rate        | 5.25e-05     |
|    loss                 | 0.0224       |
|    n_updates            | 9150         |
|    policy_gradient_loss | -0.00491     |
|    std                  | 0.55         |
|    value_loss           | 0.0359       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=1875000, episode_reward=1.97 +/- 3.02
Episode length: 261.40 +/- 49.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 261      |
|    mean_reward     | 1.97     |
| time/              |          |
|    total_timesteps | 1875000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 637     |
|    iterations      | 916     |
|    time_elapsed    | 2944    |
|    total_timesteps | 1875968 |
--------------------------------
box reached target
Eval num_timesteps=1876000, episode_reward=0.65 +/- 2.32
Episode length: 274.80 +/- 50.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 275         |
|    mean_reward          | 0.653       |
| time/                   |             |
|    total_timesteps      | 1876000     |
| train/                  |             |
|    approx_kl            | 0.005972703 |
|    clip_fraction        | 0.0474      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.64       |
|    explained_variance   | 0.909       |
|    learning_rate        | 5.25e-05    |
|    loss                 | -0.0072     |
|    n_updates            | 9160        |
|    policy_gradient_loss | -0.0085     |
|    std                  | 0.552       |
|    value_loss           | 0.015       |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1877000, episode_reward=1.47 +/- 3.02
Episode length: 254.20 +/- 60.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 254      |
|    mean_reward     | 1.47     |
| time/              |          |
|    total_timesteps | 1877000  |
---------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1878000, episode_reward=1.67 +/- 2.94
Episode length: 237.60 +/- 76.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 238      |
|    mean_reward     | 1.67     |
| time/              |          |
|    total_timesteps | 1878000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 637     |
|    iterations      | 917     |
|    time_elapsed    | 2948    |
|    total_timesteps | 1878016 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1879000, episode_reward=4.20 +/- 1.91
Episode length: 193.00 +/- 54.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 193          |
|    mean_reward          | 4.2          |
| time/                   |              |
|    total_timesteps      | 1879000      |
| train/                  |              |
|    approx_kl            | 0.0066671167 |
|    clip_fraction        | 0.0617       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.65        |
|    explained_variance   | 0.939        |
|    learning_rate        | 5.25e-05     |
|    loss                 | -0.00307     |
|    n_updates            | 9170         |
|    policy_gradient_loss | -0.00812     |
|    std                  | 0.552        |
|    value_loss           | 0.044        |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1880000, episode_reward=3.06 +/- 2.68
Episode length: 228.00 +/- 59.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 228      |
|    mean_reward     | 3.06     |
| time/              |          |
|    total_timesteps | 1880000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 637     |
|    iterations      | 918     |
|    time_elapsed    | 2950    |
|    total_timesteps | 1880064 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1881000, episode_reward=0.71 +/- 2.46
Episode length: 277.20 +/- 45.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 277         |
|    mean_reward          | 0.706       |
| time/                   |             |
|    total_timesteps      | 1881000     |
| train/                  |             |
|    approx_kl            | 0.004276272 |
|    clip_fraction        | 0.0349      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.64       |
|    explained_variance   | 0.864       |
|    learning_rate        | 5.26e-05    |
|    loss                 | 0.0194      |
|    n_updates            | 9180        |
|    policy_gradient_loss | -0.00359    |
|    std                  | 0.549       |
|    value_loss           | 0.0631      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1882000, episode_reward=2.11 +/- 2.72
Episode length: 255.20 +/- 56.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 255      |
|    mean_reward     | 2.11     |
| time/              |          |
|    total_timesteps | 1882000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 637     |
|    iterations      | 919     |
|    time_elapsed    | 2953    |
|    total_timesteps | 1882112 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1883000, episode_reward=0.73 +/- 2.24
Episode length: 279.60 +/- 40.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 280          |
|    mean_reward          | 0.731        |
| time/                   |              |
|    total_timesteps      | 1883000      |
| train/                  |              |
|    approx_kl            | 0.0047650235 |
|    clip_fraction        | 0.0628       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.64        |
|    explained_variance   | 0.946        |
|    learning_rate        | 5.26e-05     |
|    loss                 | -0.013       |
|    n_updates            | 9190         |
|    policy_gradient_loss | -0.00761     |
|    std                  | 0.551        |
|    value_loss           | 0.0143       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1884000, episode_reward=2.88 +/- 2.92
Episode length: 224.40 +/- 61.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 224      |
|    mean_reward     | 2.88     |
| time/              |          |
|    total_timesteps | 1884000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 637     |
|    iterations      | 920     |
|    time_elapsed    | 2956    |
|    total_timesteps | 1884160 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1885000, episode_reward=2.77 +/- 3.08
Episode length: 225.80 +/- 63.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 226         |
|    mean_reward          | 2.77        |
| time/                   |             |
|    total_timesteps      | 1885000     |
| train/                  |             |
|    approx_kl            | 0.005513969 |
|    clip_fraction        | 0.0636      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.64       |
|    explained_variance   | 0.947       |
|    learning_rate        | 5.26e-05    |
|    loss                 | -0.0223     |
|    n_updates            | 9200        |
|    policy_gradient_loss | -0.0108     |
|    std                  | 0.551       |
|    value_loss           | 0.0175      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1886000, episode_reward=1.17 +/- 2.22
Episode length: 280.20 +/- 39.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 280      |
|    mean_reward     | 1.17     |
| time/              |          |
|    total_timesteps | 1886000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 637     |
|    iterations      | 921     |
|    time_elapsed    | 2960    |
|    total_timesteps | 1886208 |
--------------------------------
box reached target
Eval num_timesteps=1887000, episode_reward=0.66 +/- 2.31
Episode length: 270.80 +/- 58.40
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 271        |
|    mean_reward          | 0.663      |
| time/                   |            |
|    total_timesteps      | 1887000    |
| train/                  |            |
|    approx_kl            | 0.00704804 |
|    clip_fraction        | 0.0531     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.64      |
|    explained_variance   | 0.881      |
|    learning_rate        | 5.26e-05   |
|    loss                 | -0.00686   |
|    n_updates            | 9210       |
|    policy_gradient_loss | -0.00939   |
|    std                  | 0.549      |
|    value_loss           | 0.0632     |
----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1888000, episode_reward=2.16 +/- 2.66
Episode length: 256.00 +/- 54.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 256      |
|    mean_reward     | 2.16     |
| time/              |          |
|    total_timesteps | 1888000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 637     |
|    iterations      | 922     |
|    time_elapsed    | 2963    |
|    total_timesteps | 1888256 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1889000, episode_reward=1.74 +/- 2.92
Episode length: 249.60 +/- 61.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 250          |
|    mean_reward          | 1.74         |
| time/                   |              |
|    total_timesteps      | 1889000      |
| train/                  |              |
|    approx_kl            | 0.0038039659 |
|    clip_fraction        | 0.0343       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.64        |
|    explained_variance   | 0.836        |
|    learning_rate        | 5.27e-05     |
|    loss                 | 0.0328       |
|    n_updates            | 9220         |
|    policy_gradient_loss | -0.00363     |
|    std                  | 0.549        |
|    value_loss           | 0.0536       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1890000, episode_reward=2.13 +/- 2.70
Episode length: 255.40 +/- 54.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 255      |
|    mean_reward     | 2.13     |
| time/              |          |
|    total_timesteps | 1890000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 637     |
|    iterations      | 923     |
|    time_elapsed    | 2966    |
|    total_timesteps | 1890304 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1891000, episode_reward=1.53 +/- 3.04
Episode length: 249.80 +/- 61.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 250          |
|    mean_reward          | 1.53         |
| time/                   |              |
|    total_timesteps      | 1891000      |
| train/                  |              |
|    approx_kl            | 0.0071713356 |
|    clip_fraction        | 0.0575       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.64        |
|    explained_variance   | 0.914        |
|    learning_rate        | 5.27e-05     |
|    loss                 | -0.0107      |
|    n_updates            | 9230         |
|    policy_gradient_loss | -0.00639     |
|    std                  | 0.551        |
|    value_loss           | 0.0144       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1892000, episode_reward=4.30 +/- 2.19
Episode length: 220.00 +/- 42.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 220      |
|    mean_reward     | 4.3      |
| time/              |          |
|    total_timesteps | 1892000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 637     |
|    iterations      | 924     |
|    time_elapsed    | 2969    |
|    total_timesteps | 1892352 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1893000, episode_reward=3.16 +/- 2.75
Episode length: 234.00 +/- 58.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 234         |
|    mean_reward          | 3.16        |
| time/                   |             |
|    total_timesteps      | 1893000     |
| train/                  |             |
|    approx_kl            | 0.004882098 |
|    clip_fraction        | 0.0496      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.65       |
|    explained_variance   | 0.95        |
|    learning_rate        | 5.27e-05    |
|    loss                 | 0.00346     |
|    n_updates            | 9240        |
|    policy_gradient_loss | -0.00708    |
|    std                  | 0.552       |
|    value_loss           | 0.0217      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=1894000, episode_reward=2.00 +/- 2.68
Episode length: 260.80 +/- 48.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 261      |
|    mean_reward     | 2        |
| time/              |          |
|    total_timesteps | 1894000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 637     |
|    iterations      | 925     |
|    time_elapsed    | 2972    |
|    total_timesteps | 1894400 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1895000, episode_reward=-0.25 +/- 0.56
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.247      |
| time/                   |             |
|    total_timesteps      | 1895000     |
| train/                  |             |
|    approx_kl            | 0.004948537 |
|    clip_fraction        | 0.0398      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.65       |
|    explained_variance   | 0.919       |
|    learning_rate        | 5.27e-05    |
|    loss                 | 0.0398      |
|    n_updates            | 9250        |
|    policy_gradient_loss | -0.00708    |
|    std                  | 0.551       |
|    value_loss           | 0.0262      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1896000, episode_reward=0.56 +/- 2.48
Episode length: 273.40 +/- 53.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 273      |
|    mean_reward     | 0.557    |
| time/              |          |
|    total_timesteps | 1896000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 637     |
|    iterations      | 926     |
|    time_elapsed    | 2975    |
|    total_timesteps | 1896448 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1897000, episode_reward=1.20 +/- 2.11
Episode length: 272.60 +/- 54.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 273          |
|    mean_reward          | 1.2          |
| time/                   |              |
|    total_timesteps      | 1897000      |
| train/                  |              |
|    approx_kl            | 0.0074536162 |
|    clip_fraction        | 0.0495       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.65        |
|    explained_variance   | 0.923        |
|    learning_rate        | 5.28e-05     |
|    loss                 | -0.0078      |
|    n_updates            | 9260         |
|    policy_gradient_loss | -0.00512     |
|    std                  | 0.552        |
|    value_loss           | 0.0561       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1898000, episode_reward=3.05 +/- 2.65
Episode length: 224.40 +/- 63.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 224      |
|    mean_reward     | 3.05     |
| time/              |          |
|    total_timesteps | 1898000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 637     |
|    iterations      | 927     |
|    time_elapsed    | 2978    |
|    total_timesteps | 1898496 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1899000, episode_reward=4.06 +/- 2.53
Episode length: 219.80 +/- 47.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 220          |
|    mean_reward          | 4.06         |
| time/                   |              |
|    total_timesteps      | 1899000      |
| train/                  |              |
|    approx_kl            | 0.0039505484 |
|    clip_fraction        | 0.034        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.65        |
|    explained_variance   | 0.954        |
|    learning_rate        | 5.28e-05     |
|    loss                 | 0.0132       |
|    n_updates            | 9270         |
|    policy_gradient_loss | -0.0034      |
|    std                  | 0.55         |
|    value_loss           | 0.0144       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1900000, episode_reward=1.90 +/- 2.86
Episode length: 237.00 +/- 77.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 237      |
|    mean_reward     | 1.9      |
| time/              |          |
|    total_timesteps | 1900000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 637     |
|    iterations      | 928     |
|    time_elapsed    | 2981    |
|    total_timesteps | 1900544 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1901000, episode_reward=2.97 +/- 2.85
Episode length: 230.80 +/- 58.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 231          |
|    mean_reward          | 2.97         |
| time/                   |              |
|    total_timesteps      | 1901000      |
| train/                  |              |
|    approx_kl            | 0.0040179524 |
|    clip_fraction        | 0.0354       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.64        |
|    explained_variance   | 0.871        |
|    learning_rate        | 5.28e-05     |
|    loss                 | 0.00437      |
|    n_updates            | 9280         |
|    policy_gradient_loss | -0.00612     |
|    std                  | 0.551        |
|    value_loss           | 0.0322       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1902000, episode_reward=4.47 +/- 1.99
Episode length: 217.80 +/- 51.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 218      |
|    mean_reward     | 4.47     |
| time/              |          |
|    total_timesteps | 1902000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 637     |
|    iterations      | 929     |
|    time_elapsed    | 2984    |
|    total_timesteps | 1902592 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1903000, episode_reward=0.36 +/- 2.53
Episode length: 274.40 +/- 51.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 274        |
|    mean_reward          | 0.364      |
| time/                   |            |
|    total_timesteps      | 1903000    |
| train/                  |            |
|    approx_kl            | 0.00764507 |
|    clip_fraction        | 0.0821     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.65      |
|    explained_variance   | 0.946      |
|    learning_rate        | 5.28e-05   |
|    loss                 | -0.0125    |
|    n_updates            | 9290       |
|    policy_gradient_loss | -0.00753   |
|    std                  | 0.551      |
|    value_loss           | 0.0163     |
----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1904000, episode_reward=1.79 +/- 2.99
Episode length: 257.20 +/- 59.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 257      |
|    mean_reward     | 1.79     |
| time/              |          |
|    total_timesteps | 1904000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 637     |
|    iterations      | 930     |
|    time_elapsed    | 2987    |
|    total_timesteps | 1904640 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1905000, episode_reward=4.37 +/- 2.08
Episode length: 199.20 +/- 50.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 199          |
|    mean_reward          | 4.37         |
| time/                   |              |
|    total_timesteps      | 1905000      |
| train/                  |              |
|    approx_kl            | 0.0077377902 |
|    clip_fraction        | 0.0637       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.65        |
|    explained_variance   | 0.961        |
|    learning_rate        | 5.29e-05     |
|    loss                 | -0.00149     |
|    n_updates            | 9300         |
|    policy_gradient_loss | -0.00804     |
|    std                  | 0.551        |
|    value_loss           | 0.0261       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1906000, episode_reward=3.05 +/- 2.68
Episode length: 212.40 +/- 71.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 212      |
|    mean_reward     | 3.05     |
| time/              |          |
|    total_timesteps | 1906000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 637     |
|    iterations      | 931     |
|    time_elapsed    | 2989    |
|    total_timesteps | 1906688 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1907000, episode_reward=1.32 +/- 3.12
Episode length: 248.60 +/- 63.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 249          |
|    mean_reward          | 1.32         |
| time/                   |              |
|    total_timesteps      | 1907000      |
| train/                  |              |
|    approx_kl            | 0.0066326237 |
|    clip_fraction        | 0.0543       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.64        |
|    explained_variance   | 0.849        |
|    learning_rate        | 5.29e-05     |
|    loss                 | 0.00442      |
|    n_updates            | 9310         |
|    policy_gradient_loss | -0.00788     |
|    std                  | 0.551        |
|    value_loss           | 0.0153       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1908000, episode_reward=0.46 +/- 2.45
Episode length: 273.60 +/- 52.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 274      |
|    mean_reward     | 0.459    |
| time/              |          |
|    total_timesteps | 1908000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 637     |
|    iterations      | 932     |
|    time_elapsed    | 2993    |
|    total_timesteps | 1908736 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1909000, episode_reward=3.08 +/- 2.83
Episode length: 231.80 +/- 56.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 232          |
|    mean_reward          | 3.08         |
| time/                   |              |
|    total_timesteps      | 1909000      |
| train/                  |              |
|    approx_kl            | 0.0064466354 |
|    clip_fraction        | 0.0543       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.65        |
|    explained_variance   | 0.978        |
|    learning_rate        | 5.29e-05     |
|    loss                 | -0.0215      |
|    n_updates            | 9320         |
|    policy_gradient_loss | -0.00713     |
|    std                  | 0.552        |
|    value_loss           | 0.0117       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1910000, episode_reward=2.69 +/- 3.12
Episode length: 223.80 +/- 63.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 224      |
|    mean_reward     | 2.69     |
| time/              |          |
|    total_timesteps | 1910000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 637     |
|    iterations      | 933     |
|    time_elapsed    | 2995    |
|    total_timesteps | 1910784 |
--------------------------------
box reached target
Eval num_timesteps=1911000, episode_reward=-0.45 +/- 0.36
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.448       |
| time/                   |              |
|    total_timesteps      | 1911000      |
| train/                  |              |
|    approx_kl            | 0.0054538134 |
|    clip_fraction        | 0.0458       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.64        |
|    explained_variance   | 0.968        |
|    learning_rate        | 5.29e-05     |
|    loss                 | -0.00929     |
|    n_updates            | 9330         |
|    policy_gradient_loss | -0.00501     |
|    std                  | 0.549        |
|    value_loss           | 0.0184       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1912000, episode_reward=0.92 +/- 2.37
Episode length: 279.00 +/- 42.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 279      |
|    mean_reward     | 0.918    |
| time/              |          |
|    total_timesteps | 1912000  |
---------------------------------
box reached target
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 637     |
|    iterations      | 934     |
|    time_elapsed    | 2999    |
|    total_timesteps | 1912832 |
--------------------------------
box reached target
Eval num_timesteps=1913000, episode_reward=0.46 +/- 2.49
Episode length: 277.40 +/- 45.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 277         |
|    mean_reward          | 0.46        |
| time/                   |             |
|    total_timesteps      | 1913000     |
| train/                  |             |
|    approx_kl            | 0.003807542 |
|    clip_fraction        | 0.0165      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.64       |
|    explained_variance   | 0.89        |
|    learning_rate        | 5.3e-05     |
|    loss                 | -0.0108     |
|    n_updates            | 9340        |
|    policy_gradient_loss | -0.00189    |
|    std                  | 0.548       |
|    value_loss           | 0.0359      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1914000, episode_reward=1.56 +/- 3.16
Episode length: 262.60 +/- 46.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 263      |
|    mean_reward     | 1.56     |
| time/              |          |
|    total_timesteps | 1914000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 637     |
|    iterations      | 935     |
|    time_elapsed    | 3002    |
|    total_timesteps | 1914880 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1915000, episode_reward=2.98 +/- 3.00
Episode length: 233.00 +/- 57.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 233          |
|    mean_reward          | 2.98         |
| time/                   |              |
|    total_timesteps      | 1915000      |
| train/                  |              |
|    approx_kl            | 0.0065129017 |
|    clip_fraction        | 0.0494       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.63        |
|    explained_variance   | 0.955        |
|    learning_rate        | 5.3e-05      |
|    loss                 | 0.0185       |
|    n_updates            | 9350         |
|    policy_gradient_loss | -0.00671     |
|    std                  | 0.548        |
|    value_loss           | 0.0118       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1916000, episode_reward=1.92 +/- 2.87
Episode length: 247.20 +/- 64.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 247      |
|    mean_reward     | 1.92     |
| time/              |          |
|    total_timesteps | 1916000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 637     |
|    iterations      | 936     |
|    time_elapsed    | 3005    |
|    total_timesteps | 1916928 |
--------------------------------
box reached target
Eval num_timesteps=1917000, episode_reward=0.77 +/- 2.21
Episode length: 269.00 +/- 62.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 269         |
|    mean_reward          | 0.772       |
| time/                   |             |
|    total_timesteps      | 1917000     |
| train/                  |             |
|    approx_kl            | 0.004307952 |
|    clip_fraction        | 0.037       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.63       |
|    explained_variance   | 0.969       |
|    learning_rate        | 5.3e-05     |
|    loss                 | 0.0172      |
|    n_updates            | 9360        |
|    policy_gradient_loss | -0.00434    |
|    std                  | 0.547       |
|    value_loss           | 0.00969     |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1918000, episode_reward=2.71 +/- 3.08
Episode length: 223.00 +/- 64.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 223      |
|    mean_reward     | 2.71     |
| time/              |          |
|    total_timesteps | 1918000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 637     |
|    iterations      | 937     |
|    time_elapsed    | 3008    |
|    total_timesteps | 1918976 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1919000, episode_reward=0.48 +/- 2.35
Episode length: 269.40 +/- 61.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 269          |
|    mean_reward          | 0.476        |
| time/                   |              |
|    total_timesteps      | 1919000      |
| train/                  |              |
|    approx_kl            | 0.0038467383 |
|    clip_fraction        | 0.0349       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.63        |
|    explained_variance   | 0.949        |
|    learning_rate        | 5.3e-05      |
|    loss                 | 0.00454      |
|    n_updates            | 9370         |
|    policy_gradient_loss | -0.00365     |
|    std                  | 0.548        |
|    value_loss           | 0.0244       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1920000, episode_reward=1.71 +/- 3.02
Episode length: 259.40 +/- 52.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 259      |
|    mean_reward     | 1.71     |
| time/              |          |
|    total_timesteps | 1920000  |
---------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1921000, episode_reward=1.94 +/- 2.77
Episode length: 254.60 +/- 57.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 255      |
|    mean_reward     | 1.94     |
| time/              |          |
|    total_timesteps | 1921000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 637     |
|    iterations      | 938     |
|    time_elapsed    | 3011    |
|    total_timesteps | 1921024 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1922000, episode_reward=0.22 +/- 2.47
Episode length: 271.20 +/- 57.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 271         |
|    mean_reward          | 0.215       |
| time/                   |             |
|    total_timesteps      | 1922000     |
| train/                  |             |
|    approx_kl            | 0.005489705 |
|    clip_fraction        | 0.0342      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.63       |
|    explained_variance   | 0.966       |
|    learning_rate        | 5.31e-05    |
|    loss                 | -8.23e-06   |
|    n_updates            | 9380        |
|    policy_gradient_loss | -0.00502    |
|    std                  | 0.546       |
|    value_loss           | 0.0113      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1923000, episode_reward=3.07 +/- 2.87
Episode length: 240.80 +/- 57.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 241      |
|    mean_reward     | 3.07     |
| time/              |          |
|    total_timesteps | 1923000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 637     |
|    iterations      | 939     |
|    time_elapsed    | 3014    |
|    total_timesteps | 1923072 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1924000, episode_reward=3.12 +/- 2.78
Episode length: 229.00 +/- 61.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 229          |
|    mean_reward          | 3.12         |
| time/                   |              |
|    total_timesteps      | 1924000      |
| train/                  |              |
|    approx_kl            | 0.0037030755 |
|    clip_fraction        | 0.0192       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.62        |
|    explained_variance   | 0.913        |
|    learning_rate        | 5.31e-05     |
|    loss                 | 0.0143       |
|    n_updates            | 9390         |
|    policy_gradient_loss | -0.00317     |
|    std                  | 0.544        |
|    value_loss           | 0.0468       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1925000, episode_reward=2.09 +/- 2.66
Episode length: 253.80 +/- 62.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 254      |
|    mean_reward     | 2.09     |
| time/              |          |
|    total_timesteps | 1925000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 637     |
|    iterations      | 940     |
|    time_elapsed    | 3017    |
|    total_timesteps | 1925120 |
--------------------------------
box reached target
Eval num_timesteps=1926000, episode_reward=0.27 +/- 2.63
Episode length: 272.40 +/- 55.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 272          |
|    mean_reward          | 0.27         |
| time/                   |              |
|    total_timesteps      | 1926000      |
| train/                  |              |
|    approx_kl            | 0.0058683436 |
|    clip_fraction        | 0.0499       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.62        |
|    explained_variance   | 0.901        |
|    learning_rate        | 5.31e-05     |
|    loss                 | -0.012       |
|    n_updates            | 9400         |
|    policy_gradient_loss | -0.00795     |
|    std                  | 0.543        |
|    value_loss           | 0.0225       |
------------------------------------------
box reached target
Eval num_timesteps=1927000, episode_reward=1.04 +/- 2.16
Episode length: 269.00 +/- 62.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 269      |
|    mean_reward     | 1.04     |
| time/              |          |
|    total_timesteps | 1927000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 637     |
|    iterations      | 941     |
|    time_elapsed    | 3020    |
|    total_timesteps | 1927168 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1928000, episode_reward=2.86 +/- 3.15
Episode length: 233.60 +/- 56.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 234         |
|    mean_reward          | 2.86        |
| time/                   |             |
|    total_timesteps      | 1928000     |
| train/                  |             |
|    approx_kl            | 0.005944377 |
|    clip_fraction        | 0.0616      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.62       |
|    explained_variance   | 0.803       |
|    learning_rate        | 5.31e-05    |
|    loss                 | 0.00771     |
|    n_updates            | 9410        |
|    policy_gradient_loss | -0.00695    |
|    std                  | 0.544       |
|    value_loss           | 0.0285      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=1929000, episode_reward=0.37 +/- 2.41
Episode length: 277.00 +/- 46.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 277      |
|    mean_reward     | 0.369    |
| time/              |          |
|    total_timesteps | 1929000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 637     |
|    iterations      | 942     |
|    time_elapsed    | 3023    |
|    total_timesteps | 1929216 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1930000, episode_reward=2.74 +/- 3.26
Episode length: 224.40 +/- 66.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 224         |
|    mean_reward          | 2.74        |
| time/                   |             |
|    total_timesteps      | 1930000     |
| train/                  |             |
|    approx_kl            | 0.005029535 |
|    clip_fraction        | 0.0555      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.62       |
|    explained_variance   | 0.939       |
|    learning_rate        | 5.32e-05    |
|    loss                 | 0.000413    |
|    n_updates            | 9420        |
|    policy_gradient_loss | -0.00561    |
|    std                  | 0.544       |
|    value_loss           | 0.0226      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1931000, episode_reward=1.61 +/- 2.94
Episode length: 254.40 +/- 55.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 254      |
|    mean_reward     | 1.61     |
| time/              |          |
|    total_timesteps | 1931000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 638     |
|    iterations      | 943     |
|    time_elapsed    | 3026    |
|    total_timesteps | 1931264 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1932000, episode_reward=2.79 +/- 3.10
Episode length: 236.00 +/- 52.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 236          |
|    mean_reward          | 2.79         |
| time/                   |              |
|    total_timesteps      | 1932000      |
| train/                  |              |
|    approx_kl            | 0.0043921415 |
|    clip_fraction        | 0.0356       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.62        |
|    explained_variance   | 0.978        |
|    learning_rate        | 5.32e-05     |
|    loss                 | 0.0044       |
|    n_updates            | 9430         |
|    policy_gradient_loss | -0.00625     |
|    std                  | 0.542        |
|    value_loss           | 0.0144       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=1933000, episode_reward=1.49 +/- 3.22
Episode length: 255.80 +/- 55.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 256      |
|    mean_reward     | 1.49     |
| time/              |          |
|    total_timesteps | 1933000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 638     |
|    iterations      | 944     |
|    time_elapsed    | 3029    |
|    total_timesteps | 1933312 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1934000, episode_reward=1.58 +/- 3.20
Episode length: 251.80 +/- 60.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 252         |
|    mean_reward          | 1.58        |
| time/                   |             |
|    total_timesteps      | 1934000     |
| train/                  |             |
|    approx_kl            | 0.006060411 |
|    clip_fraction        | 0.0588      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.61       |
|    explained_variance   | 0.889       |
|    learning_rate        | 5.32e-05    |
|    loss                 | -0.00836    |
|    n_updates            | 9440        |
|    policy_gradient_loss | -0.00739    |
|    std                  | 0.544       |
|    value_loss           | 0.0439      |
-----------------------------------------
Eval num_timesteps=1935000, episode_reward=-0.10 +/- 0.90
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.1     |
| time/              |          |
|    total_timesteps | 1935000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 638     |
|    iterations      | 945     |
|    time_elapsed    | 3033    |
|    total_timesteps | 1935360 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1936000, episode_reward=2.93 +/- 2.95
Episode length: 230.00 +/- 58.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 230          |
|    mean_reward          | 2.93         |
| time/                   |              |
|    total_timesteps      | 1936000      |
| train/                  |              |
|    approx_kl            | 0.0046956628 |
|    clip_fraction        | 0.0265       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.62        |
|    explained_variance   | 0.908        |
|    learning_rate        | 5.32e-05     |
|    loss                 | 0.00204      |
|    n_updates            | 9450         |
|    policy_gradient_loss | -0.00256     |
|    std                  | 0.544        |
|    value_loss           | 0.0197       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1937000, episode_reward=3.08 +/- 2.91
Episode length: 238.00 +/- 51.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 238      |
|    mean_reward     | 3.08     |
| time/              |          |
|    total_timesteps | 1937000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 638     |
|    iterations      | 946     |
|    time_elapsed    | 3036    |
|    total_timesteps | 1937408 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1938000, episode_reward=1.28 +/- 3.20
Episode length: 245.20 +/- 67.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 245          |
|    mean_reward          | 1.28         |
| time/                   |              |
|    total_timesteps      | 1938000      |
| train/                  |              |
|    approx_kl            | 0.0053523546 |
|    clip_fraction        | 0.0498       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.62        |
|    explained_variance   | 0.911        |
|    learning_rate        | 5.32e-05     |
|    loss                 | -0.00896     |
|    n_updates            | 9460         |
|    policy_gradient_loss | -0.00721     |
|    std                  | 0.544        |
|    value_loss           | 0.0101       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1939000, episode_reward=3.00 +/- 2.76
Episode length: 223.60 +/- 63.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 224      |
|    mean_reward     | 3        |
| time/              |          |
|    total_timesteps | 1939000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 638     |
|    iterations      | 947     |
|    time_elapsed    | 3038    |
|    total_timesteps | 1939456 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1940000, episode_reward=1.61 +/- 2.91
Episode length: 252.20 +/- 59.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 252          |
|    mean_reward          | 1.61         |
| time/                   |              |
|    total_timesteps      | 1940000      |
| train/                  |              |
|    approx_kl            | 0.0054476922 |
|    clip_fraction        | 0.0556       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.62        |
|    explained_variance   | 0.923        |
|    learning_rate        | 5.33e-05     |
|    loss                 | 0.0156       |
|    n_updates            | 9470         |
|    policy_gradient_loss | -0.00613     |
|    std                  | 0.544        |
|    value_loss           | 0.0569       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1941000, episode_reward=2.16 +/- 2.53
Episode length: 253.80 +/- 56.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 254      |
|    mean_reward     | 2.16     |
| time/              |          |
|    total_timesteps | 1941000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 638     |
|    iterations      | 948     |
|    time_elapsed    | 3042    |
|    total_timesteps | 1941504 |
--------------------------------
box reached target
Eval num_timesteps=1942000, episode_reward=0.39 +/- 2.49
Episode length: 270.80 +/- 58.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 271          |
|    mean_reward          | 0.395        |
| time/                   |              |
|    total_timesteps      | 1942000      |
| train/                  |              |
|    approx_kl            | 0.0057180286 |
|    clip_fraction        | 0.0563       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.62        |
|    explained_variance   | 0.942        |
|    learning_rate        | 5.33e-05     |
|    loss                 | -0.0262      |
|    n_updates            | 9480         |
|    policy_gradient_loss | -0.00603     |
|    std                  | 0.545        |
|    value_loss           | 0.0198       |
------------------------------------------
box reached target
Eval num_timesteps=1943000, episode_reward=1.08 +/- 2.34
Episode length: 278.40 +/- 43.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | 1.08     |
| time/              |          |
|    total_timesteps | 1943000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 638     |
|    iterations      | 949     |
|    time_elapsed    | 3045    |
|    total_timesteps | 1943552 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1944000, episode_reward=1.96 +/- 2.79
Episode length: 247.40 +/- 65.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 247         |
|    mean_reward          | 1.96        |
| time/                   |             |
|    total_timesteps      | 1944000     |
| train/                  |             |
|    approx_kl            | 0.004803044 |
|    clip_fraction        | 0.0402      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.62       |
|    explained_variance   | 0.869       |
|    learning_rate        | 5.33e-05    |
|    loss                 | -0.0078     |
|    n_updates            | 9490        |
|    policy_gradient_loss | -0.00473    |
|    std                  | 0.544       |
|    value_loss           | 0.0196      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1945000, episode_reward=1.40 +/- 3.07
Episode length: 244.00 +/- 68.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 244      |
|    mean_reward     | 1.4      |
| time/              |          |
|    total_timesteps | 1945000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 638     |
|    iterations      | 950     |
|    time_elapsed    | 3048    |
|    total_timesteps | 1945600 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1946000, episode_reward=4.45 +/- 1.92
Episode length: 228.00 +/- 46.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 228          |
|    mean_reward          | 4.45         |
| time/                   |              |
|    total_timesteps      | 1946000      |
| train/                  |              |
|    approx_kl            | 0.0050985264 |
|    clip_fraction        | 0.0417       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.62        |
|    explained_variance   | 0.87         |
|    learning_rate        | 5.33e-05     |
|    loss                 | -0.00994     |
|    n_updates            | 9500         |
|    policy_gradient_loss | -0.00785     |
|    std                  | 0.544        |
|    value_loss           | 0.066        |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1947000, episode_reward=2.62 +/- 3.12
Episode length: 224.00 +/- 68.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 224      |
|    mean_reward     | 2.62     |
| time/              |          |
|    total_timesteps | 1947000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 638     |
|    iterations      | 951     |
|    time_elapsed    | 3051    |
|    total_timesteps | 1947648 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1948000, episode_reward=2.02 +/- 2.65
Episode length: 255.20 +/- 55.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 255         |
|    mean_reward          | 2.02        |
| time/                   |             |
|    total_timesteps      | 1948000     |
| train/                  |             |
|    approx_kl            | 0.006465762 |
|    clip_fraction        | 0.0567      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.62       |
|    explained_variance   | 0.92        |
|    learning_rate        | 5.34e-05    |
|    loss                 | 0.0468      |
|    n_updates            | 9510        |
|    policy_gradient_loss | -0.00462    |
|    std                  | 0.544       |
|    value_loss           | 0.0433      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1949000, episode_reward=0.23 +/- 2.52
Episode length: 270.20 +/- 59.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 270      |
|    mean_reward     | 0.229    |
| time/              |          |
|    total_timesteps | 1949000  |
---------------------------------
box reached target
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 638     |
|    iterations      | 952     |
|    time_elapsed    | 3054    |
|    total_timesteps | 1949696 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1950000, episode_reward=2.82 +/- 3.11
Episode length: 242.00 +/- 55.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 242          |
|    mean_reward          | 2.82         |
| time/                   |              |
|    total_timesteps      | 1950000      |
| train/                  |              |
|    approx_kl            | 0.0045500826 |
|    clip_fraction        | 0.0334       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.61        |
|    explained_variance   | 0.892        |
|    learning_rate        | 5.34e-05     |
|    loss                 | 0.0192       |
|    n_updates            | 9520         |
|    policy_gradient_loss | -0.00603     |
|    std                  | 0.541        |
|    value_loss           | 0.0599       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1951000, episode_reward=0.79 +/- 2.38
Episode length: 269.60 +/- 60.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 270      |
|    mean_reward     | 0.795    |
| time/              |          |
|    total_timesteps | 1951000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 638     |
|    iterations      | 953     |
|    time_elapsed    | 3057    |
|    total_timesteps | 1951744 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1952000, episode_reward=1.44 +/- 3.05
Episode length: 246.40 +/- 66.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 246         |
|    mean_reward          | 1.44        |
| time/                   |             |
|    total_timesteps      | 1952000     |
| train/                  |             |
|    approx_kl            | 0.004102429 |
|    clip_fraction        | 0.03        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.61       |
|    explained_variance   | 0.823       |
|    learning_rate        | 5.34e-05    |
|    loss                 | -0.00593    |
|    n_updates            | 9530        |
|    policy_gradient_loss | -0.00363    |
|    std                  | 0.541       |
|    value_loss           | 0.0632      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1953000, episode_reward=2.02 +/- 2.61
Episode length: 254.60 +/- 56.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 255      |
|    mean_reward     | 2.02     |
| time/              |          |
|    total_timesteps | 1953000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 638     |
|    iterations      | 954     |
|    time_elapsed    | 3060    |
|    total_timesteps | 1953792 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1954000, episode_reward=1.75 +/- 2.94
Episode length: 246.00 +/- 66.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 246          |
|    mean_reward          | 1.75         |
| time/                   |              |
|    total_timesteps      | 1954000      |
| train/                  |              |
|    approx_kl            | 0.0045445315 |
|    clip_fraction        | 0.0377       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.61        |
|    explained_variance   | 0.939        |
|    learning_rate        | 5.34e-05     |
|    loss                 | 0.0177       |
|    n_updates            | 9540         |
|    policy_gradient_loss | -0.00444     |
|    std                  | 0.541        |
|    value_loss           | 0.0308       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1955000, episode_reward=1.51 +/- 2.91
Episode length: 247.60 +/- 64.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 248      |
|    mean_reward     | 1.51     |
| time/              |          |
|    total_timesteps | 1955000  |
---------------------------------
box reached target
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 638     |
|    iterations      | 955     |
|    time_elapsed    | 3063    |
|    total_timesteps | 1955840 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1956000, episode_reward=5.21 +/- 0.08
Episode length: 169.40 +/- 26.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 169         |
|    mean_reward          | 5.21        |
| time/                   |             |
|    total_timesteps      | 1956000     |
| train/                  |             |
|    approx_kl            | 0.006488356 |
|    clip_fraction        | 0.0489      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.61       |
|    explained_variance   | 0.936       |
|    learning_rate        | 5.35e-05    |
|    loss                 | -0.0222     |
|    n_updates            | 9550        |
|    policy_gradient_loss | -0.00614    |
|    std                  | 0.54        |
|    value_loss           | 0.0308      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1957000, episode_reward=2.94 +/- 2.85
Episode length: 218.60 +/- 66.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 219      |
|    mean_reward     | 2.94     |
| time/              |          |
|    total_timesteps | 1957000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 638     |
|    iterations      | 956     |
|    time_elapsed    | 3065    |
|    total_timesteps | 1957888 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1958000, episode_reward=1.36 +/- 3.17
Episode length: 249.20 +/- 62.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 249          |
|    mean_reward          | 1.36         |
| time/                   |              |
|    total_timesteps      | 1958000      |
| train/                  |              |
|    approx_kl            | 0.0052369316 |
|    clip_fraction        | 0.047        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.61        |
|    explained_variance   | 0.741        |
|    learning_rate        | 5.35e-05     |
|    loss                 | 0.0141       |
|    n_updates            | 9560         |
|    policy_gradient_loss | -0.00529     |
|    std                  | 0.54         |
|    value_loss           | 0.0376       |
------------------------------------------
box reached target
Eval num_timesteps=1959000, episode_reward=0.40 +/- 2.57
Episode length: 277.80 +/- 44.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | 0.396    |
| time/              |          |
|    total_timesteps | 1959000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 638     |
|    iterations      | 957     |
|    time_elapsed    | 3069    |
|    total_timesteps | 1959936 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1960000, episode_reward=1.97 +/- 2.68
Episode length: 248.20 +/- 63.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 248          |
|    mean_reward          | 1.97         |
| time/                   |              |
|    total_timesteps      | 1960000      |
| train/                  |              |
|    approx_kl            | 0.0062003434 |
|    clip_fraction        | 0.0456       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.61        |
|    explained_variance   | 0.782        |
|    learning_rate        | 5.35e-05     |
|    loss                 | -0.00317     |
|    n_updates            | 9570         |
|    policy_gradient_loss | -0.00793     |
|    std                  | 0.541        |
|    value_loss           | 0.0188       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1961000, episode_reward=5.20 +/- 0.13
Episode length: 177.40 +/- 13.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 177      |
|    mean_reward     | 5.2      |
| time/              |          |
|    total_timesteps | 1961000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 638     |
|    iterations      | 958     |
|    time_elapsed    | 3071    |
|    total_timesteps | 1961984 |
--------------------------------
box reached target
Eval num_timesteps=1962000, episode_reward=0.68 +/- 2.34
Episode length: 278.80 +/- 42.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 279          |
|    mean_reward          | 0.684        |
| time/                   |              |
|    total_timesteps      | 1962000      |
| train/                  |              |
|    approx_kl            | 0.0058543794 |
|    clip_fraction        | 0.0458       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.61        |
|    explained_variance   | 0.935        |
|    learning_rate        | 5.35e-05     |
|    loss                 | -0.0159      |
|    n_updates            | 9580         |
|    policy_gradient_loss | -0.00635     |
|    std                  | 0.539        |
|    value_loss           | 0.0251       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1963000, episode_reward=2.74 +/- 3.02
Episode length: 231.40 +/- 57.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 231      |
|    mean_reward     | 2.74     |
| time/              |          |
|    total_timesteps | 1963000  |
---------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1964000, episode_reward=4.27 +/- 2.11
Episode length: 210.40 +/- 47.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | 4.27     |
| time/              |          |
|    total_timesteps | 1964000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 638     |
|    iterations      | 959     |
|    time_elapsed    | 3075    |
|    total_timesteps | 1964032 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1965000, episode_reward=1.71 +/- 2.86
Episode length: 246.00 +/- 66.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 246          |
|    mean_reward          | 1.71         |
| time/                   |              |
|    total_timesteps      | 1965000      |
| train/                  |              |
|    approx_kl            | 0.0077837324 |
|    clip_fraction        | 0.0739       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.6         |
|    explained_variance   | 0.923        |
|    learning_rate        | 5.36e-05     |
|    loss                 | -0.0179      |
|    n_updates            | 9590         |
|    policy_gradient_loss | -0.00759     |
|    std                  | 0.54         |
|    value_loss           | 0.0138       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1966000, episode_reward=1.41 +/- 3.11
Episode length: 257.80 +/- 57.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 258      |
|    mean_reward     | 1.41     |
| time/              |          |
|    total_timesteps | 1966000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 638     |
|    iterations      | 960     |
|    time_elapsed    | 3078    |
|    total_timesteps | 1966080 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1967000, episode_reward=1.89 +/- 3.03
Episode length: 254.40 +/- 57.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 254          |
|    mean_reward          | 1.89         |
| time/                   |              |
|    total_timesteps      | 1967000      |
| train/                  |              |
|    approx_kl            | 0.0074310824 |
|    clip_fraction        | 0.0789       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.6         |
|    explained_variance   | 0.961        |
|    learning_rate        | 5.36e-05     |
|    loss                 | 0.00985      |
|    n_updates            | 9600         |
|    policy_gradient_loss | -0.00808     |
|    std                  | 0.54         |
|    value_loss           | 0.00989      |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1968000, episode_reward=3.02 +/- 2.83
Episode length: 230.40 +/- 59.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 230      |
|    mean_reward     | 3.02     |
| time/              |          |
|    total_timesteps | 1968000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 638     |
|    iterations      | 961     |
|    time_elapsed    | 3081    |
|    total_timesteps | 1968128 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1969000, episode_reward=4.00 +/- 2.50
Episode length: 190.40 +/- 57.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 190          |
|    mean_reward          | 4            |
| time/                   |              |
|    total_timesteps      | 1969000      |
| train/                  |              |
|    approx_kl            | 0.0056458893 |
|    clip_fraction        | 0.0483       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.6         |
|    explained_variance   | 0.937        |
|    learning_rate        | 5.36e-05     |
|    loss                 | -0.00555     |
|    n_updates            | 9610         |
|    policy_gradient_loss | -0.00709     |
|    std                  | 0.538        |
|    value_loss           | 0.0155       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1970000, episode_reward=1.87 +/- 2.78
Episode length: 243.20 +/- 69.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 243      |
|    mean_reward     | 1.87     |
| time/              |          |
|    total_timesteps | 1970000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 638     |
|    iterations      | 962     |
|    time_elapsed    | 3084    |
|    total_timesteps | 1970176 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1971000, episode_reward=2.01 +/- 2.66
Episode length: 248.60 +/- 63.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 249         |
|    mean_reward          | 2.01        |
| time/                   |             |
|    total_timesteps      | 1971000     |
| train/                  |             |
|    approx_kl            | 0.005470561 |
|    clip_fraction        | 0.0344      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.6        |
|    explained_variance   | 0.897       |
|    learning_rate        | 5.36e-05    |
|    loss                 | 0.0214      |
|    n_updates            | 9620        |
|    policy_gradient_loss | -0.00442    |
|    std                  | 0.538       |
|    value_loss           | 0.0357      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1972000, episode_reward=3.93 +/- 2.47
Episode length: 189.20 +/- 56.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 189      |
|    mean_reward     | 3.93     |
| time/              |          |
|    total_timesteps | 1972000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 638     |
|    iterations      | 963     |
|    time_elapsed    | 3087    |
|    total_timesteps | 1972224 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1973000, episode_reward=0.36 +/- 2.35
Episode length: 281.00 +/- 38.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 281         |
|    mean_reward          | 0.36        |
| time/                   |             |
|    total_timesteps      | 1973000     |
| train/                  |             |
|    approx_kl            | 0.005158825 |
|    clip_fraction        | 0.0338      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.6        |
|    explained_variance   | 0.945       |
|    learning_rate        | 5.37e-05    |
|    loss                 | 0.0033      |
|    n_updates            | 9630        |
|    policy_gradient_loss | -0.00674    |
|    std                  | 0.537       |
|    value_loss           | 0.0347      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1974000, episode_reward=0.43 +/- 2.48
Episode length: 282.20 +/- 35.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 282      |
|    mean_reward     | 0.431    |
| time/              |          |
|    total_timesteps | 1974000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 638     |
|    iterations      | 964     |
|    time_elapsed    | 3090    |
|    total_timesteps | 1974272 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1975000, episode_reward=1.69 +/- 2.96
Episode length: 246.00 +/- 66.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 246         |
|    mean_reward          | 1.69        |
| time/                   |             |
|    total_timesteps      | 1975000     |
| train/                  |             |
|    approx_kl            | 0.004138115 |
|    clip_fraction        | 0.0304      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.59       |
|    explained_variance   | 0.91        |
|    learning_rate        | 5.37e-05    |
|    loss                 | -0.000516   |
|    n_updates            | 9640        |
|    policy_gradient_loss | -0.00287    |
|    std                  | 0.536       |
|    value_loss           | 0.0497      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1976000, episode_reward=1.57 +/- 3.02
Episode length: 244.00 +/- 68.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 244      |
|    mean_reward     | 1.57     |
| time/              |          |
|    total_timesteps | 1976000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 638     |
|    iterations      | 965     |
|    time_elapsed    | 3093    |
|    total_timesteps | 1976320 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1977000, episode_reward=4.29 +/- 2.03
Episode length: 209.00 +/- 54.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 209         |
|    mean_reward          | 4.29        |
| time/                   |             |
|    total_timesteps      | 1977000     |
| train/                  |             |
|    approx_kl            | 0.006690178 |
|    clip_fraction        | 0.0675      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.59       |
|    explained_variance   | 0.923       |
|    learning_rate        | 5.37e-05    |
|    loss                 | 0.00549     |
|    n_updates            | 9650        |
|    policy_gradient_loss | -0.00798    |
|    std                  | 0.534       |
|    value_loss           | 0.0451      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1978000, episode_reward=1.91 +/- 2.83
Episode length: 243.00 +/- 70.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 243      |
|    mean_reward     | 1.91     |
| time/              |          |
|    total_timesteps | 1978000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 638     |
|    iterations      | 966     |
|    time_elapsed    | 3096    |
|    total_timesteps | 1978368 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1979000, episode_reward=1.74 +/- 2.94
Episode length: 248.80 +/- 65.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 249         |
|    mean_reward          | 1.74        |
| time/                   |             |
|    total_timesteps      | 1979000     |
| train/                  |             |
|    approx_kl            | 0.005751158 |
|    clip_fraction        | 0.0593      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.58       |
|    explained_variance   | 0.892       |
|    learning_rate        | 5.37e-05    |
|    loss                 | 0.00189     |
|    n_updates            | 9660        |
|    policy_gradient_loss | -0.00776    |
|    std                  | 0.534       |
|    value_loss           | 0.0596      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=1980000, episode_reward=0.22 +/- 2.53
Episode length: 281.60 +/- 36.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 282      |
|    mean_reward     | 0.221    |
| time/              |          |
|    total_timesteps | 1980000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 638     |
|    iterations      | 967     |
|    time_elapsed    | 3099    |
|    total_timesteps | 1980416 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1981000, episode_reward=0.78 +/- 2.34
Episode length: 279.40 +/- 41.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 279          |
|    mean_reward          | 0.781        |
| time/                   |              |
|    total_timesteps      | 1981000      |
| train/                  |              |
|    approx_kl            | 0.0039749364 |
|    clip_fraction        | 0.0394       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.58        |
|    explained_variance   | 0.86         |
|    learning_rate        | 5.38e-05     |
|    loss                 | -0.00922     |
|    n_updates            | 9670         |
|    policy_gradient_loss | -0.00602     |
|    std                  | 0.535        |
|    value_loss           | 0.0722       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=1982000, episode_reward=1.63 +/- 2.97
Episode length: 247.40 +/- 65.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 247      |
|    mean_reward     | 1.63     |
| time/              |          |
|    total_timesteps | 1982000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 639     |
|    iterations      | 968     |
|    time_elapsed    | 3102    |
|    total_timesteps | 1982464 |
--------------------------------
box reached target
Eval num_timesteps=1983000, episode_reward=0.86 +/- 2.41
Episode length: 280.40 +/- 39.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 280          |
|    mean_reward          | 0.856        |
| time/                   |              |
|    total_timesteps      | 1983000      |
| train/                  |              |
|    approx_kl            | 0.0059009637 |
|    clip_fraction        | 0.0476       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.59        |
|    explained_variance   | 0.815        |
|    learning_rate        | 5.38e-05     |
|    loss                 | 0.0252       |
|    n_updates            | 9680         |
|    policy_gradient_loss | -0.00639     |
|    std                  | 0.535        |
|    value_loss           | 0.0344       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1984000, episode_reward=3.31 +/- 2.53
Episode length: 218.80 +/- 66.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 219      |
|    mean_reward     | 3.31     |
| time/              |          |
|    total_timesteps | 1984000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 639     |
|    iterations      | 969     |
|    time_elapsed    | 3105    |
|    total_timesteps | 1984512 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1985000, episode_reward=3.07 +/- 2.83
Episode length: 225.20 +/- 62.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 225          |
|    mean_reward          | 3.07         |
| time/                   |              |
|    total_timesteps      | 1985000      |
| train/                  |              |
|    approx_kl            | 0.0062176934 |
|    clip_fraction        | 0.0568       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.59        |
|    explained_variance   | 0.882        |
|    learning_rate        | 5.38e-05     |
|    loss                 | -0.00633     |
|    n_updates            | 9690         |
|    policy_gradient_loss | -0.00544     |
|    std                  | 0.535        |
|    value_loss           | 0.0133       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1986000, episode_reward=2.76 +/- 3.15
Episode length: 241.60 +/- 57.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 242      |
|    mean_reward     | 2.76     |
| time/              |          |
|    total_timesteps | 1986000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 639     |
|    iterations      | 970     |
|    time_elapsed    | 3108    |
|    total_timesteps | 1986560 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1987000, episode_reward=1.03 +/- 2.36
Episode length: 287.20 +/- 25.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 287         |
|    mean_reward          | 1.03        |
| time/                   |             |
|    total_timesteps      | 1987000     |
| train/                  |             |
|    approx_kl            | 0.004713538 |
|    clip_fraction        | 0.0413      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.59       |
|    explained_variance   | 0.836       |
|    learning_rate        | 5.38e-05    |
|    loss                 | 0.0135      |
|    n_updates            | 9700        |
|    policy_gradient_loss | -0.00516    |
|    std                  | 0.536       |
|    value_loss           | 0.0128      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=1988000, episode_reward=1.63 +/- 2.90
Episode length: 238.20 +/- 75.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 238      |
|    mean_reward     | 1.63     |
| time/              |          |
|    total_timesteps | 1988000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 639     |
|    iterations      | 971     |
|    time_elapsed    | 3111    |
|    total_timesteps | 1988608 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1989000, episode_reward=0.15 +/- 2.58
Episode length: 276.80 +/- 46.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 277         |
|    mean_reward          | 0.153       |
| time/                   |             |
|    total_timesteps      | 1989000     |
| train/                  |             |
|    approx_kl            | 0.004060427 |
|    clip_fraction        | 0.0324      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.59       |
|    explained_variance   | 0.966       |
|    learning_rate        | 5.39e-05    |
|    loss                 | 0.0154      |
|    n_updates            | 9710        |
|    policy_gradient_loss | -0.00506    |
|    std                  | 0.535       |
|    value_loss           | 0.0197      |
-----------------------------------------
box reached target
Eval num_timesteps=1990000, episode_reward=0.46 +/- 2.52
Episode length: 274.80 +/- 50.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 275      |
|    mean_reward     | 0.461    |
| time/              |          |
|    total_timesteps | 1990000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 639     |
|    iterations      | 972     |
|    time_elapsed    | 3114    |
|    total_timesteps | 1990656 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1991000, episode_reward=3.20 +/- 2.79
Episode length: 235.40 +/- 55.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 235          |
|    mean_reward          | 3.2          |
| time/                   |              |
|    total_timesteps      | 1991000      |
| train/                  |              |
|    approx_kl            | 0.0053284615 |
|    clip_fraction        | 0.0658       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.59        |
|    explained_variance   | 0.925        |
|    learning_rate        | 5.39e-05     |
|    loss                 | 0.00433      |
|    n_updates            | 9720         |
|    policy_gradient_loss | -0.0102      |
|    std                  | 0.537        |
|    value_loss           | 0.0198       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1992000, episode_reward=1.57 +/- 3.08
Episode length: 242.00 +/- 71.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 242      |
|    mean_reward     | 1.57     |
| time/              |          |
|    total_timesteps | 1992000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 639     |
|    iterations      | 973     |
|    time_elapsed    | 3117    |
|    total_timesteps | 1992704 |
--------------------------------
box reached target
Eval num_timesteps=1993000, episode_reward=0.85 +/- 2.38
Episode length: 274.60 +/- 50.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 275          |
|    mean_reward          | 0.846        |
| time/                   |              |
|    total_timesteps      | 1993000      |
| train/                  |              |
|    approx_kl            | 0.0046389285 |
|    clip_fraction        | 0.0444       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.59        |
|    explained_variance   | 0.878        |
|    learning_rate        | 5.39e-05     |
|    loss                 | 0.021        |
|    n_updates            | 9730         |
|    policy_gradient_loss | -0.00448     |
|    std                  | 0.538        |
|    value_loss           | 0.0337       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1994000, episode_reward=1.67 +/- 2.94
Episode length: 250.40 +/- 60.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 250      |
|    mean_reward     | 1.67     |
| time/              |          |
|    total_timesteps | 1994000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 639     |
|    iterations      | 974     |
|    time_elapsed    | 3120    |
|    total_timesteps | 1994752 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1995000, episode_reward=3.48 +/- 2.48
Episode length: 233.20 +/- 55.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 233          |
|    mean_reward          | 3.48         |
| time/                   |              |
|    total_timesteps      | 1995000      |
| train/                  |              |
|    approx_kl            | 0.0075562615 |
|    clip_fraction        | 0.0551       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.6         |
|    explained_variance   | 0.863        |
|    learning_rate        | 5.39e-05     |
|    loss                 | 0.0103       |
|    n_updates            | 9740         |
|    policy_gradient_loss | -0.00786     |
|    std                  | 0.539        |
|    value_loss           | 0.0339       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=1996000, episode_reward=0.67 +/- 2.37
Episode length: 274.20 +/- 51.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 274      |
|    mean_reward     | 0.67     |
| time/              |          |
|    total_timesteps | 1996000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 639     |
|    iterations      | 975     |
|    time_elapsed    | 3123    |
|    total_timesteps | 1996800 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1997000, episode_reward=1.51 +/- 3.07
Episode length: 256.00 +/- 56.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 256         |
|    mean_reward          | 1.51        |
| time/                   |             |
|    total_timesteps      | 1997000     |
| train/                  |             |
|    approx_kl            | 0.005205946 |
|    clip_fraction        | 0.0387      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.6        |
|    explained_variance   | 0.92        |
|    learning_rate        | 5.4e-05     |
|    loss                 | 0.0167      |
|    n_updates            | 9750        |
|    policy_gradient_loss | -0.00697    |
|    std                  | 0.539       |
|    value_loss           | 0.011       |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=1998000, episode_reward=2.03 +/- 2.57
Episode length: 247.40 +/- 65.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 247      |
|    mean_reward     | 2.03     |
| time/              |          |
|    total_timesteps | 1998000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 639     |
|    iterations      | 976     |
|    time_elapsed    | 3126    |
|    total_timesteps | 1998848 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=1999000, episode_reward=1.71 +/- 2.94
Episode length: 249.80 +/- 61.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 250          |
|    mean_reward          | 1.71         |
| time/                   |              |
|    total_timesteps      | 1999000      |
| train/                  |              |
|    approx_kl            | 0.0063585844 |
|    clip_fraction        | 0.0619       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.6         |
|    explained_variance   | 0.959        |
|    learning_rate        | 5.4e-05      |
|    loss                 | -0.0286      |
|    n_updates            | 9760         |
|    policy_gradient_loss | -0.00851     |
|    std                  | 0.539        |
|    value_loss           | 0.00813      |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2000000, episode_reward=0.95 +/- 2.42
Episode length: 293.80 +/- 12.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 294      |
|    mean_reward     | 0.95     |
| time/              |          |
|    total_timesteps | 2000000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 639     |
|    iterations      | 977     |
|    time_elapsed    | 3129    |
|    total_timesteps | 2000896 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=2001000, episode_reward=1.41 +/- 3.16
Episode length: 257.00 +/- 57.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 257          |
|    mean_reward          | 1.41         |
| time/                   |              |
|    total_timesteps      | 2001000      |
| train/                  |              |
|    approx_kl            | 0.0054122917 |
|    clip_fraction        | 0.0494       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.6         |
|    explained_variance   | 0.979        |
|    learning_rate        | 5.4e-05      |
|    loss                 | -0.0138      |
|    n_updates            | 9770         |
|    policy_gradient_loss | -0.00573     |
|    std                  | 0.541        |
|    value_loss           | 0.00928      |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2002000, episode_reward=0.46 +/- 2.49
Episode length: 275.00 +/- 50.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 275      |
|    mean_reward     | 0.46     |
| time/              |          |
|    total_timesteps | 2002000  |
---------------------------------
box reached target
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 639     |
|    iterations      | 978     |
|    time_elapsed    | 3132    |
|    total_timesteps | 2002944 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2003000, episode_reward=3.09 +/- 2.80
Episode length: 232.40 +/- 63.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 232         |
|    mean_reward          | 3.09        |
| time/                   |             |
|    total_timesteps      | 2003000     |
| train/                  |             |
|    approx_kl            | 0.006616046 |
|    clip_fraction        | 0.049       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.61       |
|    explained_variance   | 0.953       |
|    learning_rate        | 5.4e-05     |
|    loss                 | -0.0095     |
|    n_updates            | 9780        |
|    policy_gradient_loss | -0.00524    |
|    std                  | 0.54        |
|    value_loss           | 0.0268      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2004000, episode_reward=2.09 +/- 2.64
Episode length: 253.20 +/- 57.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 253      |
|    mean_reward     | 2.09     |
| time/              |          |
|    total_timesteps | 2004000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 639     |
|    iterations      | 979     |
|    time_elapsed    | 3135    |
|    total_timesteps | 2004992 |
--------------------------------
box reached target
Eval num_timesteps=2005000, episode_reward=0.60 +/- 2.34
Episode length: 269.80 +/- 60.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 270          |
|    mean_reward          | 0.598        |
| time/                   |              |
|    total_timesteps      | 2005000      |
| train/                  |              |
|    approx_kl            | 0.0055818283 |
|    clip_fraction        | 0.0432       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.6         |
|    explained_variance   | 0.884        |
|    learning_rate        | 5.41e-05     |
|    loss                 | -0.00601     |
|    n_updates            | 9790         |
|    policy_gradient_loss | -0.00503     |
|    std                  | 0.537        |
|    value_loss           | 0.0207       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2006000, episode_reward=2.77 +/- 3.08
Episode length: 230.60 +/- 58.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 231      |
|    mean_reward     | 2.77     |
| time/              |          |
|    total_timesteps | 2006000  |
---------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2007000, episode_reward=0.56 +/- 2.42
Episode length: 275.40 +/- 49.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 275      |
|    mean_reward     | 0.557    |
| time/              |          |
|    total_timesteps | 2007000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 639     |
|    iterations      | 980     |
|    time_elapsed    | 3139    |
|    total_timesteps | 2007040 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2008000, episode_reward=0.64 +/- 2.38
Episode length: 278.60 +/- 42.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 279         |
|    mean_reward          | 0.638       |
| time/                   |             |
|    total_timesteps      | 2008000     |
| train/                  |             |
|    approx_kl            | 0.007459298 |
|    clip_fraction        | 0.0539      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.59       |
|    explained_variance   | 0.905       |
|    learning_rate        | 5.41e-05    |
|    loss                 | 0.00462     |
|    n_updates            | 9800        |
|    policy_gradient_loss | -0.00618    |
|    std                  | 0.536       |
|    value_loss           | 0.0513      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=2009000, episode_reward=0.56 +/- 2.44
Episode length: 272.20 +/- 55.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 272      |
|    mean_reward     | 0.557    |
| time/              |          |
|    total_timesteps | 2009000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 639     |
|    iterations      | 981     |
|    time_elapsed    | 3142    |
|    total_timesteps | 2009088 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2010000, episode_reward=5.35 +/- 0.13
Episode length: 191.40 +/- 29.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 191          |
|    mean_reward          | 5.35         |
| time/                   |              |
|    total_timesteps      | 2010000      |
| train/                  |              |
|    approx_kl            | 0.0064988965 |
|    clip_fraction        | 0.0339       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.59        |
|    explained_variance   | 0.913        |
|    learning_rate        | 5.41e-05     |
|    loss                 | -0.000539    |
|    n_updates            | 9810         |
|    policy_gradient_loss | -0.0041      |
|    std                  | 0.538        |
|    value_loss           | 0.0231       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2011000, episode_reward=4.21 +/- 1.84
Episode length: 202.40 +/- 52.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | 4.21     |
| time/              |          |
|    total_timesteps | 2011000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 639     |
|    iterations      | 982     |
|    time_elapsed    | 3145    |
|    total_timesteps | 2011136 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2012000, episode_reward=1.47 +/- 3.18
Episode length: 248.20 +/- 66.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 248          |
|    mean_reward          | 1.47         |
| time/                   |              |
|    total_timesteps      | 2012000      |
| train/                  |              |
|    approx_kl            | 0.0069680912 |
|    clip_fraction        | 0.0482       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.6         |
|    explained_variance   | 0.958        |
|    learning_rate        | 5.41e-05     |
|    loss                 | 0.0151       |
|    n_updates            | 9820         |
|    policy_gradient_loss | -0.0065      |
|    std                  | 0.538        |
|    value_loss           | 0.0312       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=2013000, episode_reward=0.48 +/- 2.46
Episode length: 269.40 +/- 61.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 269      |
|    mean_reward     | 0.482    |
| time/              |          |
|    total_timesteps | 2013000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 639     |
|    iterations      | 983     |
|    time_elapsed    | 3148    |
|    total_timesteps | 2013184 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2014000, episode_reward=2.37 +/- 2.40
Episode length: 249.60 +/- 61.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 250          |
|    mean_reward          | 2.37         |
| time/                   |              |
|    total_timesteps      | 2014000      |
| train/                  |              |
|    approx_kl            | 0.0048845876 |
|    clip_fraction        | 0.0321       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.6         |
|    explained_variance   | 0.97         |
|    learning_rate        | 5.42e-05     |
|    loss                 | -0.011       |
|    n_updates            | 9830         |
|    policy_gradient_loss | -0.00361     |
|    std                  | 0.537        |
|    value_loss           | 0.0132       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=2015000, episode_reward=0.53 +/- 2.42
Episode length: 290.00 +/- 20.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 290      |
|    mean_reward     | 0.525    |
| time/              |          |
|    total_timesteps | 2015000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 639     |
|    iterations      | 984     |
|    time_elapsed    | 3151    |
|    total_timesteps | 2015232 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2016000, episode_reward=3.99 +/- 2.50
Episode length: 209.40 +/- 57.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 209         |
|    mean_reward          | 3.99        |
| time/                   |             |
|    total_timesteps      | 2016000     |
| train/                  |             |
|    approx_kl            | 0.006628095 |
|    clip_fraction        | 0.0732      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.59       |
|    explained_variance   | 0.952       |
|    learning_rate        | 5.42e-05    |
|    loss                 | -0.0344     |
|    n_updates            | 9840        |
|    policy_gradient_loss | -0.00924    |
|    std                  | 0.536       |
|    value_loss           | 0.0134      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2017000, episode_reward=-0.46 +/- 0.46
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.456   |
| time/              |          |
|    total_timesteps | 2017000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 639     |
|    iterations      | 985     |
|    time_elapsed    | 3154    |
|    total_timesteps | 2017280 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2018000, episode_reward=2.80 +/- 3.11
Episode length: 235.60 +/- 52.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 236          |
|    mean_reward          | 2.8          |
| time/                   |              |
|    total_timesteps      | 2018000      |
| train/                  |              |
|    approx_kl            | 0.0057463637 |
|    clip_fraction        | 0.0563       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.59        |
|    explained_variance   | 0.912        |
|    learning_rate        | 5.42e-05     |
|    loss                 | 0.00313      |
|    n_updates            | 9850         |
|    policy_gradient_loss | -0.00682     |
|    std                  | 0.538        |
|    value_loss           | 0.0288       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2019000, episode_reward=1.70 +/- 2.88
Episode length: 245.40 +/- 68.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 245      |
|    mean_reward     | 1.7      |
| time/              |          |
|    total_timesteps | 2019000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 639     |
|    iterations      | 986     |
|    time_elapsed    | 3157    |
|    total_timesteps | 2019328 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2020000, episode_reward=2.96 +/- 2.91
Episode length: 217.80 +/- 67.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 218         |
|    mean_reward          | 2.96        |
| time/                   |             |
|    total_timesteps      | 2020000     |
| train/                  |             |
|    approx_kl            | 0.005046321 |
|    clip_fraction        | 0.0377      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.6        |
|    explained_variance   | 0.861       |
|    learning_rate        | 5.42e-05    |
|    loss                 | 0.00594     |
|    n_updates            | 9860        |
|    policy_gradient_loss | -0.00522    |
|    std                  | 0.539       |
|    value_loss           | 0.0659      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2021000, episode_reward=2.28 +/- 2.41
Episode length: 253.80 +/- 56.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 254      |
|    mean_reward     | 2.28     |
| time/              |          |
|    total_timesteps | 2021000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 639     |
|    iterations      | 987     |
|    time_elapsed    | 3160    |
|    total_timesteps | 2021376 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2022000, episode_reward=1.48 +/- 3.16
Episode length: 259.20 +/- 51.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 259         |
|    mean_reward          | 1.48        |
| time/                   |             |
|    total_timesteps      | 2022000     |
| train/                  |             |
|    approx_kl            | 0.006249001 |
|    clip_fraction        | 0.0415      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.6        |
|    explained_variance   | 0.928       |
|    learning_rate        | 5.43e-05    |
|    loss                 | 0.00473     |
|    n_updates            | 9870        |
|    policy_gradient_loss | -0.00676    |
|    std                  | 0.537       |
|    value_loss           | 0.0444      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2023000, episode_reward=2.98 +/- 2.93
Episode length: 227.00 +/- 60.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 227      |
|    mean_reward     | 2.98     |
| time/              |          |
|    total_timesteps | 2023000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 639     |
|    iterations      | 988     |
|    time_elapsed    | 3163    |
|    total_timesteps | 2023424 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2024000, episode_reward=2.75 +/- 3.06
Episode length: 235.60 +/- 54.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 236         |
|    mean_reward          | 2.75        |
| time/                   |             |
|    total_timesteps      | 2024000     |
| train/                  |             |
|    approx_kl            | 0.005178744 |
|    clip_fraction        | 0.0422      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.59       |
|    explained_variance   | 0.946       |
|    learning_rate        | 5.43e-05    |
|    loss                 | -0.00878    |
|    n_updates            | 9880        |
|    policy_gradient_loss | -0.00548    |
|    std                  | 0.536       |
|    value_loss           | 0.0171      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2025000, episode_reward=3.10 +/- 2.73
Episode length: 234.60 +/- 55.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 235      |
|    mean_reward     | 3.1      |
| time/              |          |
|    total_timesteps | 2025000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 639     |
|    iterations      | 989     |
|    time_elapsed    | 3166    |
|    total_timesteps | 2025472 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2026000, episode_reward=2.05 +/- 2.73
Episode length: 262.60 +/- 45.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 263          |
|    mean_reward          | 2.05         |
| time/                   |              |
|    total_timesteps      | 2026000      |
| train/                  |              |
|    approx_kl            | 0.0041653374 |
|    clip_fraction        | 0.0319       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.59        |
|    explained_variance   | 0.931        |
|    learning_rate        | 5.43e-05     |
|    loss                 | -0.00432     |
|    n_updates            | 9890         |
|    policy_gradient_loss | -0.0049      |
|    std                  | 0.535        |
|    value_loss           | 0.021        |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2027000, episode_reward=1.70 +/- 2.97
Episode length: 258.80 +/- 50.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 259      |
|    mean_reward     | 1.7      |
| time/              |          |
|    total_timesteps | 2027000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 639     |
|    iterations      | 990     |
|    time_elapsed    | 3169    |
|    total_timesteps | 2027520 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=2028000, episode_reward=1.89 +/- 2.88
Episode length: 245.80 +/- 66.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 246         |
|    mean_reward          | 1.89        |
| time/                   |             |
|    total_timesteps      | 2028000     |
| train/                  |             |
|    approx_kl            | 0.005674105 |
|    clip_fraction        | 0.0385      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.59       |
|    explained_variance   | 0.899       |
|    learning_rate        | 5.43e-05    |
|    loss                 | -0.0069     |
|    n_updates            | 9900        |
|    policy_gradient_loss | -0.00311    |
|    std                  | 0.535       |
|    value_loss           | 0.0643      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2029000, episode_reward=1.67 +/- 3.07
Episode length: 251.60 +/- 59.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 252      |
|    mean_reward     | 1.67     |
| time/              |          |
|    total_timesteps | 2029000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 639     |
|    iterations      | 991     |
|    time_elapsed    | 3172    |
|    total_timesteps | 2029568 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=2030000, episode_reward=1.75 +/- 3.00
Episode length: 253.60 +/- 57.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 254          |
|    mean_reward          | 1.75         |
| time/                   |              |
|    total_timesteps      | 2030000      |
| train/                  |              |
|    approx_kl            | 0.0052160798 |
|    clip_fraction        | 0.0504       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.59        |
|    explained_variance   | 0.908        |
|    learning_rate        | 5.44e-05     |
|    loss                 | 0.00269      |
|    n_updates            | 9910         |
|    policy_gradient_loss | -0.00676     |
|    std                  | 0.535        |
|    value_loss           | 0.0115       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2031000, episode_reward=4.26 +/- 2.03
Episode length: 205.60 +/- 58.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | 4.26     |
| time/              |          |
|    total_timesteps | 2031000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 639     |
|    iterations      | 992     |
|    time_elapsed    | 3175    |
|    total_timesteps | 2031616 |
--------------------------------
box reached target
Eval num_timesteps=2032000, episode_reward=0.44 +/- 2.54
Episode length: 272.80 +/- 54.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 273         |
|    mean_reward          | 0.445       |
| time/                   |             |
|    total_timesteps      | 2032000     |
| train/                  |             |
|    approx_kl            | 0.004858787 |
|    clip_fraction        | 0.0333      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.59       |
|    explained_variance   | 0.894       |
|    learning_rate        | 5.44e-05    |
|    loss                 | -0.0034     |
|    n_updates            | 9920        |
|    policy_gradient_loss | -0.00618    |
|    std                  | 0.535       |
|    value_loss           | 0.0391      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2033000, episode_reward=4.02 +/- 2.69
Episode length: 209.00 +/- 48.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 209      |
|    mean_reward     | 4.02     |
| time/              |          |
|    total_timesteps | 2033000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 639     |
|    iterations      | 993     |
|    time_elapsed    | 3178    |
|    total_timesteps | 2033664 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2034000, episode_reward=1.98 +/- 2.67
Episode length: 248.80 +/- 64.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 249          |
|    mean_reward          | 1.98         |
| time/                   |              |
|    total_timesteps      | 2034000      |
| train/                  |              |
|    approx_kl            | 0.0046015037 |
|    clip_fraction        | 0.0479       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.59        |
|    explained_variance   | 0.95         |
|    learning_rate        | 5.44e-05     |
|    loss                 | -0.0154      |
|    n_updates            | 9930         |
|    policy_gradient_loss | -0.00615     |
|    std                  | 0.536        |
|    value_loss           | 0.0137       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2035000, episode_reward=3.07 +/- 2.80
Episode length: 227.80 +/- 58.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 228      |
|    mean_reward     | 3.07     |
| time/              |          |
|    total_timesteps | 2035000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 639     |
|    iterations      | 994     |
|    time_elapsed    | 3181    |
|    total_timesteps | 2035712 |
--------------------------------
box reached target
Eval num_timesteps=2036000, episode_reward=0.33 +/- 2.57
Episode length: 278.80 +/- 42.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 279          |
|    mean_reward          | 0.327        |
| time/                   |              |
|    total_timesteps      | 2036000      |
| train/                  |              |
|    approx_kl            | 0.0047221356 |
|    clip_fraction        | 0.0297       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.59        |
|    explained_variance   | 0.874        |
|    learning_rate        | 5.44e-05     |
|    loss                 | -0.0109      |
|    n_updates            | 9940         |
|    policy_gradient_loss | -0.00415     |
|    std                  | 0.535        |
|    value_loss           | 0.0399       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2037000, episode_reward=4.13 +/- 2.58
Episode length: 220.20 +/- 55.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 220      |
|    mean_reward     | 4.13     |
| time/              |          |
|    total_timesteps | 2037000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 639     |
|    iterations      | 995     |
|    time_elapsed    | 3184    |
|    total_timesteps | 2037760 |
--------------------------------
Eval num_timesteps=2038000, episode_reward=-0.14 +/- 0.76
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.142      |
| time/                   |             |
|    total_timesteps      | 2038000     |
| train/                  |             |
|    approx_kl            | 0.004833661 |
|    clip_fraction        | 0.0392      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.59       |
|    explained_variance   | 0.951       |
|    learning_rate        | 5.45e-05    |
|    loss                 | 0.0331      |
|    n_updates            | 9950        |
|    policy_gradient_loss | -0.0066     |
|    std                  | 0.536       |
|    value_loss           | 0.0197      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2039000, episode_reward=1.99 +/- 2.73
Episode length: 261.80 +/- 47.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 262      |
|    mean_reward     | 1.99     |
| time/              |          |
|    total_timesteps | 2039000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 639     |
|    iterations      | 996     |
|    time_elapsed    | 3187    |
|    total_timesteps | 2039808 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2040000, episode_reward=2.73 +/- 2.97
Episode length: 223.80 +/- 62.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 224          |
|    mean_reward          | 2.73         |
| time/                   |              |
|    total_timesteps      | 2040000      |
| train/                  |              |
|    approx_kl            | 0.0061358344 |
|    clip_fraction        | 0.0621       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.59        |
|    explained_variance   | 0.956        |
|    learning_rate        | 5.45e-05     |
|    loss                 | 0.0122       |
|    n_updates            | 9960         |
|    policy_gradient_loss | -0.0086      |
|    std                  | 0.536        |
|    value_loss           | 0.0134       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2041000, episode_reward=2.76 +/- 2.97
Episode length: 229.60 +/- 59.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 230      |
|    mean_reward     | 2.76     |
| time/              |          |
|    total_timesteps | 2041000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 639     |
|    iterations      | 997     |
|    time_elapsed    | 3190    |
|    total_timesteps | 2041856 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=2042000, episode_reward=1.77 +/- 2.92
Episode length: 253.00 +/- 57.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 253         |
|    mean_reward          | 1.77        |
| time/                   |             |
|    total_timesteps      | 2042000     |
| train/                  |             |
|    approx_kl            | 0.007251263 |
|    clip_fraction        | 0.0622      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.59       |
|    explained_variance   | 0.973       |
|    learning_rate        | 5.45e-05    |
|    loss                 | -0.00674    |
|    n_updates            | 9970        |
|    policy_gradient_loss | -0.00562    |
|    std                  | 0.536       |
|    value_loss           | 0.00932     |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2043000, episode_reward=4.24 +/- 2.09
Episode length: 200.00 +/- 52.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 200      |
|    mean_reward     | 4.24     |
| time/              |          |
|    total_timesteps | 2043000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 640     |
|    iterations      | 998     |
|    time_elapsed    | 3193    |
|    total_timesteps | 2043904 |
--------------------------------
box reached target
Eval num_timesteps=2044000, episode_reward=1.00 +/- 2.08
Episode length: 276.40 +/- 47.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 276          |
|    mean_reward          | 1            |
| time/                   |              |
|    total_timesteps      | 2044000      |
| train/                  |              |
|    approx_kl            | 0.0049998863 |
|    clip_fraction        | 0.0364       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.59        |
|    explained_variance   | 0.877        |
|    learning_rate        | 5.45e-05     |
|    loss                 | -0.0254      |
|    n_updates            | 9980         |
|    policy_gradient_loss | -0.00575     |
|    std                  | 0.534        |
|    value_loss           | 0.0361       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2045000, episode_reward=4.12 +/- 2.58
Episode length: 207.60 +/- 59.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 208      |
|    mean_reward     | 4.12     |
| time/              |          |
|    total_timesteps | 2045000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 640     |
|    iterations      | 999     |
|    time_elapsed    | 3196    |
|    total_timesteps | 2045952 |
--------------------------------
box reached target
Eval num_timesteps=2046000, episode_reward=0.45 +/- 2.44
Episode length: 277.20 +/- 45.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 277         |
|    mean_reward          | 0.446       |
| time/                   |             |
|    total_timesteps      | 2046000     |
| train/                  |             |
|    approx_kl            | 0.004035364 |
|    clip_fraction        | 0.0237      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.58       |
|    explained_variance   | 0.963       |
|    learning_rate        | 5.46e-05    |
|    loss                 | -0.00202    |
|    n_updates            | 9990        |
|    policy_gradient_loss | -0.00395    |
|    std                  | 0.533       |
|    value_loss           | 0.0156      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2047000, episode_reward=3.25 +/- 2.82
Episode length: 239.40 +/- 57.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 239      |
|    mean_reward     | 3.25     |
| time/              |          |
|    total_timesteps | 2047000  |
---------------------------------
box reached target
Eval num_timesteps=2048000, episode_reward=0.60 +/- 2.34
Episode length: 269.20 +/- 61.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 269      |
|    mean_reward     | 0.599    |
| time/              |          |
|    total_timesteps | 2048000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 640     |
|    iterations      | 1000    |
|    time_elapsed    | 3199    |
|    total_timesteps | 2048000 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2049000, episode_reward=0.80 +/- 2.40
Episode length: 279.80 +/- 40.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 280          |
|    mean_reward          | 0.802        |
| time/                   |              |
|    total_timesteps      | 2049000      |
| train/                  |              |
|    approx_kl            | 0.0053274813 |
|    clip_fraction        | 0.0438       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.58        |
|    explained_variance   | 0.903        |
|    learning_rate        | 5.46e-05     |
|    loss                 | -0.0157      |
|    n_updates            | 10000        |
|    policy_gradient_loss | -0.00817     |
|    std                  | 0.536        |
|    value_loss           | 0.0117       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2050000, episode_reward=2.97 +/- 2.91
Episode length: 213.80 +/- 71.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 214      |
|    mean_reward     | 2.97     |
| time/              |          |
|    total_timesteps | 2050000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 640     |
|    iterations      | 1001    |
|    time_elapsed    | 3203    |
|    total_timesteps | 2050048 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2051000, episode_reward=2.94 +/- 2.88
Episode length: 224.60 +/- 64.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 225         |
|    mean_reward          | 2.94        |
| time/                   |             |
|    total_timesteps      | 2051000     |
| train/                  |             |
|    approx_kl            | 0.008627577 |
|    clip_fraction        | 0.0666      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.59       |
|    explained_variance   | 0.869       |
|    learning_rate        | 5.46e-05    |
|    loss                 | 0.0528      |
|    n_updates            | 10010       |
|    policy_gradient_loss | -0.00785    |
|    std                  | 0.536       |
|    value_loss           | 0.046       |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2052000, episode_reward=3.17 +/- 2.89
Episode length: 234.40 +/- 58.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 234      |
|    mean_reward     | 3.17     |
| time/              |          |
|    total_timesteps | 2052000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 640     |
|    iterations      | 1002    |
|    time_elapsed    | 3205    |
|    total_timesteps | 2052096 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2053000, episode_reward=4.06 +/- 2.30
Episode length: 193.20 +/- 56.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 193         |
|    mean_reward          | 4.06        |
| time/                   |             |
|    total_timesteps      | 2053000     |
| train/                  |             |
|    approx_kl            | 0.004249097 |
|    clip_fraction        | 0.0427      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.59       |
|    explained_variance   | 0.941       |
|    learning_rate        | 5.46e-05    |
|    loss                 | -0.00863    |
|    n_updates            | 10020       |
|    policy_gradient_loss | -0.00436    |
|    std                  | 0.534       |
|    value_loss           | 0.0186      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2054000, episode_reward=2.99 +/- 2.90
Episode length: 239.00 +/- 52.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 239      |
|    mean_reward     | 2.99     |
| time/              |          |
|    total_timesteps | 2054000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 640     |
|    iterations      | 1003    |
|    time_elapsed    | 3208    |
|    total_timesteps | 2054144 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2055000, episode_reward=4.04 +/- 2.52
Episode length: 207.00 +/- 53.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 207          |
|    mean_reward          | 4.04         |
| time/                   |              |
|    total_timesteps      | 2055000      |
| train/                  |              |
|    approx_kl            | 0.0074186604 |
|    clip_fraction        | 0.0608       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.58        |
|    explained_variance   | 0.953        |
|    learning_rate        | 5.46e-05     |
|    loss                 | -0.00101     |
|    n_updates            | 10030        |
|    policy_gradient_loss | -0.00448     |
|    std                  | 0.533        |
|    value_loss           | 0.0191       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2056000, episode_reward=1.58 +/- 3.15
Episode length: 266.00 +/- 43.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 266      |
|    mean_reward     | 1.58     |
| time/              |          |
|    total_timesteps | 2056000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 640     |
|    iterations      | 1004    |
|    time_elapsed    | 3211    |
|    total_timesteps | 2056192 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=2057000, episode_reward=2.05 +/- 2.72
Episode length: 249.00 +/- 65.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 249          |
|    mean_reward          | 2.05         |
| time/                   |              |
|    total_timesteps      | 2057000      |
| train/                  |              |
|    approx_kl            | 0.0035258052 |
|    clip_fraction        | 0.0318       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.58        |
|    explained_variance   | 0.903        |
|    learning_rate        | 5.47e-05     |
|    loss                 | -0.0263      |
|    n_updates            | 10040        |
|    policy_gradient_loss | -0.00276     |
|    std                  | 0.532        |
|    value_loss           | 0.0574       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2058000, episode_reward=3.42 +/- 2.31
Episode length: 236.80 +/- 51.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 237      |
|    mean_reward     | 3.42     |
| time/              |          |
|    total_timesteps | 2058000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 640     |
|    iterations      | 1005    |
|    time_elapsed    | 3214    |
|    total_timesteps | 2058240 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2059000, episode_reward=2.91 +/- 2.94
Episode length: 232.60 +/- 57.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 233          |
|    mean_reward          | 2.91         |
| time/                   |              |
|    total_timesteps      | 2059000      |
| train/                  |              |
|    approx_kl            | 0.0055559147 |
|    clip_fraction        | 0.0423       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.57        |
|    explained_variance   | 0.733        |
|    learning_rate        | 5.47e-05     |
|    loss                 | 0.0421       |
|    n_updates            | 10050        |
|    policy_gradient_loss | -0.00435     |
|    std                  | 0.532        |
|    value_loss           | 0.0545       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2060000, episode_reward=3.99 +/- 2.50
Episode length: 201.40 +/- 50.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 201      |
|    mean_reward     | 3.99     |
| time/              |          |
|    total_timesteps | 2060000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 640     |
|    iterations      | 1006    |
|    time_elapsed    | 3217    |
|    total_timesteps | 2060288 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2061000, episode_reward=0.89 +/- 2.35
Episode length: 281.60 +/- 36.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 282          |
|    mean_reward          | 0.89         |
| time/                   |              |
|    total_timesteps      | 2061000      |
| train/                  |              |
|    approx_kl            | 0.0056582415 |
|    clip_fraction        | 0.0408       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.57        |
|    explained_variance   | 0.879        |
|    learning_rate        | 5.47e-05     |
|    loss                 | 0.00296      |
|    n_updates            | 10060        |
|    policy_gradient_loss | -0.00536     |
|    std                  | 0.531        |
|    value_loss           | 0.0282       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2062000, episode_reward=1.46 +/- 3.20
Episode length: 256.80 +/- 52.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 257      |
|    mean_reward     | 1.46     |
| time/              |          |
|    total_timesteps | 2062000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 640     |
|    iterations      | 1007    |
|    time_elapsed    | 3220    |
|    total_timesteps | 2062336 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2063000, episode_reward=2.74 +/- 3.26
Episode length: 231.80 +/- 56.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 232          |
|    mean_reward          | 2.74         |
| time/                   |              |
|    total_timesteps      | 2063000      |
| train/                  |              |
|    approx_kl            | 0.0053093918 |
|    clip_fraction        | 0.05         |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.57        |
|    explained_variance   | 0.765        |
|    learning_rate        | 5.47e-05     |
|    loss                 | 0.0294       |
|    n_updates            | 10070        |
|    policy_gradient_loss | -0.00496     |
|    std                  | 0.53         |
|    value_loss           | 0.0813       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2064000, episode_reward=2.76 +/- 3.05
Episode length: 226.60 +/- 61.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 227      |
|    mean_reward     | 2.76     |
| time/              |          |
|    total_timesteps | 2064000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 640     |
|    iterations      | 1008    |
|    time_elapsed    | 3223    |
|    total_timesteps | 2064384 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2065000, episode_reward=3.05 +/- 2.94
Episode length: 239.60 +/- 57.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 240          |
|    mean_reward          | 3.05         |
| time/                   |              |
|    total_timesteps      | 2065000      |
| train/                  |              |
|    approx_kl            | 0.0040003723 |
|    clip_fraction        | 0.0295       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.57        |
|    explained_variance   | 0.853        |
|    learning_rate        | 5.48e-05     |
|    loss                 | 0.00974      |
|    n_updates            | 10080        |
|    policy_gradient_loss | -0.00532     |
|    std                  | 0.531        |
|    value_loss           | 0.081        |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2066000, episode_reward=2.83 +/- 3.13
Episode length: 237.80 +/- 53.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 238      |
|    mean_reward     | 2.83     |
| time/              |          |
|    total_timesteps | 2066000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 640     |
|    iterations      | 1009    |
|    time_elapsed    | 3226    |
|    total_timesteps | 2066432 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2067000, episode_reward=0.61 +/- 2.40
Episode length: 276.40 +/- 47.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 276          |
|    mean_reward          | 0.613        |
| time/                   |              |
|    total_timesteps      | 2067000      |
| train/                  |              |
|    approx_kl            | 0.0052285027 |
|    clip_fraction        | 0.0479       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.57        |
|    explained_variance   | 0.952        |
|    learning_rate        | 5.48e-05     |
|    loss                 | 0.00159      |
|    n_updates            | 10090        |
|    policy_gradient_loss | -0.00664     |
|    std                  | 0.531        |
|    value_loss           | 0.0288       |
------------------------------------------
box reached target
Eval num_timesteps=2068000, episode_reward=0.59 +/- 2.42
Episode length: 275.00 +/- 50.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 275      |
|    mean_reward     | 0.589    |
| time/              |          |
|    total_timesteps | 2068000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 640     |
|    iterations      | 1010    |
|    time_elapsed    | 3229    |
|    total_timesteps | 2068480 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2069000, episode_reward=2.87 +/- 2.92
Episode length: 226.20 +/- 60.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 226         |
|    mean_reward          | 2.87        |
| time/                   |             |
|    total_timesteps      | 2069000     |
| train/                  |             |
|    approx_kl            | 0.006609641 |
|    clip_fraction        | 0.0478      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.58       |
|    explained_variance   | 0.944       |
|    learning_rate        | 5.48e-05    |
|    loss                 | 0.00842     |
|    n_updates            | 10100       |
|    policy_gradient_loss | -0.00697    |
|    std                  | 0.534       |
|    value_loss           | 0.0144      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=2070000, episode_reward=-0.76 +/- 0.70
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.76    |
| time/              |          |
|    total_timesteps | 2070000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 640     |
|    iterations      | 1011    |
|    time_elapsed    | 3232    |
|    total_timesteps | 2070528 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=2071000, episode_reward=1.50 +/- 3.06
Episode length: 245.00 +/- 68.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 245         |
|    mean_reward          | 1.5         |
| time/                   |             |
|    total_timesteps      | 2071000     |
| train/                  |             |
|    approx_kl            | 0.006259555 |
|    clip_fraction        | 0.0478      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.58       |
|    explained_variance   | 0.827       |
|    learning_rate        | 5.48e-05    |
|    loss                 | 0.0167      |
|    n_updates            | 10110       |
|    policy_gradient_loss | -0.00632    |
|    std                  | 0.534       |
|    value_loss           | 0.0507      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2072000, episode_reward=2.72 +/- 3.20
Episode length: 238.20 +/- 54.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 238      |
|    mean_reward     | 2.72     |
| time/              |          |
|    total_timesteps | 2072000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 640     |
|    iterations      | 1012    |
|    time_elapsed    | 3235    |
|    total_timesteps | 2072576 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2073000, episode_reward=4.25 +/- 2.17
Episode length: 206.60 +/- 51.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 207          |
|    mean_reward          | 4.25         |
| time/                   |              |
|    total_timesteps      | 2073000      |
| train/                  |              |
|    approx_kl            | 0.0056706723 |
|    clip_fraction        | 0.0415       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.58        |
|    explained_variance   | 0.912        |
|    learning_rate        | 5.49e-05     |
|    loss                 | -0.00754     |
|    n_updates            | 10120        |
|    policy_gradient_loss | -0.00285     |
|    std                  | 0.534        |
|    value_loss           | 0.0236       |
------------------------------------------
box reached target
Eval num_timesteps=2074000, episode_reward=0.77 +/- 2.37
Episode length: 266.40 +/- 67.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 266      |
|    mean_reward     | 0.767    |
| time/              |          |
|    total_timesteps | 2074000  |
---------------------------------
box reached target
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 640     |
|    iterations      | 1013    |
|    time_elapsed    | 3238    |
|    total_timesteps | 2074624 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2075000, episode_reward=1.85 +/- 2.76
Episode length: 240.20 +/- 73.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 240          |
|    mean_reward          | 1.85         |
| time/                   |              |
|    total_timesteps      | 2075000      |
| train/                  |              |
|    approx_kl            | 0.0059006503 |
|    clip_fraction        | 0.0617       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.58        |
|    explained_variance   | 0.924        |
|    learning_rate        | 5.49e-05     |
|    loss                 | -0.00763     |
|    n_updates            | 10130        |
|    policy_gradient_loss | -0.00815     |
|    std                  | 0.532        |
|    value_loss           | 0.0435       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2076000, episode_reward=1.32 +/- 3.14
Episode length: 252.20 +/- 61.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 252      |
|    mean_reward     | 1.32     |
| time/              |          |
|    total_timesteps | 2076000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 640     |
|    iterations      | 1014    |
|    time_elapsed    | 3241    |
|    total_timesteps | 2076672 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=2077000, episode_reward=-0.69 +/- 0.70
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.691      |
| time/                   |             |
|    total_timesteps      | 2077000     |
| train/                  |             |
|    approx_kl            | 0.005600089 |
|    clip_fraction        | 0.0595      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.58       |
|    explained_variance   | 0.867       |
|    learning_rate        | 5.49e-05    |
|    loss                 | -0.00535    |
|    n_updates            | 10140       |
|    policy_gradient_loss | -0.0058     |
|    std                  | 0.534       |
|    value_loss           | 0.0863      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2078000, episode_reward=2.05 +/- 2.77
Episode length: 257.80 +/- 51.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 258      |
|    mean_reward     | 2.05     |
| time/              |          |
|    total_timesteps | 2078000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 640     |
|    iterations      | 1015    |
|    time_elapsed    | 3244    |
|    total_timesteps | 2078720 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=2079000, episode_reward=1.64 +/- 2.99
Episode length: 255.60 +/- 54.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 256          |
|    mean_reward          | 1.64         |
| time/                   |              |
|    total_timesteps      | 2079000      |
| train/                  |              |
|    approx_kl            | 0.0058459896 |
|    clip_fraction        | 0.0615       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.58        |
|    explained_variance   | 0.91         |
|    learning_rate        | 5.49e-05     |
|    loss                 | -0.0096      |
|    n_updates            | 10150        |
|    policy_gradient_loss | -0.00751     |
|    std                  | 0.536        |
|    value_loss           | 0.0202       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=2080000, episode_reward=0.39 +/- 2.50
Episode length: 279.80 +/- 40.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 280      |
|    mean_reward     | 0.391    |
| time/              |          |
|    total_timesteps | 2080000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 640     |
|    iterations      | 1016    |
|    time_elapsed    | 3247    |
|    total_timesteps | 2080768 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2081000, episode_reward=2.68 +/- 3.09
Episode length: 218.60 +/- 67.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 219          |
|    mean_reward          | 2.68         |
| time/                   |              |
|    total_timesteps      | 2081000      |
| train/                  |              |
|    approx_kl            | 0.0049091396 |
|    clip_fraction        | 0.0421       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.59        |
|    explained_variance   | 0.951        |
|    learning_rate        | 5.5e-05      |
|    loss                 | -0.0196      |
|    n_updates            | 10160        |
|    policy_gradient_loss | -0.00347     |
|    std                  | 0.537        |
|    value_loss           | 0.0156       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2082000, episode_reward=2.82 +/- 3.07
Episode length: 232.00 +/- 56.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 232      |
|    mean_reward     | 2.82     |
| time/              |          |
|    total_timesteps | 2082000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 640     |
|    iterations      | 1017    |
|    time_elapsed    | 3250    |
|    total_timesteps | 2082816 |
--------------------------------
box reached target
Eval num_timesteps=2083000, episode_reward=0.65 +/- 2.39
Episode length: 275.40 +/- 49.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 275         |
|    mean_reward          | 0.654       |
| time/                   |             |
|    total_timesteps      | 2083000     |
| train/                  |             |
|    approx_kl            | 0.004242083 |
|    clip_fraction        | 0.0355      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.59       |
|    explained_variance   | 0.919       |
|    learning_rate        | 5.5e-05     |
|    loss                 | 0.00207     |
|    n_updates            | 10170       |
|    policy_gradient_loss | -0.00409    |
|    std                  | 0.537       |
|    value_loss           | 0.0207      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2084000, episode_reward=1.95 +/- 2.88
Episode length: 267.80 +/- 42.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 268      |
|    mean_reward     | 1.95     |
| time/              |          |
|    total_timesteps | 2084000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 640     |
|    iterations      | 1018    |
|    time_elapsed    | 3253    |
|    total_timesteps | 2084864 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=2085000, episode_reward=1.72 +/- 2.89
Episode length: 251.40 +/- 62.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 251         |
|    mean_reward          | 1.72        |
| time/                   |             |
|    total_timesteps      | 2085000     |
| train/                  |             |
|    approx_kl            | 0.004567609 |
|    clip_fraction        | 0.0394      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.59       |
|    explained_variance   | 0.942       |
|    learning_rate        | 5.5e-05     |
|    loss                 | -0.0263     |
|    n_updates            | 10180       |
|    policy_gradient_loss | -0.0045     |
|    std                  | 0.537       |
|    value_loss           | 0.0264      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=2086000, episode_reward=1.84 +/- 2.90
Episode length: 247.20 +/- 65.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 247      |
|    mean_reward     | 1.84     |
| time/              |          |
|    total_timesteps | 2086000  |
---------------------------------
box reached target
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 640     |
|    iterations      | 1019    |
|    time_elapsed    | 3257    |
|    total_timesteps | 2086912 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=2087000, episode_reward=1.52 +/- 3.13
Episode length: 255.20 +/- 54.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 255          |
|    mean_reward          | 1.52         |
| time/                   |              |
|    total_timesteps      | 2087000      |
| train/                  |              |
|    approx_kl            | 0.0055219727 |
|    clip_fraction        | 0.0376       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.6         |
|    explained_variance   | 0.903        |
|    learning_rate        | 5.5e-05      |
|    loss                 | -0.000344    |
|    n_updates            | 10190        |
|    policy_gradient_loss | -0.00685     |
|    std                  | 0.538        |
|    value_loss           | 0.0276       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2088000, episode_reward=2.41 +/- 2.31
Episode length: 253.60 +/- 56.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 254      |
|    mean_reward     | 2.41     |
| time/              |          |
|    total_timesteps | 2088000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 640     |
|    iterations      | 1020    |
|    time_elapsed    | 3260    |
|    total_timesteps | 2088960 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=2089000, episode_reward=1.53 +/- 3.10
Episode length: 242.40 +/- 71.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 242          |
|    mean_reward          | 1.53         |
| time/                   |              |
|    total_timesteps      | 2089000      |
| train/                  |              |
|    approx_kl            | 0.0038607335 |
|    clip_fraction        | 0.031        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.59        |
|    explained_variance   | 0.975        |
|    learning_rate        | 5.51e-05     |
|    loss                 | -0.0143      |
|    n_updates            | 10200        |
|    policy_gradient_loss | -0.00455     |
|    std                  | 0.536        |
|    value_loss           | 0.0101       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2090000, episode_reward=1.96 +/- 2.73
Episode length: 251.60 +/- 59.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 252      |
|    mean_reward     | 1.96     |
| time/              |          |
|    total_timesteps | 2090000  |
---------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2091000, episode_reward=5.29 +/- 0.16
Episode length: 191.00 +/- 27.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 191      |
|    mean_reward     | 5.29     |
| time/              |          |
|    total_timesteps | 2091000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 640     |
|    iterations      | 1021    |
|    time_elapsed    | 3263    |
|    total_timesteps | 2091008 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2092000, episode_reward=1.71 +/- 2.84
Episode length: 245.40 +/- 67.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 245          |
|    mean_reward          | 1.71         |
| time/                   |              |
|    total_timesteps      | 2092000      |
| train/                  |              |
|    approx_kl            | 0.0056431843 |
|    clip_fraction        | 0.0389       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.59        |
|    explained_variance   | 0.929        |
|    learning_rate        | 5.51e-05     |
|    loss                 | -0.00534     |
|    n_updates            | 10210        |
|    policy_gradient_loss | -0.00362     |
|    std                  | 0.537        |
|    value_loss           | 0.0352       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=2093000, episode_reward=0.49 +/- 2.56
Episode length: 282.80 +/- 34.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 283      |
|    mean_reward     | 0.485    |
| time/              |          |
|    total_timesteps | 2093000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 640     |
|    iterations      | 1022    |
|    time_elapsed    | 3266    |
|    total_timesteps | 2093056 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2094000, episode_reward=1.68 +/- 2.87
Episode length: 259.40 +/- 50.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 259         |
|    mean_reward          | 1.68        |
| time/                   |             |
|    total_timesteps      | 2094000     |
| train/                  |             |
|    approx_kl            | 0.004951917 |
|    clip_fraction        | 0.0396      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.59       |
|    explained_variance   | 0.929       |
|    learning_rate        | 5.51e-05    |
|    loss                 | 0.00664     |
|    n_updates            | 10220       |
|    policy_gradient_loss | -0.00549    |
|    std                  | 0.536       |
|    value_loss           | 0.0213      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2095000, episode_reward=5.26 +/- 0.13
Episode length: 183.20 +/- 27.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 183      |
|    mean_reward     | 5.26     |
| time/              |          |
|    total_timesteps | 2095000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 640     |
|    iterations      | 1023    |
|    time_elapsed    | 3269    |
|    total_timesteps | 2095104 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2096000, episode_reward=1.84 +/- 2.85
Episode length: 249.80 +/- 61.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 250          |
|    mean_reward          | 1.84         |
| time/                   |              |
|    total_timesteps      | 2096000      |
| train/                  |              |
|    approx_kl            | 0.0058940295 |
|    clip_fraction        | 0.0466       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.59        |
|    explained_variance   | 0.913        |
|    learning_rate        | 5.51e-05     |
|    loss                 | 0.00461      |
|    n_updates            | 10230        |
|    policy_gradient_loss | -0.00552     |
|    std                  | 0.536        |
|    value_loss           | 0.0398       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2097000, episode_reward=0.38 +/- 2.45
Episode length: 274.60 +/- 50.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 275      |
|    mean_reward     | 0.384    |
| time/              |          |
|    total_timesteps | 2097000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 640     |
|    iterations      | 1024    |
|    time_elapsed    | 3272    |
|    total_timesteps | 2097152 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2098000, episode_reward=1.80 +/- 2.84
Episode length: 265.60 +/- 49.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 266         |
|    mean_reward          | 1.8         |
| time/                   |             |
|    total_timesteps      | 2098000     |
| train/                  |             |
|    approx_kl            | 0.004778897 |
|    clip_fraction        | 0.0353      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.59       |
|    explained_variance   | 0.931       |
|    learning_rate        | 5.52e-05    |
|    loss                 | -0.0156     |
|    n_updates            | 10240       |
|    policy_gradient_loss | -0.00553    |
|    std                  | 0.536       |
|    value_loss           | 0.0258      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2099000, episode_reward=2.77 +/- 3.29
Episode length: 236.20 +/- 61.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 236      |
|    mean_reward     | 2.77     |
| time/              |          |
|    total_timesteps | 2099000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 640     |
|    iterations      | 1025    |
|    time_elapsed    | 3275    |
|    total_timesteps | 2099200 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2100000, episode_reward=4.00 +/- 2.40
Episode length: 198.40 +/- 53.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 198         |
|    mean_reward          | 4           |
| time/                   |             |
|    total_timesteps      | 2100000     |
| train/                  |             |
|    approx_kl            | 0.006728505 |
|    clip_fraction        | 0.0559      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.58       |
|    explained_variance   | 0.951       |
|    learning_rate        | 5.52e-05    |
|    loss                 | -0.0206     |
|    n_updates            | 10250       |
|    policy_gradient_loss | -0.00488    |
|    std                  | 0.534       |
|    value_loss           | 0.0236      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2101000, episode_reward=2.73 +/- 3.10
Episode length: 215.80 +/- 72.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 216      |
|    mean_reward     | 2.73     |
| time/              |          |
|    total_timesteps | 2101000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 640     |
|    iterations      | 1026    |
|    time_elapsed    | 3278    |
|    total_timesteps | 2101248 |
--------------------------------
box reached target
Eval num_timesteps=2102000, episode_reward=-0.69 +/- 0.31
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.692      |
| time/                   |             |
|    total_timesteps      | 2102000     |
| train/                  |             |
|    approx_kl            | 0.006052697 |
|    clip_fraction        | 0.0553      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.58       |
|    explained_variance   | 0.893       |
|    learning_rate        | 5.52e-05    |
|    loss                 | 0.00729     |
|    n_updates            | 10260       |
|    policy_gradient_loss | -0.00803    |
|    std                  | 0.534       |
|    value_loss           | 0.0356      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=2103000, episode_reward=1.38 +/- 1.96
Episode length: 277.40 +/- 45.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 277      |
|    mean_reward     | 1.38     |
| time/              |          |
|    total_timesteps | 2103000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 640     |
|    iterations      | 1027    |
|    time_elapsed    | 3281    |
|    total_timesteps | 2103296 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2104000, episode_reward=1.80 +/- 3.06
Episode length: 270.80 +/- 38.37
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 271        |
|    mean_reward          | 1.8        |
| time/                   |            |
|    total_timesteps      | 2104000    |
| train/                  |            |
|    approx_kl            | 0.00612385 |
|    clip_fraction        | 0.0421     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.58      |
|    explained_variance   | 0.956      |
|    learning_rate        | 5.52e-05   |
|    loss                 | -0.000772  |
|    n_updates            | 10270      |
|    policy_gradient_loss | -0.00516   |
|    std                  | 0.533      |
|    value_loss           | 0.0131     |
----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2105000, episode_reward=1.82 +/- 2.85
Episode length: 250.80 +/- 60.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | 1.82     |
| time/              |          |
|    total_timesteps | 2105000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 640     |
|    iterations      | 1028    |
|    time_elapsed    | 3284    |
|    total_timesteps | 2105344 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2106000, episode_reward=1.37 +/- 3.13
Episode length: 247.40 +/- 64.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 247         |
|    mean_reward          | 1.37        |
| time/                   |             |
|    total_timesteps      | 2106000     |
| train/                  |             |
|    approx_kl            | 0.006144283 |
|    clip_fraction        | 0.063       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.58       |
|    explained_variance   | 0.925       |
|    learning_rate        | 5.53e-05    |
|    loss                 | -0.0101     |
|    n_updates            | 10280       |
|    policy_gradient_loss | -0.00655    |
|    std                  | 0.532       |
|    value_loss           | 0.0284      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2107000, episode_reward=2.97 +/- 2.94
Episode length: 237.40 +/- 56.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 237      |
|    mean_reward     | 2.97     |
| time/              |          |
|    total_timesteps | 2107000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 641     |
|    iterations      | 1029    |
|    time_elapsed    | 3287    |
|    total_timesteps | 2107392 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2108000, episode_reward=1.35 +/- 3.09
Episode length: 237.40 +/- 76.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 237          |
|    mean_reward          | 1.35         |
| time/                   |              |
|    total_timesteps      | 2108000      |
| train/                  |              |
|    approx_kl            | 0.0039498005 |
|    clip_fraction        | 0.0354       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.57        |
|    explained_variance   | 0.828        |
|    learning_rate        | 5.53e-05     |
|    loss                 | 0.00415      |
|    n_updates            | 10290        |
|    policy_gradient_loss | -0.00579     |
|    std                  | 0.53         |
|    value_loss           | 0.051        |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2109000, episode_reward=3.02 +/- 2.89
Episode length: 232.60 +/- 58.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 233      |
|    mean_reward     | 3.02     |
| time/              |          |
|    total_timesteps | 2109000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 641     |
|    iterations      | 1030    |
|    time_elapsed    | 3290    |
|    total_timesteps | 2109440 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2110000, episode_reward=1.79 +/- 2.92
Episode length: 249.80 +/- 61.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 250         |
|    mean_reward          | 1.79        |
| time/                   |             |
|    total_timesteps      | 2110000     |
| train/                  |             |
|    approx_kl            | 0.006477977 |
|    clip_fraction        | 0.0599      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.57       |
|    explained_variance   | 0.94        |
|    learning_rate        | 5.53e-05    |
|    loss                 | -0.0246     |
|    n_updates            | 10300       |
|    policy_gradient_loss | -0.00693    |
|    std                  | 0.531       |
|    value_loss           | 0.0155      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2111000, episode_reward=2.84 +/- 3.06
Episode length: 233.00 +/- 58.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 233      |
|    mean_reward     | 2.84     |
| time/              |          |
|    total_timesteps | 2111000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 641     |
|    iterations      | 1031    |
|    time_elapsed    | 3293    |
|    total_timesteps | 2111488 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=2112000, episode_reward=2.31 +/- 2.42
Episode length: 248.00 +/- 66.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 248         |
|    mean_reward          | 2.31        |
| time/                   |             |
|    total_timesteps      | 2112000     |
| train/                  |             |
|    approx_kl            | 0.006105612 |
|    clip_fraction        | 0.0433      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.57       |
|    explained_variance   | 0.816       |
|    learning_rate        | 5.53e-05    |
|    loss                 | -0.0118     |
|    n_updates            | 10310       |
|    policy_gradient_loss | -0.0055     |
|    std                  | 0.531       |
|    value_loss           | 0.0415      |
-----------------------------------------
box reached target
Eval num_timesteps=2113000, episode_reward=0.68 +/- 2.33
Episode length: 281.00 +/- 38.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 281      |
|    mean_reward     | 0.683    |
| time/              |          |
|    total_timesteps | 2113000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 641     |
|    iterations      | 1032    |
|    time_elapsed    | 3296    |
|    total_timesteps | 2113536 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2114000, episode_reward=2.71 +/- 3.03
Episode length: 203.60 +/- 78.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 204         |
|    mean_reward          | 2.71        |
| time/                   |             |
|    total_timesteps      | 2114000     |
| train/                  |             |
|    approx_kl            | 0.005867546 |
|    clip_fraction        | 0.0558      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.57       |
|    explained_variance   | 0.88        |
|    learning_rate        | 5.54e-05    |
|    loss                 | -0.00446    |
|    n_updates            | 10320       |
|    policy_gradient_loss | -0.00873    |
|    std                  | 0.532       |
|    value_loss           | 0.0225      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2115000, episode_reward=4.00 +/- 2.50
Episode length: 190.20 +/- 59.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 190      |
|    mean_reward     | 4        |
| time/              |          |
|    total_timesteps | 2115000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 641     |
|    iterations      | 1033    |
|    time_elapsed    | 3299    |
|    total_timesteps | 2115584 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2116000, episode_reward=2.99 +/- 2.72
Episode length: 222.60 +/- 64.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 223          |
|    mean_reward          | 2.99         |
| time/                   |              |
|    total_timesteps      | 2116000      |
| train/                  |              |
|    approx_kl            | 0.0063421847 |
|    clip_fraction        | 0.0514       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.57        |
|    explained_variance   | 0.949        |
|    learning_rate        | 5.54e-05     |
|    loss                 | -0.00727     |
|    n_updates            | 10330        |
|    policy_gradient_loss | -0.00558     |
|    std                  | 0.532        |
|    value_loss           | 0.0292       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2117000, episode_reward=0.14 +/- 2.49
Episode length: 274.60 +/- 50.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 275      |
|    mean_reward     | 0.141    |
| time/              |          |
|    total_timesteps | 2117000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 641     |
|    iterations      | 1034    |
|    time_elapsed    | 3302    |
|    total_timesteps | 2117632 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=2118000, episode_reward=0.62 +/- 2.57
Episode length: 286.00 +/- 28.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 286        |
|    mean_reward          | 0.622      |
| time/                   |            |
|    total_timesteps      | 2118000    |
| train/                  |            |
|    approx_kl            | 0.00546276 |
|    clip_fraction        | 0.0445     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.57      |
|    explained_variance   | 0.919      |
|    learning_rate        | 5.54e-05   |
|    loss                 | -0.00405   |
|    n_updates            | 10340      |
|    policy_gradient_loss | -0.006     |
|    std                  | 0.531      |
|    value_loss           | 0.0453     |
----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2119000, episode_reward=3.01 +/- 2.71
Episode length: 232.20 +/- 59.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 232      |
|    mean_reward     | 3.01     |
| time/              |          |
|    total_timesteps | 2119000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 641     |
|    iterations      | 1035    |
|    time_elapsed    | 3305    |
|    total_timesteps | 2119680 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2120000, episode_reward=1.76 +/- 2.91
Episode length: 260.00 +/- 54.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 260         |
|    mean_reward          | 1.76        |
| time/                   |             |
|    total_timesteps      | 2120000     |
| train/                  |             |
|    approx_kl            | 0.006404912 |
|    clip_fraction        | 0.0479      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.57       |
|    explained_variance   | 0.952       |
|    learning_rate        | 5.54e-05    |
|    loss                 | -0.0105     |
|    n_updates            | 10350       |
|    policy_gradient_loss | -0.0053     |
|    std                  | 0.531       |
|    value_loss           | 0.018       |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2121000, episode_reward=4.34 +/- 1.89
Episode length: 195.40 +/- 53.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 195      |
|    mean_reward     | 4.34     |
| time/              |          |
|    total_timesteps | 2121000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 641     |
|    iterations      | 1036    |
|    time_elapsed    | 3308    |
|    total_timesteps | 2121728 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2122000, episode_reward=2.09 +/- 2.70
Episode length: 249.80 +/- 61.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 250          |
|    mean_reward          | 2.09         |
| time/                   |              |
|    total_timesteps      | 2122000      |
| train/                  |              |
|    approx_kl            | 0.0052140867 |
|    clip_fraction        | 0.0521       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.57        |
|    explained_variance   | 0.931        |
|    learning_rate        | 5.55e-05     |
|    loss                 | -0.00374     |
|    n_updates            | 10360        |
|    policy_gradient_loss | -0.00539     |
|    std                  | 0.533        |
|    value_loss           | 0.041        |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=2123000, episode_reward=1.46 +/- 3.05
Episode length: 240.40 +/- 73.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 240      |
|    mean_reward     | 1.46     |
| time/              |          |
|    total_timesteps | 2123000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 641     |
|    iterations      | 1037    |
|    time_elapsed    | 3311    |
|    total_timesteps | 2123776 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2124000, episode_reward=4.14 +/- 2.58
Episode length: 216.20 +/- 46.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 216         |
|    mean_reward          | 4.14        |
| time/                   |             |
|    total_timesteps      | 2124000     |
| train/                  |             |
|    approx_kl            | 0.006637799 |
|    clip_fraction        | 0.0647      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.58       |
|    explained_variance   | 0.838       |
|    learning_rate        | 5.55e-05    |
|    loss                 | -0.00561    |
|    n_updates            | 10370       |
|    policy_gradient_loss | -0.00942    |
|    std                  | 0.532       |
|    value_loss           | 0.0546      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=2125000, episode_reward=-0.33 +/- 0.65
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.327   |
| time/              |          |
|    total_timesteps | 2125000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 641     |
|    iterations      | 1038    |
|    time_elapsed    | 3314    |
|    total_timesteps | 2125824 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2126000, episode_reward=2.76 +/- 3.07
Episode length: 220.40 +/- 65.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 220         |
|    mean_reward          | 2.76        |
| time/                   |             |
|    total_timesteps      | 2126000     |
| train/                  |             |
|    approx_kl            | 0.005695909 |
|    clip_fraction        | 0.0526      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.57       |
|    explained_variance   | 0.941       |
|    learning_rate        | 5.55e-05    |
|    loss                 | 0.0299      |
|    n_updates            | 10380       |
|    policy_gradient_loss | -0.00675    |
|    std                  | 0.532       |
|    value_loss           | 0.0315      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2127000, episode_reward=1.18 +/- 2.17
Episode length: 274.80 +/- 50.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 275      |
|    mean_reward     | 1.18     |
| time/              |          |
|    total_timesteps | 2127000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 641     |
|    iterations      | 1039    |
|    time_elapsed    | 3317    |
|    total_timesteps | 2127872 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=2128000, episode_reward=2.04 +/- 2.79
Episode length: 259.20 +/- 58.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 259          |
|    mean_reward          | 2.04         |
| time/                   |              |
|    total_timesteps      | 2128000      |
| train/                  |              |
|    approx_kl            | 0.0063310303 |
|    clip_fraction        | 0.059        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.57        |
|    explained_variance   | 0.851        |
|    learning_rate        | 5.55e-05     |
|    loss                 | 0.0169       |
|    n_updates            | 10390        |
|    policy_gradient_loss | -0.00553     |
|    std                  | 0.531        |
|    value_loss           | 0.0457       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2129000, episode_reward=0.12 +/- 2.47
Episode length: 279.80 +/- 40.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 280      |
|    mean_reward     | 0.119    |
| time/              |          |
|    total_timesteps | 2129000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 641     |
|    iterations      | 1040    |
|    time_elapsed    | 3320    |
|    total_timesteps | 2129920 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2130000, episode_reward=2.74 +/- 3.16
Episode length: 219.40 +/- 66.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 219          |
|    mean_reward          | 2.74         |
| time/                   |              |
|    total_timesteps      | 2130000      |
| train/                  |              |
|    approx_kl            | 0.0039564064 |
|    clip_fraction        | 0.0352       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.57        |
|    explained_variance   | 0.937        |
|    learning_rate        | 5.56e-05     |
|    loss                 | -0.00706     |
|    n_updates            | 10400        |
|    policy_gradient_loss | -0.00434     |
|    std                  | 0.532        |
|    value_loss           | 0.023        |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=2131000, episode_reward=1.39 +/- 3.23
Episode length: 253.40 +/- 57.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 253      |
|    mean_reward     | 1.39     |
| time/              |          |
|    total_timesteps | 2131000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 641     |
|    iterations      | 1041    |
|    time_elapsed    | 3323    |
|    total_timesteps | 2131968 |
--------------------------------
box reached target
Eval num_timesteps=2132000, episode_reward=0.53 +/- 2.43
Episode length: 275.20 +/- 49.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 275         |
|    mean_reward          | 0.528       |
| time/                   |             |
|    total_timesteps      | 2132000     |
| train/                  |             |
|    approx_kl            | 0.005790907 |
|    clip_fraction        | 0.0542      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.58       |
|    explained_variance   | 0.875       |
|    learning_rate        | 5.56e-05    |
|    loss                 | 0.0961      |
|    n_updates            | 10410       |
|    policy_gradient_loss | -0.0051     |
|    std                  | 0.533       |
|    value_loss           | 0.0532      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2133000, episode_reward=2.93 +/- 3.02
Episode length: 237.80 +/- 56.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 238      |
|    mean_reward     | 2.93     |
| time/              |          |
|    total_timesteps | 2133000  |
---------------------------------
box reached target
Eval num_timesteps=2134000, episode_reward=0.45 +/- 2.53
Episode length: 273.00 +/- 54.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 273      |
|    mean_reward     | 0.454    |
| time/              |          |
|    total_timesteps | 2134000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 641     |
|    iterations      | 1042    |
|    time_elapsed    | 3327    |
|    total_timesteps | 2134016 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2135000, episode_reward=2.79 +/- 3.02
Episode length: 242.20 +/- 52.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 242          |
|    mean_reward          | 2.79         |
| time/                   |              |
|    total_timesteps      | 2135000      |
| train/                  |              |
|    approx_kl            | 0.0068793884 |
|    clip_fraction        | 0.0595       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.58        |
|    explained_variance   | 0.843        |
|    learning_rate        | 5.56e-05     |
|    loss                 | -0.000195    |
|    n_updates            | 10420        |
|    policy_gradient_loss | -0.00545     |
|    std                  | 0.533        |
|    value_loss           | 0.0242       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2136000, episode_reward=3.17 +/- 2.65
Episode length: 231.60 +/- 56.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 232      |
|    mean_reward     | 3.17     |
| time/              |          |
|    total_timesteps | 2136000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 641     |
|    iterations      | 1043    |
|    time_elapsed    | 3330    |
|    total_timesteps | 2136064 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=2137000, episode_reward=2.24 +/- 2.69
Episode length: 263.00 +/- 48.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 263          |
|    mean_reward          | 2.24         |
| time/                   |              |
|    total_timesteps      | 2137000      |
| train/                  |              |
|    approx_kl            | 0.0075886976 |
|    clip_fraction        | 0.0789       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.58        |
|    explained_variance   | 0.873        |
|    learning_rate        | 5.56e-05     |
|    loss                 | -0.0145      |
|    n_updates            | 10430        |
|    policy_gradient_loss | -0.00844     |
|    std                  | 0.533        |
|    value_loss           | 0.00967      |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2138000, episode_reward=2.88 +/- 3.03
Episode length: 232.80 +/- 55.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 233      |
|    mean_reward     | 2.88     |
| time/              |          |
|    total_timesteps | 2138000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 641     |
|    iterations      | 1044    |
|    time_elapsed    | 3333    |
|    total_timesteps | 2138112 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=2139000, episode_reward=1.24 +/- 1.97
Episode length: 286.00 +/- 28.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 286         |
|    mean_reward          | 1.24        |
| time/                   |             |
|    total_timesteps      | 2139000     |
| train/                  |             |
|    approx_kl            | 0.005254366 |
|    clip_fraction        | 0.0404      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.58       |
|    explained_variance   | 0.902       |
|    learning_rate        | 5.57e-05    |
|    loss                 | 0.0211      |
|    n_updates            | 10440       |
|    policy_gradient_loss | -0.00435    |
|    std                  | 0.533       |
|    value_loss           | 0.0244      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2140000, episode_reward=2.79 +/- 2.96
Episode length: 248.80 +/- 61.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 249      |
|    mean_reward     | 2.79     |
| time/              |          |
|    total_timesteps | 2140000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 641     |
|    iterations      | 1045    |
|    time_elapsed    | 3336    |
|    total_timesteps | 2140160 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2141000, episode_reward=2.18 +/- 2.42
Episode length: 255.00 +/- 55.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 255          |
|    mean_reward          | 2.18         |
| time/                   |              |
|    total_timesteps      | 2141000      |
| train/                  |              |
|    approx_kl            | 0.0062742475 |
|    clip_fraction        | 0.0453       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.58        |
|    explained_variance   | 0.939        |
|    learning_rate        | 5.57e-05     |
|    loss                 | 0.00993      |
|    n_updates            | 10450        |
|    policy_gradient_loss | -0.00678     |
|    std                  | 0.533        |
|    value_loss           | 0.00962      |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2142000, episode_reward=1.81 +/- 2.91
Episode length: 260.00 +/- 53.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 260      |
|    mean_reward     | 1.81     |
| time/              |          |
|    total_timesteps | 2142000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 641     |
|    iterations      | 1046    |
|    time_elapsed    | 3339    |
|    total_timesteps | 2142208 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2143000, episode_reward=0.58 +/- 2.53
Episode length: 278.00 +/- 44.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 278          |
|    mean_reward          | 0.58         |
| time/                   |              |
|    total_timesteps      | 2143000      |
| train/                  |              |
|    approx_kl            | 0.0065288115 |
|    clip_fraction        | 0.048        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.58        |
|    explained_variance   | 0.811        |
|    learning_rate        | 5.57e-05     |
|    loss                 | 0.0283       |
|    n_updates            | 10460        |
|    policy_gradient_loss | -0.00434     |
|    std                  | 0.533        |
|    value_loss           | 0.067        |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2144000, episode_reward=2.83 +/- 2.98
Episode length: 210.60 +/- 75.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 211      |
|    mean_reward     | 2.83     |
| time/              |          |
|    total_timesteps | 2144000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 641     |
|    iterations      | 1047    |
|    time_elapsed    | 3342    |
|    total_timesteps | 2144256 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2145000, episode_reward=2.07 +/- 2.56
Episode length: 237.40 +/- 76.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 237          |
|    mean_reward          | 2.07         |
| time/                   |              |
|    total_timesteps      | 2145000      |
| train/                  |              |
|    approx_kl            | 0.0068710446 |
|    clip_fraction        | 0.0506       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.58        |
|    explained_variance   | 0.883        |
|    learning_rate        | 5.57e-05     |
|    loss                 | 0.0143       |
|    n_updates            | 10470        |
|    policy_gradient_loss | -0.00804     |
|    std                  | 0.532        |
|    value_loss           | 0.0651       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2146000, episode_reward=2.85 +/- 2.98
Episode length: 235.00 +/- 56.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 235      |
|    mean_reward     | 2.85     |
| time/              |          |
|    total_timesteps | 2146000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 641     |
|    iterations      | 1048    |
|    time_elapsed    | 3345    |
|    total_timesteps | 2146304 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2147000, episode_reward=1.58 +/- 3.06
Episode length: 256.00 +/- 54.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 256         |
|    mean_reward          | 1.58        |
| time/                   |             |
|    total_timesteps      | 2147000     |
| train/                  |             |
|    approx_kl            | 0.003743831 |
|    clip_fraction        | 0.034       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.57       |
|    explained_variance   | 0.845       |
|    learning_rate        | 5.58e-05    |
|    loss                 | 0.00553     |
|    n_updates            | 10480       |
|    policy_gradient_loss | -0.00343    |
|    std                  | 0.531       |
|    value_loss           | 0.0349      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2148000, episode_reward=3.23 +/- 2.84
Episode length: 242.80 +/- 47.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 243      |
|    mean_reward     | 3.23     |
| time/              |          |
|    total_timesteps | 2148000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 641     |
|    iterations      | 1049    |
|    time_elapsed    | 3348    |
|    total_timesteps | 2148352 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2149000, episode_reward=2.81 +/- 3.12
Episode length: 240.40 +/- 54.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 240          |
|    mean_reward          | 2.81         |
| time/                   |              |
|    total_timesteps      | 2149000      |
| train/                  |              |
|    approx_kl            | 0.0055659814 |
|    clip_fraction        | 0.0482       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.57        |
|    explained_variance   | 0.91         |
|    learning_rate        | 5.58e-05     |
|    loss                 | 0.0229       |
|    n_updates            | 10490        |
|    policy_gradient_loss | -0.00577     |
|    std                  | 0.531        |
|    value_loss           | 0.0423       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2150000, episode_reward=1.82 +/- 2.75
Episode length: 254.00 +/- 56.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 254      |
|    mean_reward     | 1.82     |
| time/              |          |
|    total_timesteps | 2150000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 641     |
|    iterations      | 1050    |
|    time_elapsed    | 3351    |
|    total_timesteps | 2150400 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=2151000, episode_reward=0.73 +/- 2.33
Episode length: 279.20 +/- 41.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 279         |
|    mean_reward          | 0.734       |
| time/                   |             |
|    total_timesteps      | 2151000     |
| train/                  |             |
|    approx_kl            | 0.005065877 |
|    clip_fraction        | 0.045       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.57       |
|    explained_variance   | 0.962       |
|    learning_rate        | 5.58e-05    |
|    loss                 | -0.00839    |
|    n_updates            | 10500       |
|    policy_gradient_loss | -0.00796    |
|    std                  | 0.532       |
|    value_loss           | 0.016       |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2152000, episode_reward=1.99 +/- 2.69
Episode length: 254.20 +/- 59.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 254      |
|    mean_reward     | 1.99     |
| time/              |          |
|    total_timesteps | 2152000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 641     |
|    iterations      | 1051    |
|    time_elapsed    | 3354    |
|    total_timesteps | 2152448 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2153000, episode_reward=1.65 +/- 2.97
Episode length: 253.00 +/- 57.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 253         |
|    mean_reward          | 1.65        |
| time/                   |             |
|    total_timesteps      | 2153000     |
| train/                  |             |
|    approx_kl            | 0.005396569 |
|    clip_fraction        | 0.0436      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.57       |
|    explained_variance   | 0.977       |
|    learning_rate        | 5.58e-05    |
|    loss                 | -0.00331    |
|    n_updates            | 10510       |
|    policy_gradient_loss | -0.0067     |
|    std                  | 0.529       |
|    value_loss           | 0.017       |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2154000, episode_reward=2.80 +/- 3.23
Episode length: 249.20 +/- 54.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 249      |
|    mean_reward     | 2.8      |
| time/              |          |
|    total_timesteps | 2154000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 641     |
|    iterations      | 1052    |
|    time_elapsed    | 3357    |
|    total_timesteps | 2154496 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2155000, episode_reward=4.31 +/- 1.90
Episode length: 216.40 +/- 44.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 216         |
|    mean_reward          | 4.31        |
| time/                   |             |
|    total_timesteps      | 2155000     |
| train/                  |             |
|    approx_kl            | 0.004775124 |
|    clip_fraction        | 0.0364      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.56       |
|    explained_variance   | 0.924       |
|    learning_rate        | 5.59e-05    |
|    loss                 | -0.013      |
|    n_updates            | 10520       |
|    policy_gradient_loss | -0.00672    |
|    std                  | 0.529       |
|    value_loss           | 0.0411      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2156000, episode_reward=0.54 +/- 2.46
Episode length: 272.20 +/- 55.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 272      |
|    mean_reward     | 0.538    |
| time/              |          |
|    total_timesteps | 2156000  |
---------------------------------
box reached target
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 641     |
|    iterations      | 1053    |
|    time_elapsed    | 3360    |
|    total_timesteps | 2156544 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=2157000, episode_reward=0.38 +/- 2.51
Episode length: 268.20 +/- 63.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 268          |
|    mean_reward          | 0.381        |
| time/                   |              |
|    total_timesteps      | 2157000      |
| train/                  |              |
|    approx_kl            | 0.0043612635 |
|    clip_fraction        | 0.0276       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.56        |
|    explained_variance   | 0.927        |
|    learning_rate        | 5.59e-05     |
|    loss                 | 0.00587      |
|    n_updates            | 10530        |
|    policy_gradient_loss | -0.00371     |
|    std                  | 0.529        |
|    value_loss           | 0.0272       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=2158000, episode_reward=0.10 +/- 2.58
Episode length: 279.60 +/- 40.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 280      |
|    mean_reward     | 0.101    |
| time/              |          |
|    total_timesteps | 2158000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 641     |
|    iterations      | 1054    |
|    time_elapsed    | 3363    |
|    total_timesteps | 2158592 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2159000, episode_reward=4.04 +/- 2.52
Episode length: 202.40 +/- 52.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 202         |
|    mean_reward          | 4.04        |
| time/                   |             |
|    total_timesteps      | 2159000     |
| train/                  |             |
|    approx_kl            | 0.006079103 |
|    clip_fraction        | 0.054       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.56       |
|    explained_variance   | 0.91        |
|    learning_rate        | 5.59e-05    |
|    loss                 | 0.0123      |
|    n_updates            | 10540       |
|    policy_gradient_loss | -0.00665    |
|    std                  | 0.529       |
|    value_loss           | 0.0299      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2160000, episode_reward=3.05 +/- 2.69
Episode length: 228.00 +/- 58.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 228      |
|    mean_reward     | 3.05     |
| time/              |          |
|    total_timesteps | 2160000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 641     |
|    iterations      | 1055    |
|    time_elapsed    | 3366    |
|    total_timesteps | 2160640 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2161000, episode_reward=2.46 +/- 2.36
Episode length: 257.40 +/- 60.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 257          |
|    mean_reward          | 2.46         |
| time/                   |              |
|    total_timesteps      | 2161000      |
| train/                  |              |
|    approx_kl            | 0.0067522763 |
|    clip_fraction        | 0.0602       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.56        |
|    explained_variance   | 0.883        |
|    learning_rate        | 5.59e-05     |
|    loss                 | -0.0266      |
|    n_updates            | 10550        |
|    policy_gradient_loss | -0.00684     |
|    std                  | 0.529        |
|    value_loss           | 0.025        |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2162000, episode_reward=2.75 +/- 3.03
Episode length: 231.80 +/- 60.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 232      |
|    mean_reward     | 2.75     |
| time/              |          |
|    total_timesteps | 2162000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 641     |
|    iterations      | 1056    |
|    time_elapsed    | 3369    |
|    total_timesteps | 2162688 |
--------------------------------
box reached target
Eval num_timesteps=2163000, episode_reward=1.23 +/- 2.18
Episode length: 288.00 +/- 24.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 1.23        |
| time/                   |             |
|    total_timesteps      | 2163000     |
| train/                  |             |
|    approx_kl            | 0.006742348 |
|    clip_fraction        | 0.0613      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.56       |
|    explained_variance   | 0.893       |
|    learning_rate        | 5.6e-05     |
|    loss                 | -0.0184     |
|    n_updates            | 10560       |
|    policy_gradient_loss | -0.00594    |
|    std                  | 0.529       |
|    value_loss           | 0.0542      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2164000, episode_reward=0.66 +/- 2.57
Episode length: 279.80 +/- 40.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 280      |
|    mean_reward     | 0.658    |
| time/              |          |
|    total_timesteps | 2164000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 641     |
|    iterations      | 1057    |
|    time_elapsed    | 3372    |
|    total_timesteps | 2164736 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2165000, episode_reward=4.20 +/- 2.04
Episode length: 194.00 +/- 54.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 194          |
|    mean_reward          | 4.2          |
| time/                   |              |
|    total_timesteps      | 2165000      |
| train/                  |              |
|    approx_kl            | 0.0055630025 |
|    clip_fraction        | 0.0417       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.56        |
|    explained_variance   | 0.925        |
|    learning_rate        | 5.6e-05      |
|    loss                 | -0.00627     |
|    n_updates            | 10570        |
|    policy_gradient_loss | -0.00546     |
|    std                  | 0.528        |
|    value_loss           | 0.0173       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2166000, episode_reward=4.29 +/- 1.91
Episode length: 204.80 +/- 47.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 205      |
|    mean_reward     | 4.29     |
| time/              |          |
|    total_timesteps | 2166000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 641     |
|    iterations      | 1058    |
|    time_elapsed    | 3375    |
|    total_timesteps | 2166784 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2167000, episode_reward=2.83 +/- 3.10
Episode length: 237.40 +/- 55.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 237          |
|    mean_reward          | 2.83         |
| time/                   |              |
|    total_timesteps      | 2167000      |
| train/                  |              |
|    approx_kl            | 0.0048694285 |
|    clip_fraction        | 0.0422       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.56        |
|    explained_variance   | 0.765        |
|    learning_rate        | 5.6e-05      |
|    loss                 | -0.0313      |
|    n_updates            | 10580        |
|    policy_gradient_loss | -0.00493     |
|    std                  | 0.532        |
|    value_loss           | 0.02         |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=2168000, episode_reward=1.98 +/- 2.77
Episode length: 250.80 +/- 62.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | 1.98     |
| time/              |          |
|    total_timesteps | 2168000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 641     |
|    iterations      | 1059    |
|    time_elapsed    | 3378    |
|    total_timesteps | 2168832 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2169000, episode_reward=4.40 +/- 2.05
Episode length: 221.20 +/- 45.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 221          |
|    mean_reward          | 4.4          |
| time/                   |              |
|    total_timesteps      | 2169000      |
| train/                  |              |
|    approx_kl            | 0.0061529987 |
|    clip_fraction        | 0.0595       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.58        |
|    explained_variance   | 0.689        |
|    learning_rate        | 5.6e-05      |
|    loss                 | 0.00192      |
|    n_updates            | 10590        |
|    policy_gradient_loss | -0.00918     |
|    std                  | 0.535        |
|    value_loss           | 0.0148       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2170000, episode_reward=1.39 +/- 3.14
Episode length: 250.00 +/- 61.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 250      |
|    mean_reward     | 1.39     |
| time/              |          |
|    total_timesteps | 2170000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 641     |
|    iterations      | 1060    |
|    time_elapsed    | 3381    |
|    total_timesteps | 2170880 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=2171000, episode_reward=0.36 +/- 2.43
Episode length: 270.80 +/- 58.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 271         |
|    mean_reward          | 0.358       |
| time/                   |             |
|    total_timesteps      | 2171000     |
| train/                  |             |
|    approx_kl            | 0.005966445 |
|    clip_fraction        | 0.0611      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.58       |
|    explained_variance   | 0.904       |
|    learning_rate        | 5.61e-05    |
|    loss                 | -0.0195     |
|    n_updates            | 10600       |
|    policy_gradient_loss | -0.00604    |
|    std                  | 0.533       |
|    value_loss           | 0.0285      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2172000, episode_reward=3.11 +/- 2.87
Episode length: 226.80 +/- 62.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 227      |
|    mean_reward     | 3.11     |
| time/              |          |
|    total_timesteps | 2172000  |
---------------------------------
box reached target
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 642     |
|    iterations      | 1061    |
|    time_elapsed    | 3384    |
|    total_timesteps | 2172928 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2173000, episode_reward=2.91 +/- 2.96
Episode length: 227.40 +/- 63.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 227          |
|    mean_reward          | 2.91         |
| time/                   |              |
|    total_timesteps      | 2173000      |
| train/                  |              |
|    approx_kl            | 0.0054047913 |
|    clip_fraction        | 0.035        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.57        |
|    explained_variance   | 0.947        |
|    learning_rate        | 5.61e-05     |
|    loss                 | -0.0229      |
|    n_updates            | 10610        |
|    policy_gradient_loss | -0.00614     |
|    std                  | 0.531        |
|    value_loss           | 0.0239       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2174000, episode_reward=1.66 +/- 2.90
Episode length: 239.40 +/- 74.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 239      |
|    mean_reward     | 1.66     |
| time/              |          |
|    total_timesteps | 2174000  |
---------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 642     |
|    iterations      | 1062    |
|    time_elapsed    | 3387    |
|    total_timesteps | 2174976 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=2175000, episode_reward=1.80 +/- 2.79
Episode length: 243.20 +/- 70.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 243          |
|    mean_reward          | 1.8          |
| time/                   |              |
|    total_timesteps      | 2175000      |
| train/                  |              |
|    approx_kl            | 0.0053933365 |
|    clip_fraction        | 0.0354       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.57        |
|    explained_variance   | 0.86         |
|    learning_rate        | 5.61e-05     |
|    loss                 | 0.0119       |
|    n_updates            | 10620        |
|    policy_gradient_loss | -0.00422     |
|    std                  | 0.531        |
|    value_loss           | 0.0624       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2176000, episode_reward=2.71 +/- 3.20
Episode length: 222.00 +/- 64.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 222      |
|    mean_reward     | 2.71     |
| time/              |          |
|    total_timesteps | 2176000  |
---------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2177000, episode_reward=3.08 +/- 2.79
Episode length: 246.40 +/- 44.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 246      |
|    mean_reward     | 3.08     |
| time/              |          |
|    total_timesteps | 2177000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 641     |
|    iterations      | 1063    |
|    time_elapsed    | 3391    |
|    total_timesteps | 2177024 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2178000, episode_reward=2.79 +/- 3.10
Episode length: 223.80 +/- 68.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 224         |
|    mean_reward          | 2.79        |
| time/                   |             |
|    total_timesteps      | 2178000     |
| train/                  |             |
|    approx_kl            | 0.008455647 |
|    clip_fraction        | 0.073       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.57       |
|    explained_variance   | 0.898       |
|    learning_rate        | 5.61e-05    |
|    loss                 | -0.0044     |
|    n_updates            | 10630       |
|    policy_gradient_loss | -0.0075     |
|    std                  | 0.532       |
|    value_loss           | 0.0349      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2179000, episode_reward=4.31 +/- 2.01
Episode length: 210.20 +/- 52.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | 4.31     |
| time/              |          |
|    total_timesteps | 2179000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 642     |
|    iterations      | 1064    |
|    time_elapsed    | 3393    |
|    total_timesteps | 2179072 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2180000, episode_reward=1.86 +/- 3.02
Episode length: 257.40 +/- 52.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 257          |
|    mean_reward          | 1.86         |
| time/                   |              |
|    total_timesteps      | 2180000      |
| train/                  |              |
|    approx_kl            | 0.0073445505 |
|    clip_fraction        | 0.0593       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.57        |
|    explained_variance   | 0.907        |
|    learning_rate        | 5.61e-05     |
|    loss                 | 0.00715      |
|    n_updates            | 10640        |
|    policy_gradient_loss | -0.00527     |
|    std                  | 0.531        |
|    value_loss           | 0.0227       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2181000, episode_reward=1.65 +/- 3.04
Episode length: 261.40 +/- 51.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 261      |
|    mean_reward     | 1.65     |
| time/              |          |
|    total_timesteps | 2181000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 642     |
|    iterations      | 1065    |
|    time_elapsed    | 3396    |
|    total_timesteps | 2181120 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2182000, episode_reward=0.91 +/- 2.40
Episode length: 275.00 +/- 50.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 275          |
|    mean_reward          | 0.908        |
| time/                   |              |
|    total_timesteps      | 2182000      |
| train/                  |              |
|    approx_kl            | 0.0061706826 |
|    clip_fraction        | 0.0468       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.57        |
|    explained_variance   | 0.864        |
|    learning_rate        | 5.62e-05     |
|    loss                 | 0.0493       |
|    n_updates            | 10650        |
|    policy_gradient_loss | -0.00758     |
|    std                  | 0.53         |
|    value_loss           | 0.0958       |
------------------------------------------
box reached target
box reached target
Eval num_timesteps=2183000, episode_reward=1.70 +/- 3.07
Episode length: 281.40 +/- 23.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 281      |
|    mean_reward     | 1.7      |
| time/              |          |
|    total_timesteps | 2183000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 642     |
|    iterations      | 1066    |
|    time_elapsed    | 3400    |
|    total_timesteps | 2183168 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2184000, episode_reward=4.13 +/- 2.55
Episode length: 213.00 +/- 45.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 213         |
|    mean_reward          | 4.13        |
| time/                   |             |
|    total_timesteps      | 2184000     |
| train/                  |             |
|    approx_kl            | 0.006707794 |
|    clip_fraction        | 0.0499      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.57       |
|    explained_variance   | 0.866       |
|    learning_rate        | 5.62e-05    |
|    loss                 | -0.012      |
|    n_updates            | 10660       |
|    policy_gradient_loss | -0.00777    |
|    std                  | 0.531       |
|    value_loss           | 0.0219      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2185000, episode_reward=1.71 +/- 2.91
Episode length: 253.40 +/- 57.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 253      |
|    mean_reward     | 1.71     |
| time/              |          |
|    total_timesteps | 2185000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 642     |
|    iterations      | 1067    |
|    time_elapsed    | 3403    |
|    total_timesteps | 2185216 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=2186000, episode_reward=1.77 +/- 2.88
Episode length: 246.40 +/- 65.67
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 246        |
|    mean_reward          | 1.77       |
| time/                   |            |
|    total_timesteps      | 2186000    |
| train/                  |            |
|    approx_kl            | 0.00628468 |
|    clip_fraction        | 0.0426     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.57      |
|    explained_variance   | 0.84       |
|    learning_rate        | 5.62e-05   |
|    loss                 | 0.0629     |
|    n_updates            | 10670      |
|    policy_gradient_loss | -0.00525   |
|    std                  | 0.532      |
|    value_loss           | 0.0712     |
----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2187000, episode_reward=1.69 +/- 3.00
Episode length: 262.60 +/- 45.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 263      |
|    mean_reward     | 1.69     |
| time/              |          |
|    total_timesteps | 2187000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 642     |
|    iterations      | 1068    |
|    time_elapsed    | 3406    |
|    total_timesteps | 2187264 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2188000, episode_reward=1.84 +/- 3.07
Episode length: 273.00 +/- 40.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 273          |
|    mean_reward          | 1.84         |
| time/                   |              |
|    total_timesteps      | 2188000      |
| train/                  |              |
|    approx_kl            | 0.0045906296 |
|    clip_fraction        | 0.0536       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.57        |
|    explained_variance   | 0.677        |
|    learning_rate        | 5.62e-05     |
|    loss                 | 0.00368      |
|    n_updates            | 10680        |
|    policy_gradient_loss | -0.00643     |
|    std                  | 0.532        |
|    value_loss           | 0.0736       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2189000, episode_reward=3.06 +/- 2.82
Episode length: 227.00 +/- 66.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 227      |
|    mean_reward     | 3.06     |
| time/              |          |
|    total_timesteps | 2189000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 642     |
|    iterations      | 1069    |
|    time_elapsed    | 3409    |
|    total_timesteps | 2189312 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2190000, episode_reward=2.89 +/- 2.91
Episode length: 234.40 +/- 54.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 234         |
|    mean_reward          | 2.89        |
| time/                   |             |
|    total_timesteps      | 2190000     |
| train/                  |             |
|    approx_kl            | 0.006266346 |
|    clip_fraction        | 0.0472      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.57       |
|    explained_variance   | 0.918       |
|    learning_rate        | 5.63e-05    |
|    loss                 | 0.0116      |
|    n_updates            | 10690       |
|    policy_gradient_loss | -0.00515    |
|    std                  | 0.533       |
|    value_loss           | 0.018       |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2191000, episode_reward=4.03 +/- 2.59
Episode length: 197.00 +/- 55.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 197      |
|    mean_reward     | 4.03     |
| time/              |          |
|    total_timesteps | 2191000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 642     |
|    iterations      | 1070    |
|    time_elapsed    | 3411    |
|    total_timesteps | 2191360 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2192000, episode_reward=2.01 +/- 2.69
Episode length: 245.60 +/- 67.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 246          |
|    mean_reward          | 2.01         |
| time/                   |              |
|    total_timesteps      | 2192000      |
| train/                  |              |
|    approx_kl            | 0.0043273023 |
|    clip_fraction        | 0.0426       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.57        |
|    explained_variance   | 0.924        |
|    learning_rate        | 5.63e-05     |
|    loss                 | 0.0207       |
|    n_updates            | 10700        |
|    policy_gradient_loss | -0.00474     |
|    std                  | 0.532        |
|    value_loss           | 0.0411       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2193000, episode_reward=2.85 +/- 2.81
Episode length: 230.40 +/- 56.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 230      |
|    mean_reward     | 2.85     |
| time/              |          |
|    total_timesteps | 2193000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 642     |
|    iterations      | 1071    |
|    time_elapsed    | 3414    |
|    total_timesteps | 2193408 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2194000, episode_reward=1.71 +/- 2.97
Episode length: 259.40 +/- 50.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 259          |
|    mean_reward          | 1.71         |
| time/                   |              |
|    total_timesteps      | 2194000      |
| train/                  |              |
|    approx_kl            | 0.0073244725 |
|    clip_fraction        | 0.0674       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.57        |
|    explained_variance   | 0.934        |
|    learning_rate        | 5.63e-05     |
|    loss                 | -0.0053      |
|    n_updates            | 10710        |
|    policy_gradient_loss | -0.00932     |
|    std                  | 0.533        |
|    value_loss           | 0.0242       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2195000, episode_reward=2.98 +/- 2.88
Episode length: 221.80 +/- 65.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 222      |
|    mean_reward     | 2.98     |
| time/              |          |
|    total_timesteps | 2195000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 642     |
|    iterations      | 1072    |
|    time_elapsed    | 3417    |
|    total_timesteps | 2195456 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2196000, episode_reward=3.17 +/- 2.96
Episode length: 237.60 +/- 51.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 238         |
|    mean_reward          | 3.17        |
| time/                   |             |
|    total_timesteps      | 2196000     |
| train/                  |             |
|    approx_kl            | 0.004604376 |
|    clip_fraction        | 0.0414      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.57       |
|    explained_variance   | 0.847       |
|    learning_rate        | 5.63e-05    |
|    loss                 | 0.0349      |
|    n_updates            | 10720       |
|    policy_gradient_loss | -0.00432    |
|    std                  | 0.531       |
|    value_loss           | 0.0621      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2197000, episode_reward=5.32 +/- 0.18
Episode length: 181.80 +/- 25.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 182      |
|    mean_reward     | 5.32     |
| time/              |          |
|    total_timesteps | 2197000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 642     |
|    iterations      | 1073    |
|    time_elapsed    | 3420    |
|    total_timesteps | 2197504 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2198000, episode_reward=1.85 +/- 2.75
Episode length: 248.40 +/- 63.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 248          |
|    mean_reward          | 1.85         |
| time/                   |              |
|    total_timesteps      | 2198000      |
| train/                  |              |
|    approx_kl            | 0.0055618407 |
|    clip_fraction        | 0.0515       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.57        |
|    explained_variance   | 0.853        |
|    learning_rate        | 5.64e-05     |
|    loss                 | 0.00822      |
|    n_updates            | 10730        |
|    policy_gradient_loss | -0.00567     |
|    std                  | 0.532        |
|    value_loss           | 0.0173       |
------------------------------------------
box reached target
Eval num_timesteps=2199000, episode_reward=0.66 +/- 2.34
Episode length: 267.00 +/- 66.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 267      |
|    mean_reward     | 0.659    |
| time/              |          |
|    total_timesteps | 2199000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 642     |
|    iterations      | 1074    |
|    time_elapsed    | 3423    |
|    total_timesteps | 2199552 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2200000, episode_reward=5.24 +/- 0.11
Episode length: 198.20 +/- 35.12
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 198        |
|    mean_reward          | 5.24       |
| time/                   |            |
|    total_timesteps      | 2200000    |
| train/                  |            |
|    approx_kl            | 0.00636461 |
|    clip_fraction        | 0.0509     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.57      |
|    explained_variance   | 0.934      |
|    learning_rate        | 5.64e-05   |
|    loss                 | -0.00815   |
|    n_updates            | 10740      |
|    policy_gradient_loss | -0.00621   |
|    std                  | 0.532      |
|    value_loss           | 0.0109     |
----------------------------------------
box reached target
box reached target
Eval num_timesteps=2201000, episode_reward=1.67 +/- 2.87
Episode length: 247.00 +/- 66.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 247      |
|    mean_reward     | 1.67     |
| time/              |          |
|    total_timesteps | 2201000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 642     |
|    iterations      | 1075    |
|    time_elapsed    | 3426    |
|    total_timesteps | 2201600 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2202000, episode_reward=3.23 +/- 2.50
Episode length: 225.00 +/- 66.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 225         |
|    mean_reward          | 3.23        |
| time/                   |             |
|    total_timesteps      | 2202000     |
| train/                  |             |
|    approx_kl            | 0.005989319 |
|    clip_fraction        | 0.0585      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.57       |
|    explained_variance   | 0.93        |
|    learning_rate        | 5.64e-05    |
|    loss                 | 0.0221      |
|    n_updates            | 10750       |
|    policy_gradient_loss | -0.00465    |
|    std                  | 0.531       |
|    value_loss           | 0.032       |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2203000, episode_reward=0.97 +/- 2.29
Episode length: 275.60 +/- 48.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 276      |
|    mean_reward     | 0.967    |
| time/              |          |
|    total_timesteps | 2203000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 642     |
|    iterations      | 1076    |
|    time_elapsed    | 3429    |
|    total_timesteps | 2203648 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2204000, episode_reward=2.97 +/- 2.74
Episode length: 234.00 +/- 55.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 234          |
|    mean_reward          | 2.97         |
| time/                   |              |
|    total_timesteps      | 2204000      |
| train/                  |              |
|    approx_kl            | 0.0048678257 |
|    clip_fraction        | 0.0294       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.57        |
|    explained_variance   | 0.946        |
|    learning_rate        | 5.64e-05     |
|    loss                 | -0.0208      |
|    n_updates            | 10760        |
|    policy_gradient_loss | -0.00306     |
|    std                  | 0.532        |
|    value_loss           | 0.0142       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2205000, episode_reward=1.71 +/- 2.99
Episode length: 251.20 +/- 60.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | 1.71     |
| time/              |          |
|    total_timesteps | 2205000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 642     |
|    iterations      | 1077    |
|    time_elapsed    | 3432    |
|    total_timesteps | 2205696 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=2206000, episode_reward=1.09 +/- 2.03
Episode length: 282.20 +/- 35.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 282         |
|    mean_reward          | 1.09        |
| time/                   |             |
|    total_timesteps      | 2206000     |
| train/                  |             |
|    approx_kl            | 0.007986471 |
|    clip_fraction        | 0.0614      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.57       |
|    explained_variance   | 0.873       |
|    learning_rate        | 5.65e-05    |
|    loss                 | 0.0152      |
|    n_updates            | 10770       |
|    policy_gradient_loss | -0.00659    |
|    std                  | 0.53        |
|    value_loss           | 0.0605      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2207000, episode_reward=4.07 +/- 2.51
Episode length: 205.20 +/- 50.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 205      |
|    mean_reward     | 4.07     |
| time/              |          |
|    total_timesteps | 2207000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 642     |
|    iterations      | 1078    |
|    time_elapsed    | 3435    |
|    total_timesteps | 2207744 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2208000, episode_reward=1.98 +/- 2.77
Episode length: 246.40 +/- 65.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 246          |
|    mean_reward          | 1.98         |
| time/                   |              |
|    total_timesteps      | 2208000      |
| train/                  |              |
|    approx_kl            | 0.0054887123 |
|    clip_fraction        | 0.0466       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.56        |
|    explained_variance   | 0.856        |
|    learning_rate        | 5.65e-05     |
|    loss                 | -0.0111      |
|    n_updates            | 10780        |
|    policy_gradient_loss | -0.00544     |
|    std                  | 0.529        |
|    value_loss           | 0.041        |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2209000, episode_reward=0.66 +/- 2.34
Episode length: 276.20 +/- 47.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 276      |
|    mean_reward     | 0.658    |
| time/              |          |
|    total_timesteps | 2209000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 642     |
|    iterations      | 1079    |
|    time_elapsed    | 3438    |
|    total_timesteps | 2209792 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2210000, episode_reward=2.89 +/- 2.92
Episode length: 226.80 +/- 65.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 227         |
|    mean_reward          | 2.89        |
| time/                   |             |
|    total_timesteps      | 2210000     |
| train/                  |             |
|    approx_kl            | 0.008818572 |
|    clip_fraction        | 0.0896      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.56       |
|    explained_variance   | 0.929       |
|    learning_rate        | 5.65e-05    |
|    loss                 | -0.000511   |
|    n_updates            | 10790       |
|    policy_gradient_loss | -0.0124     |
|    std                  | 0.529       |
|    value_loss           | 0.027       |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2211000, episode_reward=2.09 +/- 2.84
Episode length: 269.20 +/- 43.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 269      |
|    mean_reward     | 2.09     |
| time/              |          |
|    total_timesteps | 2211000  |
---------------------------------
box reached target
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 642     |
|    iterations      | 1080    |
|    time_elapsed    | 3441    |
|    total_timesteps | 2211840 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2212000, episode_reward=4.12 +/- 2.56
Episode length: 192.80 +/- 56.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 193         |
|    mean_reward          | 4.12        |
| time/                   |             |
|    total_timesteps      | 2212000     |
| train/                  |             |
|    approx_kl            | 0.006079457 |
|    clip_fraction        | 0.0513      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.56       |
|    explained_variance   | 0.899       |
|    learning_rate        | 5.65e-05    |
|    loss                 | 0.00151     |
|    n_updates            | 10800       |
|    policy_gradient_loss | -0.00544    |
|    std                  | 0.53        |
|    value_loss           | 0.0207      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2213000, episode_reward=0.60 +/- 2.48
Episode length: 283.80 +/- 32.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 284      |
|    mean_reward     | 0.604    |
| time/              |          |
|    total_timesteps | 2213000  |
---------------------------------
box reached target
box reached target
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 642     |
|    iterations      | 1081    |
|    time_elapsed    | 3444    |
|    total_timesteps | 2213888 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2214000, episode_reward=3.09 +/- 2.69
Episode length: 218.00 +/- 67.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 218         |
|    mean_reward          | 3.09        |
| time/                   |             |
|    total_timesteps      | 2214000     |
| train/                  |             |
|    approx_kl            | 0.005284352 |
|    clip_fraction        | 0.0506      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.56       |
|    explained_variance   | 0.889       |
|    learning_rate        | 5.66e-05    |
|    loss                 | 0.00614     |
|    n_updates            | 10810       |
|    policy_gradient_loss | -0.0076     |
|    std                  | 0.528       |
|    value_loss           | 0.0571      |
-----------------------------------------
box reached target
Eval num_timesteps=2215000, episode_reward=0.87 +/- 2.30
Episode length: 282.80 +/- 34.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 283      |
|    mean_reward     | 0.868    |
| time/              |          |
|    total_timesteps | 2215000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 642     |
|    iterations      | 1082    |
|    time_elapsed    | 3447    |
|    total_timesteps | 2215936 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=2216000, episode_reward=0.67 +/- 2.44
Episode length: 281.60 +/- 36.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 282          |
|    mean_reward          | 0.667        |
| time/                   |              |
|    total_timesteps      | 2216000      |
| train/                  |              |
|    approx_kl            | 0.0053754086 |
|    clip_fraction        | 0.0479       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.56        |
|    explained_variance   | 0.847        |
|    learning_rate        | 5.66e-05     |
|    loss                 | 0.00991      |
|    n_updates            | 10820        |
|    policy_gradient_loss | -0.0067      |
|    std                  | 0.529        |
|    value_loss           | 0.0347       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2217000, episode_reward=1.60 +/- 2.92
Episode length: 246.00 +/- 67.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 246      |
|    mean_reward     | 1.6      |
| time/              |          |
|    total_timesteps | 2217000  |
---------------------------------
box reached target
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 642     |
|    iterations      | 1083    |
|    time_elapsed    | 3450    |
|    total_timesteps | 2217984 |
--------------------------------
Eval num_timesteps=2218000, episode_reward=-0.52 +/- 0.56
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.519      |
| time/                   |             |
|    total_timesteps      | 2218000     |
| train/                  |             |
|    approx_kl            | 0.004211449 |
|    clip_fraction        | 0.0436      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.56       |
|    explained_variance   | 0.876       |
|    learning_rate        | 5.66e-05    |
|    loss                 | 0.00455     |
|    n_updates            | 10830       |
|    policy_gradient_loss | -0.006      |
|    std                  | 0.529       |
|    value_loss           | 0.0624      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=2219000, episode_reward=1.83 +/- 2.94
Episode length: 259.60 +/- 49.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 260      |
|    mean_reward     | 1.83     |
| time/              |          |
|    total_timesteps | 2219000  |
---------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2220000, episode_reward=2.33 +/- 2.42
Episode length: 254.60 +/- 63.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 255      |
|    mean_reward     | 2.33     |
| time/              |          |
|    total_timesteps | 2220000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 642     |
|    iterations      | 1084    |
|    time_elapsed    | 3454    |
|    total_timesteps | 2220032 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2221000, episode_reward=1.32 +/- 2.16
Episode length: 280.40 +/- 39.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 280          |
|    mean_reward          | 1.32         |
| time/                   |              |
|    total_timesteps      | 2221000      |
| train/                  |              |
|    approx_kl            | 0.0047754003 |
|    clip_fraction        | 0.0485       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.56        |
|    explained_variance   | 0.846        |
|    learning_rate        | 5.66e-05     |
|    loss                 | 0.0183       |
|    n_updates            | 10840        |
|    policy_gradient_loss | -0.00549     |
|    std                  | 0.529        |
|    value_loss           | 0.0376       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2222000, episode_reward=1.77 +/- 2.86
Episode length: 245.60 +/- 67.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 246      |
|    mean_reward     | 1.77     |
| time/              |          |
|    total_timesteps | 2222000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 642     |
|    iterations      | 1085    |
|    time_elapsed    | 3457    |
|    total_timesteps | 2222080 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=2223000, episode_reward=0.91 +/- 2.32
Episode length: 277.00 +/- 46.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 277          |
|    mean_reward          | 0.914        |
| time/                   |              |
|    total_timesteps      | 2223000      |
| train/                  |              |
|    approx_kl            | 0.0046933303 |
|    clip_fraction        | 0.035        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.56        |
|    explained_variance   | 0.848        |
|    learning_rate        | 5.67e-05     |
|    loss                 | 0.00256      |
|    n_updates            | 10850        |
|    policy_gradient_loss | -0.00474     |
|    std                  | 0.53         |
|    value_loss           | 0.0551       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2224000, episode_reward=2.82 +/- 3.06
Episode length: 236.80 +/- 55.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 237      |
|    mean_reward     | 2.82     |
| time/              |          |
|    total_timesteps | 2224000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 642     |
|    iterations      | 1086    |
|    time_elapsed    | 3460    |
|    total_timesteps | 2224128 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=2225000, episode_reward=2.04 +/- 2.66
Episode length: 255.40 +/- 54.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 255         |
|    mean_reward          | 2.04        |
| time/                   |             |
|    total_timesteps      | 2225000     |
| train/                  |             |
|    approx_kl            | 0.006999896 |
|    clip_fraction        | 0.0504      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.56       |
|    explained_variance   | 0.843       |
|    learning_rate        | 5.67e-05    |
|    loss                 | -0.0262     |
|    n_updates            | 10860       |
|    policy_gradient_loss | -0.00577    |
|    std                  | 0.529       |
|    value_loss           | 0.0411      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2226000, episode_reward=1.69 +/- 2.79
Episode length: 252.20 +/- 59.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 252      |
|    mean_reward     | 1.69     |
| time/              |          |
|    total_timesteps | 2226000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 642     |
|    iterations      | 1087    |
|    time_elapsed    | 3463    |
|    total_timesteps | 2226176 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2227000, episode_reward=2.08 +/- 2.63
Episode length: 253.00 +/- 57.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 253          |
|    mean_reward          | 2.08         |
| time/                   |              |
|    total_timesteps      | 2227000      |
| train/                  |              |
|    approx_kl            | 0.0050928853 |
|    clip_fraction        | 0.0342       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.56        |
|    explained_variance   | 0.823        |
|    learning_rate        | 5.67e-05     |
|    loss                 | 0.000637     |
|    n_updates            | 10870        |
|    policy_gradient_loss | -0.00325     |
|    std                  | 0.53         |
|    value_loss           | 0.0291       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2228000, episode_reward=2.96 +/- 2.65
Episode length: 241.60 +/- 49.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 242      |
|    mean_reward     | 2.96     |
| time/              |          |
|    total_timesteps | 2228000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 642     |
|    iterations      | 1088    |
|    time_elapsed    | 3466    |
|    total_timesteps | 2228224 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2229000, episode_reward=2.95 +/- 2.84
Episode length: 223.00 +/- 62.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 223         |
|    mean_reward          | 2.95        |
| time/                   |             |
|    total_timesteps      | 2229000     |
| train/                  |             |
|    approx_kl            | 0.004453932 |
|    clip_fraction        | 0.0356      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.57       |
|    explained_variance   | 0.876       |
|    learning_rate        | 5.67e-05    |
|    loss                 | 0.0205      |
|    n_updates            | 10880       |
|    policy_gradient_loss | -0.00335    |
|    std                  | 0.531       |
|    value_loss           | 0.0654      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2230000, episode_reward=2.21 +/- 2.44
Episode length: 261.40 +/- 51.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 261      |
|    mean_reward     | 2.21     |
| time/              |          |
|    total_timesteps | 2230000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 642     |
|    iterations      | 1089    |
|    time_elapsed    | 3469    |
|    total_timesteps | 2230272 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2231000, episode_reward=3.05 +/- 2.80
Episode length: 224.40 +/- 62.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 224          |
|    mean_reward          | 3.05         |
| time/                   |              |
|    total_timesteps      | 2231000      |
| train/                  |              |
|    approx_kl            | 0.0061589074 |
|    clip_fraction        | 0.0336       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.57        |
|    explained_variance   | 0.876        |
|    learning_rate        | 5.68e-05     |
|    loss                 | 0.00636      |
|    n_updates            | 10890        |
|    policy_gradient_loss | -0.00435     |
|    std                  | 0.53         |
|    value_loss           | 0.0389       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2232000, episode_reward=0.69 +/- 2.39
Episode length: 274.80 +/- 50.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 275      |
|    mean_reward     | 0.69     |
| time/              |          |
|    total_timesteps | 2232000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 642     |
|    iterations      | 1090    |
|    time_elapsed    | 3472    |
|    total_timesteps | 2232320 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2233000, episode_reward=4.42 +/- 1.86
Episode length: 216.00 +/- 49.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 216         |
|    mean_reward          | 4.42        |
| time/                   |             |
|    total_timesteps      | 2233000     |
| train/                  |             |
|    approx_kl            | 0.006535193 |
|    clip_fraction        | 0.0505      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.57       |
|    explained_variance   | 0.948       |
|    learning_rate        | 5.68e-05    |
|    loss                 | -0.0133     |
|    n_updates            | 10900       |
|    policy_gradient_loss | -0.00737    |
|    std                  | 0.53        |
|    value_loss           | 0.0229      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2234000, episode_reward=4.26 +/- 2.19
Episode length: 193.40 +/- 57.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 193      |
|    mean_reward     | 4.26     |
| time/              |          |
|    total_timesteps | 2234000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 642     |
|    iterations      | 1091    |
|    time_elapsed    | 3475    |
|    total_timesteps | 2234368 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2235000, episode_reward=2.65 +/- 3.04
Episode length: 224.60 +/- 65.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 225         |
|    mean_reward          | 2.65        |
| time/                   |             |
|    total_timesteps      | 2235000     |
| train/                  |             |
|    approx_kl            | 0.004818327 |
|    clip_fraction        | 0.025       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.56       |
|    explained_variance   | 0.898       |
|    learning_rate        | 5.68e-05    |
|    loss                 | 0.0265      |
|    n_updates            | 10910       |
|    policy_gradient_loss | -0.00388    |
|    std                  | 0.529       |
|    value_loss           | 0.0634      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2236000, episode_reward=3.90 +/- 2.61
Episode length: 200.20 +/- 58.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 200      |
|    mean_reward     | 3.9      |
| time/              |          |
|    total_timesteps | 2236000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 642     |
|    iterations      | 1092    |
|    time_elapsed    | 3478    |
|    total_timesteps | 2236416 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2237000, episode_reward=2.16 +/- 2.60
Episode length: 261.20 +/- 49.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 261         |
|    mean_reward          | 2.16        |
| time/                   |             |
|    total_timesteps      | 2237000     |
| train/                  |             |
|    approx_kl            | 0.006334149 |
|    clip_fraction        | 0.0584      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.56       |
|    explained_variance   | 0.79        |
|    learning_rate        | 5.68e-05    |
|    loss                 | 0.00712     |
|    n_updates            | 10920       |
|    policy_gradient_loss | -0.0068     |
|    std                  | 0.53        |
|    value_loss           | 0.0705      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2238000, episode_reward=2.59 +/- 2.32
Episode length: 265.00 +/- 57.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 265      |
|    mean_reward     | 2.59     |
| time/              |          |
|    total_timesteps | 2238000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 642     |
|    iterations      | 1093    |
|    time_elapsed    | 3481    |
|    total_timesteps | 2238464 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2239000, episode_reward=0.36 +/- 2.34
Episode length: 271.40 +/- 57.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 271         |
|    mean_reward          | 0.362       |
| time/                   |             |
|    total_timesteps      | 2239000     |
| train/                  |             |
|    approx_kl            | 0.004669656 |
|    clip_fraction        | 0.027       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.56       |
|    explained_variance   | 0.951       |
|    learning_rate        | 5.69e-05    |
|    loss                 | 0.001       |
|    n_updates            | 10930       |
|    policy_gradient_loss | -0.00312    |
|    std                  | 0.53        |
|    value_loss           | 0.0179      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2240000, episode_reward=4.18 +/- 2.74
Episode length: 232.20 +/- 37.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 232      |
|    mean_reward     | 4.18     |
| time/              |          |
|    total_timesteps | 2240000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 643     |
|    iterations      | 1094    |
|    time_elapsed    | 3484    |
|    total_timesteps | 2240512 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=2241000, episode_reward=1.71 +/- 3.02
Episode length: 256.40 +/- 53.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 256         |
|    mean_reward          | 1.71        |
| time/                   |             |
|    total_timesteps      | 2241000     |
| train/                  |             |
|    approx_kl            | 0.007246837 |
|    clip_fraction        | 0.0684      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.57       |
|    explained_variance   | 0.862       |
|    learning_rate        | 5.69e-05    |
|    loss                 | 0.00815     |
|    n_updates            | 10940       |
|    policy_gradient_loss | -0.00809    |
|    std                  | 0.531       |
|    value_loss           | 0.0724      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2242000, episode_reward=3.25 +/- 2.60
Episode length: 228.60 +/- 63.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 229      |
|    mean_reward     | 3.25     |
| time/              |          |
|    total_timesteps | 2242000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 643     |
|    iterations      | 1095    |
|    time_elapsed    | 3487    |
|    total_timesteps | 2242560 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2243000, episode_reward=2.88 +/- 2.92
Episode length: 226.20 +/- 63.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 226         |
|    mean_reward          | 2.88        |
| time/                   |             |
|    total_timesteps      | 2243000     |
| train/                  |             |
|    approx_kl            | 0.007173602 |
|    clip_fraction        | 0.068       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.57       |
|    explained_variance   | 0.847       |
|    learning_rate        | 5.69e-05    |
|    loss                 | -0.0185     |
|    n_updates            | 10950       |
|    policy_gradient_loss | -0.00689    |
|    std                  | 0.531       |
|    value_loss           | 0.0649      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2244000, episode_reward=1.81 +/- 2.80
Episode length: 250.00 +/- 62.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 250      |
|    mean_reward     | 1.81     |
| time/              |          |
|    total_timesteps | 2244000  |
---------------------------------
box reached target
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 643     |
|    iterations      | 1096    |
|    time_elapsed    | 3490    |
|    total_timesteps | 2244608 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2245000, episode_reward=3.26 +/- 2.58
Episode length: 230.00 +/- 63.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 230         |
|    mean_reward          | 3.26        |
| time/                   |             |
|    total_timesteps      | 2245000     |
| train/                  |             |
|    approx_kl            | 0.002722425 |
|    clip_fraction        | 0.017       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.57       |
|    explained_variance   | 0.842       |
|    learning_rate        | 5.69e-05    |
|    loss                 | -0.011      |
|    n_updates            | 10960       |
|    policy_gradient_loss | -0.00249    |
|    std                  | 0.531       |
|    value_loss           | 0.0605      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2246000, episode_reward=-0.29 +/- 0.78
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.287   |
| time/              |          |
|    total_timesteps | 2246000  |
---------------------------------
box reached target
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 643     |
|    iterations      | 1097    |
|    time_elapsed    | 3493    |
|    total_timesteps | 2246656 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2247000, episode_reward=4.33 +/- 1.95
Episode length: 210.00 +/- 49.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 210         |
|    mean_reward          | 4.33        |
| time/                   |             |
|    total_timesteps      | 2247000     |
| train/                  |             |
|    approx_kl            | 0.004437263 |
|    clip_fraction        | 0.0429      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.56       |
|    explained_variance   | 0.948       |
|    learning_rate        | 5.7e-05     |
|    loss                 | 0.0337      |
|    n_updates            | 10970       |
|    policy_gradient_loss | -0.00843    |
|    std                  | 0.528       |
|    value_loss           | 0.0339      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2248000, episode_reward=1.86 +/- 2.97
Episode length: 257.80 +/- 51.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 258      |
|    mean_reward     | 1.86     |
| time/              |          |
|    total_timesteps | 2248000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 643     |
|    iterations      | 1098    |
|    time_elapsed    | 3496    |
|    total_timesteps | 2248704 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2249000, episode_reward=4.39 +/- 1.91
Episode length: 221.40 +/- 42.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 221          |
|    mean_reward          | 4.39         |
| time/                   |              |
|    total_timesteps      | 2249000      |
| train/                  |              |
|    approx_kl            | 0.0053248573 |
|    clip_fraction        | 0.0439       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.55        |
|    explained_variance   | 0.948        |
|    learning_rate        | 5.7e-05      |
|    loss                 | 0.0123       |
|    n_updates            | 10980        |
|    policy_gradient_loss | -0.00658     |
|    std                  | 0.527        |
|    value_loss           | 0.0206       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2250000, episode_reward=4.06 +/- 2.68
Episode length: 220.20 +/- 43.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 220      |
|    mean_reward     | 4.06     |
| time/              |          |
|    total_timesteps | 2250000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 643     |
|    iterations      | 1099    |
|    time_elapsed    | 3499    |
|    total_timesteps | 2250752 |
--------------------------------
box reached target
Eval num_timesteps=2251000, episode_reward=0.84 +/- 2.31
Episode length: 271.60 +/- 56.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 272          |
|    mean_reward          | 0.843        |
| time/                   |              |
|    total_timesteps      | 2251000      |
| train/                  |              |
|    approx_kl            | 0.0050229956 |
|    clip_fraction        | 0.052        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.55        |
|    explained_variance   | 0.889        |
|    learning_rate        | 5.7e-05      |
|    loss                 | 0.000681     |
|    n_updates            | 10990        |
|    policy_gradient_loss | -0.00747     |
|    std                  | 0.527        |
|    value_loss           | 0.0294       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2252000, episode_reward=1.88 +/- 2.79
Episode length: 248.00 +/- 66.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 248      |
|    mean_reward     | 1.88     |
| time/              |          |
|    total_timesteps | 2252000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 643     |
|    iterations      | 1100    |
|    time_elapsed    | 3502    |
|    total_timesteps | 2252800 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2253000, episode_reward=4.23 +/- 1.87
Episode length: 200.80 +/- 50.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 201          |
|    mean_reward          | 4.23         |
| time/                   |              |
|    total_timesteps      | 2253000      |
| train/                  |              |
|    approx_kl            | 0.0031795523 |
|    clip_fraction        | 0.0333       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.56        |
|    explained_variance   | 0.784        |
|    learning_rate        | 5.7e-05      |
|    loss                 | -0.0167      |
|    n_updates            | 11000        |
|    policy_gradient_loss | -0.00413     |
|    std                  | 0.529        |
|    value_loss           | 0.0441       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2254000, episode_reward=2.97 +/- 2.85
Episode length: 225.40 +/- 66.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 225      |
|    mean_reward     | 2.97     |
| time/              |          |
|    total_timesteps | 2254000  |
---------------------------------
box reached target
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 643     |
|    iterations      | 1101    |
|    time_elapsed    | 3505    |
|    total_timesteps | 2254848 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2255000, episode_reward=3.08 +/- 2.77
Episode length: 234.60 +/- 59.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 235         |
|    mean_reward          | 3.08        |
| time/                   |             |
|    total_timesteps      | 2255000     |
| train/                  |             |
|    approx_kl            | 0.004938125 |
|    clip_fraction        | 0.0427      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.56       |
|    explained_variance   | 0.897       |
|    learning_rate        | 5.71e-05    |
|    loss                 | 0.00539     |
|    n_updates            | 11010       |
|    policy_gradient_loss | -0.00412    |
|    std                  | 0.529       |
|    value_loss           | 0.038       |
-----------------------------------------
box reached target
Eval num_timesteps=2256000, episode_reward=0.03 +/- 0.49
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | 0.034    |
| time/              |          |
|    total_timesteps | 2256000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 643     |
|    iterations      | 1102    |
|    time_elapsed    | 3508    |
|    total_timesteps | 2256896 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=2257000, episode_reward=1.61 +/- 2.95
Episode length: 252.80 +/- 59.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 253          |
|    mean_reward          | 1.61         |
| time/                   |              |
|    total_timesteps      | 2257000      |
| train/                  |              |
|    approx_kl            | 0.0042795893 |
|    clip_fraction        | 0.0421       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.56        |
|    explained_variance   | 0.887        |
|    learning_rate        | 5.71e-05     |
|    loss                 | 0.0111       |
|    n_updates            | 11020        |
|    policy_gradient_loss | -0.00459     |
|    std                  | 0.529        |
|    value_loss           | 0.0357       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2258000, episode_reward=3.03 +/- 2.78
Episode length: 228.20 +/- 60.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 228      |
|    mean_reward     | 3.03     |
| time/              |          |
|    total_timesteps | 2258000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 643     |
|    iterations      | 1103    |
|    time_elapsed    | 3511    |
|    total_timesteps | 2258944 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2259000, episode_reward=3.03 +/- 2.73
Episode length: 219.00 +/- 67.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 219         |
|    mean_reward          | 3.03        |
| time/                   |             |
|    total_timesteps      | 2259000     |
| train/                  |             |
|    approx_kl            | 0.005607635 |
|    clip_fraction        | 0.0448      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.56       |
|    explained_variance   | 0.836       |
|    learning_rate        | 5.71e-05    |
|    loss                 | -0.00438    |
|    n_updates            | 11030       |
|    policy_gradient_loss | -0.00843    |
|    std                  | 0.53        |
|    value_loss           | 0.0331      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=2260000, episode_reward=0.43 +/- 2.39
Episode length: 273.00 +/- 54.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 273      |
|    mean_reward     | 0.428    |
| time/              |          |
|    total_timesteps | 2260000  |
---------------------------------
box reached target
box reached target
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 643     |
|    iterations      | 1104    |
|    time_elapsed    | 3514    |
|    total_timesteps | 2260992 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=2261000, episode_reward=1.53 +/- 3.11
Episode length: 263.20 +/- 49.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 263          |
|    mean_reward          | 1.53         |
| time/                   |              |
|    total_timesteps      | 2261000      |
| train/                  |              |
|    approx_kl            | 0.0048638787 |
|    clip_fraction        | 0.0382       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.56        |
|    explained_variance   | 0.905        |
|    learning_rate        | 5.71e-05     |
|    loss                 | -0.0177      |
|    n_updates            | 11040        |
|    policy_gradient_loss | -0.00516     |
|    std                  | 0.53         |
|    value_loss           | 0.0671       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2262000, episode_reward=2.62 +/- 3.21
Episode length: 219.20 +/- 67.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 219      |
|    mean_reward     | 2.62     |
| time/              |          |
|    total_timesteps | 2262000  |
---------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2263000, episode_reward=1.77 +/- 2.83
Episode length: 249.60 +/- 62.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 250      |
|    mean_reward     | 1.77     |
| time/              |          |
|    total_timesteps | 2263000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 643     |
|    iterations      | 1105    |
|    time_elapsed    | 3517    |
|    total_timesteps | 2263040 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2264000, episode_reward=2.47 +/- 2.40
Episode length: 254.40 +/- 55.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 254          |
|    mean_reward          | 2.47         |
| time/                   |              |
|    total_timesteps      | 2264000      |
| train/                  |              |
|    approx_kl            | 0.0038677962 |
|    clip_fraction        | 0.0226       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.56        |
|    explained_variance   | 0.827        |
|    learning_rate        | 5.72e-05     |
|    loss                 | -0.00842     |
|    n_updates            | 11050        |
|    policy_gradient_loss | -0.00247     |
|    std                  | 0.53         |
|    value_loss           | 0.0383       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2265000, episode_reward=1.59 +/- 3.04
Episode length: 245.60 +/- 68.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 246      |
|    mean_reward     | 1.59     |
| time/              |          |
|    total_timesteps | 2265000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 643     |
|    iterations      | 1106    |
|    time_elapsed    | 3520    |
|    total_timesteps | 2265088 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2266000, episode_reward=4.32 +/- 1.92
Episode length: 222.80 +/- 39.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 223         |
|    mean_reward          | 4.32        |
| time/                   |             |
|    total_timesteps      | 2266000     |
| train/                  |             |
|    approx_kl            | 0.004451092 |
|    clip_fraction        | 0.0414      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.57       |
|    explained_variance   | 0.8         |
|    learning_rate        | 5.72e-05    |
|    loss                 | -0.0133     |
|    n_updates            | 11060       |
|    policy_gradient_loss | -0.0037     |
|    std                  | 0.531       |
|    value_loss           | 0.0621      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2267000, episode_reward=1.70 +/- 3.00
Episode length: 259.60 +/- 50.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 260      |
|    mean_reward     | 1.7      |
| time/              |          |
|    total_timesteps | 2267000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 643     |
|    iterations      | 1107    |
|    time_elapsed    | 3523    |
|    total_timesteps | 2267136 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2268000, episode_reward=2.77 +/- 3.13
Episode length: 234.40 +/- 66.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 234          |
|    mean_reward          | 2.77         |
| time/                   |              |
|    total_timesteps      | 2268000      |
| train/                  |              |
|    approx_kl            | 0.0053513246 |
|    clip_fraction        | 0.0378       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.57        |
|    explained_variance   | 0.924        |
|    learning_rate        | 5.72e-05     |
|    loss                 | 0.0139       |
|    n_updates            | 11070        |
|    policy_gradient_loss | -0.00479     |
|    std                  | 0.532        |
|    value_loss           | 0.0348       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2269000, episode_reward=3.99 +/- 2.50
Episode length: 200.40 +/- 52.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 200      |
|    mean_reward     | 3.99     |
| time/              |          |
|    total_timesteps | 2269000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 643     |
|    iterations      | 1108    |
|    time_elapsed    | 3526    |
|    total_timesteps | 2269184 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2270000, episode_reward=4.03 +/- 2.55
Episode length: 219.00 +/- 55.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 219          |
|    mean_reward          | 4.03         |
| time/                   |              |
|    total_timesteps      | 2270000      |
| train/                  |              |
|    approx_kl            | 0.0049487995 |
|    clip_fraction        | 0.0498       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.57        |
|    explained_variance   | 0.924        |
|    learning_rate        | 5.72e-05     |
|    loss                 | -0.00708     |
|    n_updates            | 11080        |
|    policy_gradient_loss | -0.00743     |
|    std                  | 0.531        |
|    value_loss           | 0.0312       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2271000, episode_reward=3.18 +/- 2.48
Episode length: 210.20 +/- 73.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | 3.18     |
| time/              |          |
|    total_timesteps | 2271000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 643     |
|    iterations      | 1109    |
|    time_elapsed    | 3529    |
|    total_timesteps | 2271232 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2272000, episode_reward=1.84 +/- 2.78
Episode length: 245.60 +/- 68.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 246         |
|    mean_reward          | 1.84        |
| time/                   |             |
|    total_timesteps      | 2272000     |
| train/                  |             |
|    approx_kl            | 0.004403708 |
|    clip_fraction        | 0.0397      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.57       |
|    explained_variance   | 0.889       |
|    learning_rate        | 5.73e-05    |
|    loss                 | 0.00735     |
|    n_updates            | 11090       |
|    policy_gradient_loss | -0.00556    |
|    std                  | 0.531       |
|    value_loss           | 0.0394      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2273000, episode_reward=2.42 +/- 2.40
Episode length: 261.00 +/- 47.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 261      |
|    mean_reward     | 2.42     |
| time/              |          |
|    total_timesteps | 2273000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 643     |
|    iterations      | 1110    |
|    time_elapsed    | 3532    |
|    total_timesteps | 2273280 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=2274000, episode_reward=2.07 +/- 2.63
Episode length: 246.60 +/- 65.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 247         |
|    mean_reward          | 2.07        |
| time/                   |             |
|    total_timesteps      | 2274000     |
| train/                  |             |
|    approx_kl            | 0.004454357 |
|    clip_fraction        | 0.0263      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.57       |
|    explained_variance   | 0.895       |
|    learning_rate        | 5.73e-05    |
|    loss                 | 0.00215     |
|    n_updates            | 11100       |
|    policy_gradient_loss | -0.003      |
|    std                  | 0.532       |
|    value_loss           | 0.0233      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2275000, episode_reward=2.25 +/- 2.68
Episode length: 257.60 +/- 52.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 258      |
|    mean_reward     | 2.25     |
| time/              |          |
|    total_timesteps | 2275000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 643     |
|    iterations      | 1111    |
|    time_elapsed    | 3535    |
|    total_timesteps | 2275328 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2276000, episode_reward=3.02 +/- 2.81
Episode length: 212.20 +/- 72.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 212          |
|    mean_reward          | 3.02         |
| time/                   |              |
|    total_timesteps      | 2276000      |
| train/                  |              |
|    approx_kl            | 0.0050636795 |
|    clip_fraction        | 0.0321       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.57        |
|    explained_variance   | 0.752        |
|    learning_rate        | 5.73e-05     |
|    loss                 | 0.00652      |
|    n_updates            | 11110        |
|    policy_gradient_loss | -0.00272     |
|    std                  | 0.532        |
|    value_loss           | 0.0707       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2277000, episode_reward=3.11 +/- 2.81
Episode length: 233.00 +/- 54.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 233      |
|    mean_reward     | 3.11     |
| time/              |          |
|    total_timesteps | 2277000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 643     |
|    iterations      | 1112    |
|    time_elapsed    | 3538    |
|    total_timesteps | 2277376 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2278000, episode_reward=3.05 +/- 2.76
Episode length: 222.40 +/- 65.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 222       |
|    mean_reward          | 3.05      |
| time/                   |           |
|    total_timesteps      | 2278000   |
| train/                  |           |
|    approx_kl            | 0.0040421 |
|    clip_fraction        | 0.045     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.57     |
|    explained_variance   | 0.943     |
|    learning_rate        | 5.73e-05  |
|    loss                 | -0.0203   |
|    n_updates            | 11120     |
|    policy_gradient_loss | -0.00628  |
|    std                  | 0.533     |
|    value_loss           | 0.0256    |
---------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2279000, episode_reward=4.31 +/- 2.11
Episode length: 211.00 +/- 49.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 211      |
|    mean_reward     | 4.31     |
| time/              |          |
|    total_timesteps | 2279000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 643     |
|    iterations      | 1113    |
|    time_elapsed    | 3541    |
|    total_timesteps | 2279424 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2280000, episode_reward=2.08 +/- 2.74
Episode length: 270.20 +/- 41.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 270         |
|    mean_reward          | 2.08        |
| time/                   |             |
|    total_timesteps      | 2280000     |
| train/                  |             |
|    approx_kl            | 0.005671818 |
|    clip_fraction        | 0.0615      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.57       |
|    explained_variance   | 0.905       |
|    learning_rate        | 5.74e-05    |
|    loss                 | -0.00621    |
|    n_updates            | 11130       |
|    policy_gradient_loss | -0.00619    |
|    std                  | 0.532       |
|    value_loss           | 0.0193      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2281000, episode_reward=3.94 +/- 2.61
Episode length: 187.80 +/- 58.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 188      |
|    mean_reward     | 3.94     |
| time/              |          |
|    total_timesteps | 2281000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 643     |
|    iterations      | 1114    |
|    time_elapsed    | 3544    |
|    total_timesteps | 2281472 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2282000, episode_reward=3.43 +/- 2.53
Episode length: 225.60 +/- 61.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 226          |
|    mean_reward          | 3.43         |
| time/                   |              |
|    total_timesteps      | 2282000      |
| train/                  |              |
|    approx_kl            | 0.0058378233 |
|    clip_fraction        | 0.0458       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.57        |
|    explained_variance   | 0.883        |
|    learning_rate        | 5.74e-05     |
|    loss                 | 0.0221       |
|    n_updates            | 11140        |
|    policy_gradient_loss | -0.00839     |
|    std                  | 0.532        |
|    value_loss           | 0.0381       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2283000, episode_reward=3.39 +/- 2.45
Episode length: 240.40 +/- 53.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 240      |
|    mean_reward     | 3.39     |
| time/              |          |
|    total_timesteps | 2283000  |
---------------------------------
box reached target
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 643     |
|    iterations      | 1115    |
|    time_elapsed    | 3547    |
|    total_timesteps | 2283520 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2284000, episode_reward=3.98 +/- 2.49
Episode length: 213.00 +/- 50.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 213         |
|    mean_reward          | 3.98        |
| time/                   |             |
|    total_timesteps      | 2284000     |
| train/                  |             |
|    approx_kl            | 0.004540921 |
|    clip_fraction        | 0.0395      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.57       |
|    explained_variance   | 0.928       |
|    learning_rate        | 5.74e-05    |
|    loss                 | 0.0175      |
|    n_updates            | 11150       |
|    policy_gradient_loss | -0.00477    |
|    std                  | 0.53        |
|    value_loss           | 0.0474      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2285000, episode_reward=2.89 +/- 3.06
Episode length: 253.40 +/- 44.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 253      |
|    mean_reward     | 2.89     |
| time/              |          |
|    total_timesteps | 2285000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 643     |
|    iterations      | 1116    |
|    time_elapsed    | 3550    |
|    total_timesteps | 2285568 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2286000, episode_reward=4.14 +/- 2.58
Episode length: 222.60 +/- 50.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 223         |
|    mean_reward          | 4.14        |
| time/                   |             |
|    total_timesteps      | 2286000     |
| train/                  |             |
|    approx_kl            | 0.005897797 |
|    clip_fraction        | 0.0419      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.56       |
|    explained_variance   | 0.935       |
|    learning_rate        | 5.74e-05    |
|    loss                 | 0.0175      |
|    n_updates            | 11160       |
|    policy_gradient_loss | -0.00478    |
|    std                  | 0.53        |
|    value_loss           | 0.0148      |
-----------------------------------------
box reached target
box reached target
Eval num_timesteps=2287000, episode_reward=2.04 +/- 2.65
Episode length: 252.40 +/- 60.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 252      |
|    mean_reward     | 2.04     |
| time/              |          |
|    total_timesteps | 2287000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 643     |
|    iterations      | 1117    |
|    time_elapsed    | 3553    |
|    total_timesteps | 2287616 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=2288000, episode_reward=1.18 +/- 2.37
Episode length: 281.40 +/- 37.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 281          |
|    mean_reward          | 1.18         |
| time/                   |              |
|    total_timesteps      | 2288000      |
| train/                  |              |
|    approx_kl            | 0.0044472963 |
|    clip_fraction        | 0.0273       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.56        |
|    explained_variance   | 0.917        |
|    learning_rate        | 5.75e-05     |
|    loss                 | -0.00305     |
|    n_updates            | 11170        |
|    policy_gradient_loss | -0.003       |
|    std                  | 0.53         |
|    value_loss           | 0.0254       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2289000, episode_reward=2.29 +/- 2.62
Episode length: 262.80 +/- 46.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 263      |
|    mean_reward     | 2.29     |
| time/              |          |
|    total_timesteps | 2289000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 643     |
|    iterations      | 1118    |
|    time_elapsed    | 3556    |
|    total_timesteps | 2289664 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=2290000, episode_reward=1.99 +/- 2.70
Episode length: 261.40 +/- 48.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 261          |
|    mean_reward          | 1.99         |
| time/                   |              |
|    total_timesteps      | 2290000      |
| train/                  |              |
|    approx_kl            | 0.0034690425 |
|    clip_fraction        | 0.0386       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.56        |
|    explained_variance   | 0.831        |
|    learning_rate        | 5.75e-05     |
|    loss                 | 0.00603      |
|    n_updates            | 11180        |
|    policy_gradient_loss | -0.00343     |
|    std                  | 0.528        |
|    value_loss           | 0.0479       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2291000, episode_reward=0.79 +/- 2.31
Episode length: 276.20 +/- 47.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 276      |
|    mean_reward     | 0.789    |
| time/              |          |
|    total_timesteps | 2291000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 643     |
|    iterations      | 1119    |
|    time_elapsed    | 3559    |
|    total_timesteps | 2291712 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2292000, episode_reward=3.28 +/- 2.39
Episode length: 236.20 +/- 52.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 236          |
|    mean_reward          | 3.28         |
| time/                   |              |
|    total_timesteps      | 2292000      |
| train/                  |              |
|    approx_kl            | 0.0048161596 |
|    clip_fraction        | 0.0365       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.56        |
|    explained_variance   | 0.825        |
|    learning_rate        | 5.75e-05     |
|    loss                 | 0.0381       |
|    n_updates            | 11190        |
|    policy_gradient_loss | -0.00426     |
|    std                  | 0.528        |
|    value_loss           | 0.046        |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2293000, episode_reward=4.25 +/- 2.15
Episode length: 211.20 +/- 51.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 211      |
|    mean_reward     | 4.25     |
| time/              |          |
|    total_timesteps | 2293000  |
---------------------------------
box reached target
box reached target
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 643     |
|    iterations      | 1120    |
|    time_elapsed    | 3562    |
|    total_timesteps | 2293760 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2294000, episode_reward=2.89 +/- 2.93
Episode length: 226.80 +/- 61.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 227         |
|    mean_reward          | 2.89        |
| time/                   |             |
|    total_timesteps      | 2294000     |
| train/                  |             |
|    approx_kl            | 0.007922262 |
|    clip_fraction        | 0.0477      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.56       |
|    explained_variance   | 0.901       |
|    learning_rate        | 5.75e-05    |
|    loss                 | 0.0608      |
|    n_updates            | 11200       |
|    policy_gradient_loss | -0.00736    |
|    std                  | 0.528       |
|    value_loss           | 0.068       |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2295000, episode_reward=2.41 +/- 2.48
Episode length: 266.00 +/- 42.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 266      |
|    mean_reward     | 2.41     |
| time/              |          |
|    total_timesteps | 2295000  |
---------------------------------
box reached target
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 643     |
|    iterations      | 1121    |
|    time_elapsed    | 3565    |
|    total_timesteps | 2295808 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2296000, episode_reward=4.30 +/- 2.38
Episode length: 229.00 +/- 39.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 229         |
|    mean_reward          | 4.3         |
| time/                   |             |
|    total_timesteps      | 2296000     |
| train/                  |             |
|    approx_kl            | 0.008055761 |
|    clip_fraction        | 0.0647      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.56       |
|    explained_variance   | 0.893       |
|    learning_rate        | 5.75e-05    |
|    loss                 | -0.0125     |
|    n_updates            | 11210       |
|    policy_gradient_loss | -0.00879    |
|    std                  | 0.53        |
|    value_loss           | 0.0429      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2297000, episode_reward=3.08 +/- 2.79
Episode length: 232.60 +/- 59.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 233      |
|    mean_reward     | 3.08     |
| time/              |          |
|    total_timesteps | 2297000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 643     |
|    iterations      | 1122    |
|    time_elapsed    | 3568    |
|    total_timesteps | 2297856 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2298000, episode_reward=2.24 +/- 2.65
Episode length: 262.40 +/- 54.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 262          |
|    mean_reward          | 2.24         |
| time/                   |              |
|    total_timesteps      | 2298000      |
| train/                  |              |
|    approx_kl            | 0.0072984872 |
|    clip_fraction        | 0.0797       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.56        |
|    explained_variance   | 0.958        |
|    learning_rate        | 5.76e-05     |
|    loss                 | -0.000651    |
|    n_updates            | 11220        |
|    policy_gradient_loss | -0.00826     |
|    std                  | 0.53         |
|    value_loss           | 0.024        |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2299000, episode_reward=0.77 +/- 2.40
Episode length: 278.80 +/- 42.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 279      |
|    mean_reward     | 0.769    |
| time/              |          |
|    total_timesteps | 2299000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 644     |
|    iterations      | 1123    |
|    time_elapsed    | 3571    |
|    total_timesteps | 2299904 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2300000, episode_reward=3.07 +/- 2.61
Episode length: 221.40 +/- 64.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 221         |
|    mean_reward          | 3.07        |
| time/                   |             |
|    total_timesteps      | 2300000     |
| train/                  |             |
|    approx_kl            | 0.005561766 |
|    clip_fraction        | 0.0532      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.56       |
|    explained_variance   | 0.959       |
|    learning_rate        | 5.76e-05    |
|    loss                 | 0.0105      |
|    n_updates            | 11230       |
|    policy_gradient_loss | -0.00716    |
|    std                  | 0.529       |
|    value_loss           | 0.0157      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2301000, episode_reward=1.83 +/- 3.02
Episode length: 261.60 +/- 47.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 262      |
|    mean_reward     | 1.83     |
| time/              |          |
|    total_timesteps | 2301000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 644     |
|    iterations      | 1124    |
|    time_elapsed    | 3574    |
|    total_timesteps | 2301952 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2302000, episode_reward=3.00 +/- 2.88
Episode length: 227.60 +/- 60.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 228          |
|    mean_reward          | 3            |
| time/                   |              |
|    total_timesteps      | 2302000      |
| train/                  |              |
|    approx_kl            | 0.0044098613 |
|    clip_fraction        | 0.0299       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.56        |
|    explained_variance   | 0.933        |
|    learning_rate        | 5.76e-05     |
|    loss                 | -0.00375     |
|    n_updates            | 11240        |
|    policy_gradient_loss | -0.00516     |
|    std                  | 0.53         |
|    value_loss           | 0.0339       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2303000, episode_reward=1.74 +/- 2.93
Episode length: 253.60 +/- 57.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 254      |
|    mean_reward     | 1.74     |
| time/              |          |
|    total_timesteps | 2303000  |
---------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2304000, episode_reward=1.73 +/- 2.96
Episode length: 263.60 +/- 45.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 264      |
|    mean_reward     | 1.73     |
| time/              |          |
|    total_timesteps | 2304000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 643     |
|    iterations      | 1125    |
|    time_elapsed    | 3577    |
|    total_timesteps | 2304000 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=2305000, episode_reward=0.66 +/- 2.33
Episode length: 275.00 +/- 50.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 275         |
|    mean_reward          | 0.656       |
| time/                   |             |
|    total_timesteps      | 2305000     |
| train/                  |             |
|    approx_kl            | 0.004672275 |
|    clip_fraction        | 0.0313      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.56       |
|    explained_variance   | 0.935       |
|    learning_rate        | 5.76e-05    |
|    loss                 | -0.00428    |
|    n_updates            | 11250       |
|    policy_gradient_loss | -0.00297    |
|    std                  | 0.528       |
|    value_loss           | 0.0298      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2306000, episode_reward=0.73 +/- 2.24
Episode length: 271.80 +/- 56.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 272      |
|    mean_reward     | 0.732    |
| time/              |          |
|    total_timesteps | 2306000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 643     |
|    iterations      | 1126    |
|    time_elapsed    | 3580    |
|    total_timesteps | 2306048 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2307000, episode_reward=2.09 +/- 2.58
Episode length: 257.80 +/- 51.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 258          |
|    mean_reward          | 2.09         |
| time/                   |              |
|    total_timesteps      | 2307000      |
| train/                  |              |
|    approx_kl            | 0.0047660805 |
|    clip_fraction        | 0.0306       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.55        |
|    explained_variance   | 0.868        |
|    learning_rate        | 5.77e-05     |
|    loss                 | 0.0179       |
|    n_updates            | 11260        |
|    policy_gradient_loss | -0.00333     |
|    std                  | 0.526        |
|    value_loss           | 0.0368       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2308000, episode_reward=1.57 +/- 3.02
Episode length: 253.60 +/- 56.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 254      |
|    mean_reward     | 1.57     |
| time/              |          |
|    total_timesteps | 2308000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 644     |
|    iterations      | 1127    |
|    time_elapsed    | 3583    |
|    total_timesteps | 2308096 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2309000, episode_reward=1.03 +/- 2.24
Episode length: 287.60 +/- 24.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 288          |
|    mean_reward          | 1.03         |
| time/                   |              |
|    total_timesteps      | 2309000      |
| train/                  |              |
|    approx_kl            | 0.0045586172 |
|    clip_fraction        | 0.0279       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.55        |
|    explained_variance   | 0.92         |
|    learning_rate        | 5.77e-05     |
|    loss                 | 0.0157       |
|    n_updates            | 11270        |
|    policy_gradient_loss | -0.00224     |
|    std                  | 0.525        |
|    value_loss           | 0.029        |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2310000, episode_reward=1.94 +/- 2.67
Episode length: 242.60 +/- 70.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 243      |
|    mean_reward     | 1.94     |
| time/              |          |
|    total_timesteps | 2310000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 644     |
|    iterations      | 1128    |
|    time_elapsed    | 3586    |
|    total_timesteps | 2310144 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2311000, episode_reward=3.17 +/- 2.78
Episode length: 240.00 +/- 51.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 240         |
|    mean_reward          | 3.17        |
| time/                   |             |
|    total_timesteps      | 2311000     |
| train/                  |             |
|    approx_kl            | 0.004580221 |
|    clip_fraction        | 0.0383      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.54       |
|    explained_variance   | 0.92        |
|    learning_rate        | 5.77e-05    |
|    loss                 | -0.0102     |
|    n_updates            | 11280       |
|    policy_gradient_loss | -0.00493    |
|    std                  | 0.525       |
|    value_loss           | 0.0459      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2312000, episode_reward=4.28 +/- 2.36
Episode length: 230.60 +/- 45.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 231      |
|    mean_reward     | 4.28     |
| time/              |          |
|    total_timesteps | 2312000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 644     |
|    iterations      | 1129    |
|    time_elapsed    | 3589    |
|    total_timesteps | 2312192 |
--------------------------------
box reached target
box reached target
Eval num_timesteps=2313000, episode_reward=1.80 +/- 2.97
Episode length: 253.60 +/- 62.73
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 254       |
|    mean_reward          | 1.8       |
| time/                   |           |
|    total_timesteps      | 2313000   |
| train/                  |           |
|    approx_kl            | 0.0070856 |
|    clip_fraction        | 0.0397    |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.55     |
|    explained_variance   | 0.865     |
|    learning_rate        | 5.77e-05  |
|    loss                 | -0.000989 |
|    n_updates            | 11290     |
|    policy_gradient_loss | -0.00542  |
|    std                  | 0.526     |
|    value_loss           | 0.039     |
---------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2314000, episode_reward=3.08 +/- 2.72
Episode length: 229.40 +/- 57.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 229      |
|    mean_reward     | 3.08     |
| time/              |          |
|    total_timesteps | 2314000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 644     |
|    iterations      | 1130    |
|    time_elapsed    | 3592    |
|    total_timesteps | 2314240 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2315000, episode_reward=0.80 +/- 2.53
Episode length: 283.60 +/- 32.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 284         |
|    mean_reward          | 0.797       |
| time/                   |             |
|    total_timesteps      | 2315000     |
| train/                  |             |
|    approx_kl            | 0.006093405 |
|    clip_fraction        | 0.059       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.55       |
|    explained_variance   | 0.957       |
|    learning_rate        | 5.78e-05    |
|    loss                 | 0.00513     |
|    n_updates            | 11300       |
|    policy_gradient_loss | -0.00728    |
|    std                  | 0.525       |
|    value_loss           | 0.0136      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2316000, episode_reward=2.20 +/- 2.60
Episode length: 248.40 +/- 63.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 248      |
|    mean_reward     | 2.2      |
| time/              |          |
|    total_timesteps | 2316000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 644     |
|    iterations      | 1131    |
|    time_elapsed    | 3596    |
|    total_timesteps | 2316288 |
--------------------------------
box reached target
Eval num_timesteps=2317000, episode_reward=1.07 +/- 2.09
Episode length: 270.40 +/- 59.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 270          |
|    mean_reward          | 1.07         |
| time/                   |              |
|    total_timesteps      | 2317000      |
| train/                  |              |
|    approx_kl            | 0.0057253214 |
|    clip_fraction        | 0.0417       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.55        |
|    explained_variance   | 0.943        |
|    learning_rate        | 5.78e-05     |
|    loss                 | -0.0115      |
|    n_updates            | 11310        |
|    policy_gradient_loss | -0.00544     |
|    std                  | 0.525        |
|    value_loss           | 0.0167       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2318000, episode_reward=1.24 +/- 2.15
Episode length: 283.00 +/- 34.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 283      |
|    mean_reward     | 1.24     |
| time/              |          |
|    total_timesteps | 2318000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 644     |
|    iterations      | 1132    |
|    time_elapsed    | 3599    |
|    total_timesteps | 2318336 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2319000, episode_reward=-0.44 +/- 0.56
Episode length: 300.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | -0.442      |
| time/                   |             |
|    total_timesteps      | 2319000     |
| train/                  |             |
|    approx_kl            | 0.004889707 |
|    clip_fraction        | 0.0356      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.54       |
|    explained_variance   | 0.939       |
|    learning_rate        | 5.78e-05    |
|    loss                 | -0.00936    |
|    n_updates            | 11320       |
|    policy_gradient_loss | -0.00352    |
|    std                  | 0.524       |
|    value_loss           | 0.0149      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2320000, episode_reward=2.05 +/- 2.61
Episode length: 249.40 +/- 66.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 249      |
|    mean_reward     | 2.05     |
| time/              |          |
|    total_timesteps | 2320000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 644     |
|    iterations      | 1133    |
|    time_elapsed    | 3602    |
|    total_timesteps | 2320384 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2321000, episode_reward=2.33 +/- 2.50
Episode length: 250.00 +/- 61.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 250         |
|    mean_reward          | 2.33        |
| time/                   |             |
|    total_timesteps      | 2321000     |
| train/                  |             |
|    approx_kl            | 0.005321471 |
|    clip_fraction        | 0.0379      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.54       |
|    explained_variance   | 0.796       |
|    learning_rate        | 5.78e-05    |
|    loss                 | 0.0187      |
|    n_updates            | 11330       |
|    policy_gradient_loss | -0.00414    |
|    std                  | 0.525       |
|    value_loss           | 0.0822      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2322000, episode_reward=3.09 +/- 2.79
Episode length: 223.20 +/- 65.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 223      |
|    mean_reward     | 3.09     |
| time/              |          |
|    total_timesteps | 2322000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 644     |
|    iterations      | 1134    |
|    time_elapsed    | 3605    |
|    total_timesteps | 2322432 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2323000, episode_reward=1.90 +/- 2.88
Episode length: 254.60 +/- 63.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 255         |
|    mean_reward          | 1.9         |
| time/                   |             |
|    total_timesteps      | 2323000     |
| train/                  |             |
|    approx_kl            | 0.004522061 |
|    clip_fraction        | 0.0312      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.54       |
|    explained_variance   | 0.84        |
|    learning_rate        | 5.79e-05    |
|    loss                 | 0.00826     |
|    n_updates            | 11340       |
|    policy_gradient_loss | -0.00342    |
|    std                  | 0.522       |
|    value_loss           | 0.0472      |
-----------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2324000, episode_reward=3.03 +/- 2.96
Episode length: 246.20 +/- 54.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 246      |
|    mean_reward     | 3.03     |
| time/              |          |
|    total_timesteps | 2324000  |
---------------------------------
box reached target
--------------------------------
| time/              |         |
|    fps             | 644     |
|    iterations      | 1135    |
|    time_elapsed    | 3608    |
|    total_timesteps | 2324480 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2325000, episode_reward=5.28 +/- 0.16
Episode length: 192.00 +/- 15.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 192          |
|    mean_reward          | 5.28         |
| time/                   |              |
|    total_timesteps      | 2325000      |
| train/                  |              |
|    approx_kl            | 0.0052678753 |
|    clip_fraction        | 0.0438       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.53        |
|    explained_variance   | 0.845        |
|    learning_rate        | 5.79e-05     |
|    loss                 | 0.0392       |
|    n_updates            | 11350        |
|    policy_gradient_loss | -0.00285     |
|    std                  | 0.521        |
|    value_loss           | 0.094        |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2326000, episode_reward=4.23 +/- 2.37
Episode length: 209.20 +/- 45.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 209      |
|    mean_reward     | 4.23     |
| time/              |          |
|    total_timesteps | 2326000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 644     |
|    iterations      | 1136    |
|    time_elapsed    | 3611    |
|    total_timesteps | 2326528 |
--------------------------------
box reached target
Eval num_timesteps=2327000, episode_reward=-0.03 +/- 0.71
Episode length: 300.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | -0.027       |
| time/                   |              |
|    total_timesteps      | 2327000      |
| train/                  |              |
|    approx_kl            | 0.0059375274 |
|    clip_fraction        | 0.0469       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.53        |
|    explained_variance   | 0.835        |
|    learning_rate        | 5.79e-05     |
|    loss                 | 0.02         |
|    n_updates            | 11360        |
|    policy_gradient_loss | -0.00686     |
|    std                  | 0.521        |
|    value_loss           | 0.0647       |
------------------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2328000, episode_reward=2.21 +/- 2.60
Episode length: 253.40 +/- 57.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 253      |
|    mean_reward     | 2.21     |
| time/              |          |
|    total_timesteps | 2328000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 644     |
|    iterations      | 1137    |
|    time_elapsed    | 3614    |
|    total_timesteps | 2328576 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2329000, episode_reward=2.11 +/- 2.67
Episode length: 249.20 +/- 64.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 249          |
|    mean_reward          | 2.11         |
| time/                   |              |
|    total_timesteps      | 2329000      |
| train/                  |              |
|    approx_kl            | 0.0060292296 |
|    clip_fraction        | 0.0533       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.53        |
|    explained_variance   | 0.829        |
|    learning_rate        | 5.79e-05     |
|    loss                 | -0.00192     |
|    n_updates            | 11370        |
|    policy_gradient_loss | -0.00784     |
|    std                  | 0.52         |
|    value_loss           | 0.0535       |
------------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2330000, episode_reward=0.51 +/- 2.52
Episode length: 286.00 +/- 28.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 286      |
|    mean_reward     | 0.506    |
| time/              |          |
|    total_timesteps | 2330000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 644     |
|    iterations      | 1138    |
|    time_elapsed    | 3617    |
|    total_timesteps | 2330624 |
--------------------------------
box reached target
box reached target
box reached target
box reached target
Eval num_timesteps=2331000, episode_reward=3.39 +/- 2.39
Episode length: 225.60 +/- 64.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 226         |
|    mean_reward          | 3.39        |
| time/                   |             |
|    total_timesteps      | 2331000     |
| train/                  |             |
|    approx_kl            | 0.007921579 |
|    clip_fraction        | 0.0668      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.53       |
|    explained_variance   | 0.857       |
|    learning_rate        | 5.8e-05     |
|    loss                 | 0.0191      |
|    n_updates            | 11380       |
|    policy_gradient_loss | -0.00635    |
|    std                  | 0.52        |
|    value_loss           | 0.0648      |
-----------------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2332000, episode_reward=3.27 +/- 2.49
Episode length: 223.80 +/- 63.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 224      |
|    mean_reward     | 3.27     |
| time/              |          |
|    total_timesteps | 2332000  |
---------------------------------
box reached target
box reached target
--------------------------------
| time/              |         |
|    fps             | 644     |
|    iterations      | 1139    |
|    time_elapsed    | 3620    |
|    total_timesteps | 2332672 |
--------------------------------
box reached target
Eval num_timesteps=2333000, episode_reward=0.95 +/- 2.28
Episode length: 276.20 +/- 47.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 276         |
|    mean_reward          | 0.949       |
| time/                   |             |
|    total_timesteps      | 2333000     |
| train/                  |             |
|    approx_kl            | 0.006136682 |
|    clip_fraction        | 0.0476      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.52       |
|    explained_variance   | 0.822       |
|    learning_rate        | 5.8e-05     |
|    loss                 | 0.0395      |
|    n_updates            | 11390       |
|    policy_gradient_loss | -0.00499    |
|    std                  | 0.518       |
|    value_loss           | 0.0584      |
-----------------------------------------
box reached target
Eval num_timesteps=2334000, episode_reward=-0.33 +/- 0.70
Episode length: 300.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 300      |
|    mean_reward     | -0.33    |
| time/              |          |
|    total_timesteps | 2334000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 644     |
|    iterations      | 1140    |
|    time_elapsed    | 3623    |
|    total_timesteps | 2334720 |
--------------------------------
box reached target
box reached target
box reached target
Eval num_timesteps=2335000, episode_reward=3.15 +/- 2.75
Episode length: 254.00 +/- 41.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 254          |
|    mean_reward          | 3.15         |
| time/                   |              |
|    total_timesteps      | 2335000      |
| train/                  |              |
|    approx_kl            | 0.0068732463 |
|    clip_fraction        | 0.0499       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.52        |
|    explained_variance   | 0.692        |
|    learning_rate        | 5.8e-05      |
|    loss                 | 0.0089       |
|    n_updates            | 11400        |
|    policy_gradient_loss | -0.00596     |
|    std                  | 0.517        |
|    value_loss           | 0.0536       |
------------------------------------------
slurmstepd: error: *** JOB 9100 ON sxm003 CANCELLED AT 2024-03-08T17:27:39 DUE TO TIME LIMIT ***
